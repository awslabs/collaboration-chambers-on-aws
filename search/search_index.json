{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Scale-Out Computing on AWS ?","text":"<p>Scale-Out Computing on AWS is a solution that helps customers more easily deploy and operate a multiuser environment for computationally intensive workflows. The solution features a large selection of compute resources; fast network backbone; unlimited storage; and budget and cost management directly integrated within AWS. The solution also deploys a user interface (UI) and automation tools that allows you to create your own queues, scheduler resources, Amazon Machine Images (AMIs), software, and libraries.  This solution is designed to provide a production ready reference implementation to be a starting point for deploying an AWS environment to run scale-out workloads, allowing you to focus on running simulations designed to solve complex computational problems.  </p>"},{"location":"#easy-installation","title":"Easy installation","text":"<p>Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation </p> <p>Did you know?</p> <ul> <li>You can have multiple Scale-Out Computing on AWS clusters on the same AWS account</li> <li>Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators</li> </ul>"},{"location":"#access-your-cluster-in-1-click","title":"Access your cluster in 1 click","text":"<p>You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization)1 or through SSH.</p>"},{"location":"#simple-job-submission","title":"Simple Job Submission","text":"<p>Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS. Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS picks the most optimal hardware (defined by the HPC administrator)</p> <pre><code># Advanced Configuration\nuser@host$ qsub -l instance_type=c5n.18xlarge \\\n-l instance_ami=ami-123abcde\n    -l nodes=2 -l scratch_size=300 -l efa_support=true\n-l spot_price=1.55 myscript.sh\n\n# Basic Configuration\nuser@host$ qsub myscript.sh\n</code></pre> <p>Info</p> <ul> <li>Check our Web-Based utility to generate you submission command</li> <li>Refer to this page for tutorial and examples</li> <li>Refer to this page to list all supported parameters</li> <li>Jobs can also be submitted via HTTP API or via web interface</li> </ul>"},{"location":"#os-agnostic-and-support-for-custom-ami","title":"OS agnostic and support for custom AMI","text":"<p>Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=&lt;ami_id&gt; at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs). SOCA supports heterogeneous environment, so you can have concurrent jobs running different operating system on the same cluster. </p> <p>AMI using OS different than the scheduler</p> <p>In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution.</p> <p>In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI</p> <pre><code>user@host$ qsub -l instance_ami=&lt;ami_id&gt; -l base_os=rhel7 myscript.sh\n</code></pre> <p>Scale-Out Computing on AWS AMI requirements</p> <p>When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment. Read this page for AMI creation best practices</p>"},{"location":"#web-user-interface","title":"Web User Interface","text":"<p>Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as:</p> <ul> <li>Start/Stop DCV sessions in 1 click</li> <li>Download private key in both PEM or PPK format</li> <li>Check the queue and job status in real-time</li> <li>Add/Remove LDAP users </li> <li>Access the analytic dashboard</li> <li>Access your filesystem</li> <li>Understand why your jobs are stuck in the queue</li> <li>Create Application profiles and let your users submit job directly via the web interface</li> </ul>"},{"location":"#http-rest-api","title":"HTTP Rest API","text":"<p>Users can submit/retrieve/delete jobs remotely via an HTTP REST API</p>"},{"location":"#budgets-and-cost-management","title":"Budgets and Cost Management","text":"<p>You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. </p> <p>Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired.</p> <p>Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend.</p>"},{"location":"#detailed-cluster-analytics","title":"Detailed Cluster Analytics","text":"<p>Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity.</p> <p>Don't know where to start?</p> <p>Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana.</p>"},{"location":"#100-customizable","title":"100% Customizable","text":"<p>Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates, shell scripts and python code. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github.</p>"},{"location":"#persistent-and-unlimited-storage","title":"Persistent and Unlimited Storage","text":"<p>Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS</p>"},{"location":"#centralized-user-management","title":"Centralized user-management","text":"<p>Customers can create unlimited LDAP users and groups. By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster.</p>"},{"location":"#automatic-backup","title":"Automatic backup","text":"<p>Scale-Out Computing on AWS automatically backup your data with no additional effort required on your side.</p>"},{"location":"#support-for-network-licenses","title":"Support for network licenses","text":"<p>Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available. </p>"},{"location":"#automatic-errors-handling","title":"Automatic Errors Handling","text":"<p>Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone. To simplify troubleshooting, all these errors are reported on the web interface</p>"},{"location":"#custom-fair-share","title":"Custom fair-share","text":"<p>Each user is given a score which vary based on:</p> <ul> <li>Number of job in the queue</li> <li>Time each job is queued</li> <li>Priority of each job</li> <li>Type of instance</li> </ul> <p>Job that belong to the user with the highest score will start next. Fair Share is is configured at the queue level (so you can have one queue using FIFO and another one Fair Share)</p>"},{"location":"#and-more","title":"And more ...","text":"<p>Refer to the various sections (tutorial/security/analytics ...) to learn more about this solution</p> <ol> <li> <p>DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.*\u00a0\u21a9</p> </li> </ol>"},{"location":"Custom-AMIs/","title":"About","text":"<p>This section describes how to create custom Amazon Machine Images (AMIs) for the SOCA compute nodes. These AMIs can reduce the time that it takes for a new node to start by pre-installing software and packages.</p> <p>Refer to the left sidebar for more detailed resources.</p>"},{"location":"Custom-AMIs/image-builder/","title":"EC2 Image Builder","text":"<p>SOCA uses EC2 Image Builder Pipelines to automate the creation of custom AMIs. By default it creates pipelines for four different CentOS 7 based AMIs.</p> <ul> <li>CentOS 7 SOCA AMI: Preinstalls the SOCA software</li> <li>CentOS 7 SOCA Desktop: Same as CentOS 7 SOCA AMI plus installs the DCV software for Linux Desktop instances.</li> <li>CentOS 7 EDA AMI: Same as CentOS 7 SOCA AMI plus installs packages required by EDA tools.</li> <li>CentOS 7 SOCA Desktop: Same as CentOS 7 EDA AMI plus installs the DCV software for Linux Desktop instances.</li> </ul> <p>By default the pipelines are created and executed. You can follow the *BuildUrl links in the stack outputs to get the status of the AMIs and the AMI IDs once the builds are complete.</p> <p>It also has a pipeline that in the process of building the AMI creates a yum mirror and stores it in S3 and also saves the SOCA software in the same S3 bucket.</p> <ul> <li>CentOS 7 MirrorRepos: Creates yum repo mirror in S3 and mirrors source code required by SOCA in the same S3 bucket.</li> </ul> <p>The S3 bucket and folder for the mirror are stored in Systems Manager Parameter Store with the following keys:</p> <ul> <li>/StackName/repositories/distribution/date-timestamp/RepositoryBucket</li> <li>/StackName/repositories/distribution/date-timestamp/RepositoryFolder</li> <li>/StackName/repositories/distribution/latest/RepositoryBucket</li> <li>/StackName/repositories/distribution/latest/RepositoryBucket</li> </ul> <p>These SSM parameters can be passed as parameters to SOCA so that it uses the S3 repository instead of public mirrors off of the internet. This allows you to use a stable version of the repos and also removes the requirement for internet access because the S3 bucket can be accessed using a VPC Endpoint.</p>"},{"location":"Custom-AMIs/image-builder/#manually-running-the-pipelines-to-create-the-amis","title":"Manually Running the Pipelines to Create the AMIs","text":"<p>You can also manually trigger the pipelines to create the AMIs.</p>"},{"location":"Custom-AMIs/image-builder/#running-pipelines-using-the-console","title":"Running Pipelines Using the Console","text":"<p>Follow the ImagePipelineUrl link in the stack outputs to go to the EC2 Image Builder pipeline. From Actions select **Run Pipeline*. This will trigger the pipeline and you can monitor it's progress by selecting Images on the left.</p>"},{"location":"Custom-AMIs/image-builder/#running-pipelines-using-the-aws-cli","title":"Running Pipelines Using the AWS CLI","text":"<p>Go to the CloudFormation console and the Outputs tab of your SOCA stack. The command to manually run the pipelines are in the outputs.</p>"},{"location":"Custom-AMIs/image-builder/#monitoring-pipeline-builds","title":"Monitoring Pipeline Builds","text":"<p>Go to the EC2 Image Builder console, select Image Pipelines, and select a pipeline to see it's output images.</p>"},{"location":"Custom-AMIs/image-builder/#debug","title":"Debug","text":"<p>By default, the pipelines are configured to terminate the build instance if a build fails. Unfortunately, this can make debug difficult. The logs are stored in S3, but the names are difficult to find. Debug is easier if you change the TerminateBuildInstanceOnFailure parameter to false so that the instance is left running after a failed build. Then you can easily see the output in <code>/var/log/messages</code></p> <pre><code>grep ImageBuilder /var/log/messages | less\n</code></pre>"},{"location":"FAQ/","title":"Knowledge Base - List all Errors","text":"<p>This page only list errors related to CloudFormation. Submit a ticket if your error is not listed here</p> <p>CloudFormation troubleshooting</p> <p>We recommend to disable \"RollBack on Failure\" to simplify CloudFormation debugging/troubleshooting</p>"},{"location":"FAQ/#errors-during-stack-creation","title":"Errors during Stack Creation","text":""},{"location":"FAQ/#c1-the-following-resources-failed-to-create-checksocaprerequisite","title":"C1: The following resource(s) failed to create: CheckSOCAPreRequisite.","text":"<pre><code>- Stack: Primary Template\n- Event: Failed to create resource. See the details in CloudWatch Log Stream: 2020/03/17/[$LATEST]xxxxx\n- Resolution: Refer to the Physical ID message. It's could be because you used uppercase in the stack name, have a stack name longer than 20 chars or use a non-supported AWS region\n</code></pre>"},{"location":"FAQ/#errors-during-stack-deletion","title":"Errors during Stack Deletion","text":""},{"location":"FAQ/#d1-cannot-delete-entity-must-delete-policies-first","title":"D1: Cannot delete entity, must delete policies first.","text":"<pre><code>- Stack: Security\n- Event:  Cannot delete entity, must delete policies first. (Service: AmazonIdentityManagement; Status Code: 409; Error Code: DeleteConflict; Request ID: x)\n- Resolution: You have added extra policies to the IAM roles created by SOCA. Remove the policy or delete the role entirely\n</code></pre>"},{"location":"FAQ/#d2-backup-vault-cannot-be-deleted-contains-recovery-points","title":"D2: Backup vault cannot be deleted (contains  recovery points) <pre><code>- Stack: Configuration\n- Event: Backup vault cannot be deleted (contains 3 recovery points) (Service: AWSBackup; Status Code: 400; Error Code: InvalidRequestException; Request ID:  x)\n- Resolution: You must manually remove the recovery points from your SOCA Backup Vault\n</code></pre>","text":""},{"location":"FAQ/#errors-post-deployment","title":"Errors Post Deployment","text":""},{"location":"FAQ/#p1-why-do-i-see-502-bad-gateway-when-i-try-to-log-into-the-soca-web-ui","title":"P1: Why do I see \"502 Bad Gateway\" when I try to log into the SOCA web UI? <pre><code>- Resolution: This error indicates that they web server is not running on the scheduler server.  If you just installed SOCA, wait until you can log into the scheduler server before trying to access the management web UI.\n</code></pre>","text":""},{"location":"analytics/","title":"About","text":"<p>Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity.</p> <p>Don't know where to start?</p> <p>Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana.</p> <p>Refer to the left sidebar for more detailed resources</p>"},{"location":"analytics/build-kibana-dashboards/","title":"Create your own analytics dashboard","text":"<p>Pre-Requisites</p> <p>You must have configure your Kibana index first</p> <p>On your Kibana cluster, click \"Visualize\" to create a new visualization. Below are some example to help you get started</p> <p>Note: For each dashboard, you can get detailed data at user, queue, job or project level by simply using the \"Filters\" section</p>"},{"location":"analytics/build-kibana-dashboards/#money-spent-by-instance-type","title":"Money spent by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Sum</li> <li>Field: estimated_price_ondemand</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By: metric: Sum of estimated_price_ondemand</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By:  metric: Sum of estimated_price_ondemand</li> </ul> </li> </ul> <p></p>"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user","title":"Jobs per user","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p> </p>"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user-split-by-instance-type","title":"Jobs per user split by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p> </p>"},{"location":"analytics/build-kibana-dashboards/#most-active-projects","title":"Most active projects","text":"<p>Configuration</p> <ul> <li>Select \"Pie\" and \"jobs\" index</li> <li>Slice Size (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Split Slices (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: project.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p> <p>If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard)</p> <p></p>"},{"location":"analytics/build-kibana-dashboards/#instance-type-launched-by-user","title":"Instance type launched by user","text":"<p>Configuration</p> <ul> <li>Select \"Heat Map\" and \"jobs\" index</li> <li>Value (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Y Axis (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"analytics/build-kibana-dashboards/#number-of-nodes-in-the-cluster","title":"Number of nodes in the cluster","text":"<p>Configuration</p> <ul> <li>Select \"Lines\" and \"pbsnodes\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Unique Count</li> <li>Field: Mom.keyword</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Date Histogram,</li> <li>Field: timestamp</li> <li>Interval: Minute</li> </ul> </li> </ul> <p></p>"},{"location":"analytics/build-kibana-dashboards/#total-number-of-cpus-by-instance-type","title":"Total number of CPUs by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Area\" and \"pbsnodes\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Sum</li> <li>Field: resources_available.ncpus</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Date Histogram,</li> <li>Field: timestamp</li> <li>Interval: minute</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: resources_available.instance_type.keyword</li> <li>Order By: metric: Sum</li> </ul> </li> </ul> <p></p>"},{"location":"analytics/build-kibana-dashboards/#detailed-information-per-user","title":"Detailed information per user","text":"<p>Configuration</p> <ul> <li>Select \"Datatables\" and \"jobs\" index</li> <li>Metric (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Split Rows (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Rows (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: instance_type_used.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Rows (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: price_ondemand.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Rows (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: job_name.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"analytics/build-kibana-dashboards/#find-the-price-for-a-given-simulation","title":"Find the price for a given simulation","text":"<p>Each job comes with <code>price_ondemand</code> and <code>price_reserved</code> attributes which are calculated based on: <code>number of nodes * ( simulation_hours * instance_hourly_rate )</code></p> <p></p>"},{"location":"analytics/monitor-cluster-activity/","title":"Monitor your cluster and job activity","text":""},{"location":"analytics/monitor-cluster-activity/#dashboard-url","title":"Dashboard URL","text":"<p>Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\" </p>"},{"location":"analytics/monitor-cluster-activity/#create-indexes","title":"Create Indexes","text":"<p>Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\"</p> <p></p> <p>Go under Management and Click Index Patterns</p> <p></p> <p>Create your first index by typing pbsnodes*.</p> <p></p> <p>Click next, and then specify the Time Filter key (timestamp). Once done, click Create Index Pattern.</p> <p></p> <p>Repeat the same operation for jobs* index </p> <p></p> <p>This time,  select start_iso as time filter key.</p> <p></p> <p>Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data</p> <p></p>"},{"location":"analytics/monitor-cluster-activity/#index-information","title":"Index Information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time ____"},{"location":"analytics/monitor-cluster-activity/#examples","title":"Examples","text":""},{"location":"analytics/monitor-cluster-activity/#cluster-node","title":"Cluster Node","text":""},{"location":"analytics/monitor-cluster-activity/#job-metadata","title":"Job Metadata","text":""},{"location":"analytics/monitor-cluster-activity/#troubleshooting-access-permission","title":"Troubleshooting access permission","text":"<p>Access to ElasticSearch is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: <pre><code>{\"Message\":\"User: anonymous is not authorized to perform: es:ESHttpGet\"}\n</code></pre></p> <p>To solve this issue, log in to AWS Console  and go to ElasticSearch Service dashboard. Select  your ElasticSearch cluster and click \"Modify Access Policy\"</p> <p></p> <p>Finally, simply add your new IP under the \"Condition\" block, then click Submit</p> <p></p> <p>Please note it may take up to 5 minutes for your IP to be whitelisted</p>"},{"location":"analytics/monitor-cluster-activity/#create-your-own-dashboard","title":"Create your own dashboard","text":""},{"location":"analytics/monitor-system-with-metricbeat/","title":"Monitor Nodes with MetricBeat","text":""},{"location":"analytics/monitor-system-with-metricbeat/#what-is-metricbeat","title":"What is MetricBeat","text":"<p>Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server.</p>"},{"location":"analytics/monitor-system-with-metricbeat/#how-to-enable-metricbeat","title":"How to enable MetricBeat","text":"<p>MetricBeat is disabled by default. To enable it either submit a job with <code>system_metrics=True</code> or enable this parameter at the queue level. When this feature is active, SOCA will automatically install and configure MetricBeat on all compute nodes provisioned for your jobs. Edit <code>ComputeNodeConfigureMetrics.sh</code> as needed (eg: change the check period, number of process to track etc ...)</p> <p>Note</p> <p>The very first job using MetricBeat will take an extra 45 secs as SOCA will perform the initial dashboard setup on Kibana. This is a one time operation, and can be deactivated if you do not want to install MetricBeat dashboards by default</p>"},{"location":"analytics/monitor-system-with-metricbeat/#access-metricbeat-data","title":"Access MetricBeat data","text":"<p>MetricBeat is automatically integrated with your ElasticSearch cluster. Click 'Dashboard' and search for \"MetricBeat System\"</p> <p></p> <p>\"Host Overview ECS\" dashboard will give you system information related to nodes provisioned per job, users or queue. By default ELK reports \"Last 15 minutes\" data, so make sure you update the time selection accordingly</p> <p></p> <p>You can filter the results by job id, job owner, queue, process name or host IP. See the example below which return metrics information for job 19544</p> <p></p>"},{"location":"budget/","title":"About","text":"<p>You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. </p> <p>Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired.</p> <p>Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend.</p> <p>Refer to the left sidebar for more detailed resources</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/","title":"Keep control of your HPC cost on AWS","text":"<p>Scale-Out Computing on AWS offers multiple ways to make sure you will stay within budget while running your HPC workloads on AWS</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-who-can-submit-jobs","title":"Limit who can submit jobs","text":"<p>Only allow specific individual users or/and LDAP groups to submit jobs. Refer to this page for examples and documentation</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-what-type-of-ec2-instance-can-be-provisioned","title":"Limit what type of EC2 instance can be provisioned","text":"<p>Control what type of EC2 instances can be provisioned for any given queue. Refer to this page for examples and documentation</p> <p>Accelerated Computing Instances</p> <p>Unless required for your workloads, it's recommended to exclude \"p2\", \"p3\", \"g2\", \"g3\", \"p3dn\" or other GPU instances type. </p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#force-jobs-to-run-only-on-reserved-instances","title":"Force jobs to run only on Reserved Instances","text":"<p>You can limit a job to run only on Reserved Instances if you specify <code>force_ri=True</code> (Documentation) flag at job submission or for the entire queue. Your job will stay in the queue if you do not have any Reserved Instance available.</p> <p></p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-the-number-of-concurrent-jobs-or-provisioned-instances","title":"Limit the number of concurrent jobs or provisioned instances","text":"<p>You can limit the number of concurrent running jobs or provisioned instances at the queue level. Edit <code>queue_mapping.yml</code>, specify either <code>max_running_jobs</code> or <code>max_provisioned_instances</code> to the limit you do not want to exceed. <pre><code>queue_type:\n  compute:\n    queues: [\"myqueue\"]\nmax_running_jobs: 5\nmax_provisioned_instances: 10\n</code></pre></p> <p>In this example, the maximum number of running job for \"myqueue\" will be 5. Similarly, jobs cannot request more than 10 instances (note: you can also limit the type/family of instances you want your user to provision)</p> <p>These settings are independent so you can choose to either limit by # jobs, # instances, both or none.</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#create-a-budget","title":"Create a budget","text":"<p>Creating an AWS Budget will ensure jobs can't be submitted if the budget allocated to the team/queue/project has exceeded the authorized amount. Refer to this page for examples and documentation</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#review-your-hpc-cost-in-a-central-dashboard","title":"Review your HPC cost in a central dashboard","text":"<p>Stay on top of your AWS costs in real time. Quickly visualize your overall usage and find answers to your most common questions:</p> <ul> <li> <p>Who are my top users?</p> </li> <li> <p>How much money did we spend for Project A?</p> </li> <li> <p>How much storage did we use for Queue B?</p> </li> <li> <p>Where my money is going (storage, compute ...)</p> </li> <li> <p>Etc ...</p> </li> </ul> <p>Refer to this page for examples and documentation</p>"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#best-practices","title":"Best practices","text":"<p>Assuming you are on-boarding a new team, here are our recommend best practices:</p> <p>1 - Create LDAP account for all users</p> <p>2 - Create LDAP group for the team. Add all users to the group</p> <p>3 - Create a new queue</p> <p>4 - Limit the queue to the LDAP group you just created</p> <p>5 - Limit the type of EC2 instances your users can provision</p> <p>6 - If needed, configure restricted parameters</p> <p>7 - Create a Budget to make sure the new team won't spend more than what's authorized</p> <p>8 - Limit your job to only run on your Reserved Instances or limit the number of provisioned instances for your queue</p>"},{"location":"budget/review-hpc-costs/","title":"Review your HPC costs","text":""},{"location":"budget/review-hpc-costs/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<p>Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to):</p> <ul> <li>Project Name</li> <li>Job Owner</li> <li>Job Name</li> <li>Job Queue</li> <li>Job Id</li> </ul> <p>These are the default tags and you can add your own tags if needed.  </p>"},{"location":"budget/review-hpc-costs/#step1-enable-cost-allocation-tags","title":"Step1: Enable Cost Allocation Tags","text":"<p>Be patient</p> <p>It could take up to 24 hours for the tags to be active</p> <p>Click on your account name (1) then select \"My Billing Dashboard\" (2)  </p> <p></p> <p>Then click Cost Allocation tag</p> <p></p> <p>Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\"  </p> <p></p>"},{"location":"budget/review-hpc-costs/#step-2-enable-cost-explorer","title":"Step 2: Enable Cost Explorer","text":"<p>In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2).</p> <p></p>"},{"location":"budget/review-hpc-costs/#step-3-query-cost-explorer","title":"Step 3: Query Cost Explorer","text":"<p>Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2).  </p> <p></p> <p>To get more detailed information, select 'Group By' and apply additional filters.  Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue  </p> <p></p>"},{"location":"budget/set-up-budget-project/","title":"Set up budget per project","text":"<p>On this page, I will demonstrate how to configure a budget for a given project and reject job if you exceed the allocated budget</p> <p>For this example, I will create a budget named \"Project 1\" and prevent user to submit job if (1) they do not belong to the project and (2) if the budget has expired.</p> <p>First, read this link to understand how to monitor your cluster cost and budgets on AWS.</p>"},{"location":"budget/set-up-budget-project/#configure-the-scheduler-hook","title":"Configure the scheduler hook","text":"<p>To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at <code>/apps/soca/&lt;YOUR_SOCA_CLUSTER_ID&gt;/cluster_hooks/queuejob/check_project_budget.py</code> First, edit this file and manually enter your AWS account id:</p> <pre><code># User Variables\naws_account_id = '&lt;ENTER_YOUR_AWS_ACCOUNT_ID&gt;'\nbudget_config_file = '/apps/soca/&lt;YOUR_SOCA_CLUSTER_ID&gt;/cluster_manager/settings/project_cost_manager.txt'\nuser_must_belong_to_project = True  # Change if you don't want to restrict project to a list of users\nallow_job_no_project = False  # Change if you do not want to enforce project at job submission\nallow_user_multiple_projects = True  # Change if you want to restrict a user to one project\n</code></pre> <p>Then enable the hook by running the following commands as root (on the scheduler host):</p> <pre><code>user@host: qmgr -c \"create hook check_project_budget event=queuejob\"\nuser@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/&lt;YOUR_SOCA_CLUSTER_ID&gt;/cluster_hooks/queuejob/check_project_budget.py\"\n</code></pre>"},{"location":"budget/set-up-budget-project/#test-it","title":"Test it","text":""},{"location":"budget/set-up-budget-project/#submit-a-job-when-budget-is-valid","title":"Submit a job when budget is valid","text":"<p>Go to AWS Billing, click Budget on the left sidebar and create a new budget</p> <p></p> <p>Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements.  For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject)</p> <p></p> <p>Set up a email notification when your budget exceed 80% then click \"Confirm Budget\"</p> <p></p> <p>As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job</p> <pre><code>user@host$ qsub -- /bin/echo Hello\nqsub: Error. You tried to submit job without project. Specify project using -P parameter\n</code></pre> <p>This does not work because the job was submitted without project defined. If you still want to let your users do that, edit <code>allow_job_no_project = False</code> on the hook file. Let's try the same request but specify <code>-P \"Project 1\"</code> during submission:</p> <pre><code>user@host$ qsub -P \"Project 1\" -- /bin/echo Hello\nqsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt\n</code></pre> <p>This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit <code>/apps/soca/cluster_manager/settings/project_cost_manager.txt</code> and configure your budget for this user:</p> <pre><code># This file is used to prevent job submission when budget allocated to a project exceed your threshold\n# This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py\n# Help &amp; Documentation: https://soca.dev/tutorials/set-up-budget-project/\n#\n#\n# Syntax:\n#   [project 1]\n#   user1\n#   user2\n#   [project 2]\n#   user1\n#   user3\n#   [project blabla]\n#   user4\n#   user5\n\n[Project 1]\nmickael\n</code></pre> <p>Important</p> <p>The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive)</p> <p>Save this file and try to submit a job, this time the job should go to the queue</p> <pre><code>user@host$ qsub -P \"Project 1\" -- /bin/echo Hello\n5.ip-10-0-1-223\n</code></pre>"},{"location":"budget/set-up-budget-project/#submit-a-job-when-budget-is-invalid","title":"Submit a job when budget is invalid","text":"<p>Now let's go back to your AWS budget, and let's simulate we are over-budget</p> <p></p> <p>Now try to submit a job for \"Project 1\", your request should be rejected <pre><code>user@host$ qsub -P \"Project 1\" -- /bin/echo Hello\nqsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash\n</code></pre></p> <p>The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P )</p> <p>As mentioned above, the project name on <code>/apps/soca/cluster_manager/settings/project_cost_manager.txt</code> and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: <pre><code>bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello\nqsub: Error. Unable to query AWS Budget API. ERROR: An error occurred (NotFoundException) when calling the DescribeBudget operation: [Exception=NotFoundException] Failed to call DescribeBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Failed to call GetBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Unable to get budget: Project 2 - the budget doesn't exist.\n</code></pre></p>"},{"location":"debug/","title":"About","text":"<p>This section shares how to debug issues in SOCA.</p> <p>Refer to the left sidebar for more detailed resources.</p>"},{"location":"debug/desktop-instance/","title":"Desktop Instance Debug","text":""},{"location":"debug/desktop-instance/#session-doesnt-start","title":"Session Doesn't Start","text":"<p>Check that efs file systems are mounted at /data and /apps.</p> <pre><code>&gt; df -h /data /apps\nFilesystem                                 Size  Used Avail Use% Mounted on\nfs-nnnnnnnn.efs.eu-west-1.amazonaws.com:/  8.0E  5.0M  8.0E   1% /data\nfs-nnnnnnnn.efs.eu-west-1.amazonaws.com:/  8.0E  1.1G  8.0E   1% /apps\n</code></pre> <p>Check the instance's bootstrap logs:</p> <pre><code>/apps/soca/&lt;clusterid&gt;/cluster_node_bootstrap/logs/desktop/&lt;user&gt;/LinuxDesktop*/ip-*/*\n</code></pre> <p>Connect to the instance and make sure that the dcv session started:</p> <pre><code>&gt; dcv list-sessions\n</code></pre> <p>Check the application log for errors:</p> <pre><code>/apps/soca/&lt;clusterid&gt;/cluster_web_ui/logs/application.log\n</code></pre> <p>If successfull you'll see an entry like the following with a 200 return code. Look for other other errors to debug launch failures.</p> <pre><code>[2020-12-28 16:05:53,150] [INFO] [remote_desktop] [Checking https://soca-&lt;cluster-id&gt;-viewer-&lt;account-id&gt;.&lt;region&gt;.elb.amazonaws.com/ip-nn-nn-nn-nn/ for &lt;LinuxDCVSessions 1&gt; and received status 200 ]\n</code></pre> <p>Check if can connect to the instance from the scheduler:</p> <pre><code>$ curl -k -I &lt;WebUserInterface-URL&gt;/ip-nn-nn-nn-nn\nroot@ip-10-0-0-27 logs]# curl -k -I https://soca-cdkpriv2a-viewer-667237311.us-east-1.elb.amazonaws.com/ip-10-0-173-237/\nHTTP/2 200\ndate: Wed, 02 Feb 2022 18:45:28 GMT\ncontent-type: text/html; charset=\"utf-8\"\ncontent-length: 439\nserver: dcv\nx-frame-options: DENY\nstrict-transport-security: max-age=31536000; includeSubDomains\n</code></pre>"},{"location":"security/","title":"About","text":"<p>This section share all security best practices you should be aware of when using SOCA.</p> <p>Refer to the left sidebar for more detailed resources</p>"},{"location":"security/backup-restore-your-cluster/","title":"Backup your entire cluster automatically","text":"<p>By default, Scale-Out Computing on AWS automatically backup your EC2 scheduler and EFS filesystems every day and keep the backups for 30 days using AWS Backup. During the installation, Scale-Out Computing on AWS creates a new backup vault and one backup plan. You can edit them if needed.</p>"},{"location":"security/backup-restore-your-cluster/#what-is-aws-backups","title":"What is AWS Backups?","text":"<p>AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud.  Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems.</p>"},{"location":"security/backup-restore-your-cluster/#backup-vault","title":"Backup Vault","text":"<p>A \"Backup Vault\" is where all your backups are stored. Your vault is automatically encrypted using your Key Management Service (KMS) key and reference to your SOCA cluster ID (<code>soca-mycluster</code> in this example)</p> <p></p>"},{"location":"security/backup-restore-your-cluster/#backup-plan","title":"Backup Plan","text":"<p>A \"Backup Plan\" is where you define all your backup strategy such as backup frequency, data retention or resource assignments.</p> <p></p>"},{"location":"security/backup-restore-your-cluster/#backup-rules-red-section","title":"Backup rules (red section)","text":"<p>By default, Scale-Out Computing on AWS creates one backup rule with the following parameters:</p> <ul> <li>Backup will start every day between 5AM and 6AM UTC (blue section)</li> <li>Backup will expire after 1 month (orange section)</li> <li>Backup is stored on the encrypted vault created by SOCA (purple section)</li> </ul> <p></p> <p>If needed, you can edit this rule (or create a new one) to match your company backup strategy.</p>"},{"location":"security/backup-restore-your-cluster/#resources-assignments-green-section","title":"Resources Assignments (green section)","text":"<p>By default, Scale-Out Computing on AWS backup all data using the <code>soca:BackupPlan</code> tag. Value of this tag must match your cluster ID (<code>soca-mycluster</code> in this example).</p> <p></p> <p>Supported Resources</p> <p>AWS Backup only support the following resources:</p> <ul> <li>EC2 instances</li> <li>EBS disks</li> <li>EFS filesystems</li> <li>RDS (not used by SOCA)</li> <li>DynamoDB (not used by SOCA)</li> </ul>"},{"location":"security/backup-restore-your-cluster/#how-to-addremove-resources-to-the-backup-plan","title":"How to add/remove resources to the backup plan","text":"<p>Backup resources are managed by <code>soca:BackupPlan</code> tag. The value of this tag must match the value of your Backup Plan (by default it should match the name of your cluster). Apply this tag to any EC2 instance, EBS volumes or EFS filesystem you want to backup.</p> <p></p>"},{"location":"security/backup-restore-your-cluster/#how-to-restore-a-backup","title":"How to restore a backup?","text":"<p>On the left sidebar, click \"Protected Resources\" then choose the resource you want to restore</p> <p></p> <p>This will open a new window with additional information about this resource (either EFS or EC2). Select the latest entry you want to restore from the \"Backups\" section then click \"Restore\"</p> <p></p> <p>This will open a regular EC2 launch instance or EFS wizard. Specify the parameters (VPC, Subnet, Security Group, IAM role ...) you want to use and click \"Restore Backup\"</p> <p>Restore Role</p> <ul> <li>By default, you can only apply <code>SchedulerIAMRole</code> or <code>ComputeNodeIAMRole</code> to the EC2 resource you are restoring. If you want to restore one EC2 instance with a different role, you must edit <code>iam:PassRole</code> policy of your <code>SOCA-Backup</code> role.</li> <li>Make sure to use the <code>SOCA-Backup</code> IAM role created by SOCA during initial installation. If you want to use the default role created by AWS Backup, make sure to add <code>iam:PassRole</code> permission.</li> </ul>"},{"location":"security/backup-restore-your-cluster/#how-to-delete-a-backup","title":"How to delete a backup ?","text":"<p>Select your vault, choose which recovery point you want to remove under the \"Backups\" section then click \"Delete\".</p> <p></p>"},{"location":"security/backup-restore-your-cluster/#check-the-status-of-the-backup-jobs","title":"Check the status of the backup jobs","text":"<p>On the left sidebar, check \"Jobs\" to verify if your backup jobs are running correctly</p> <p></p>"},{"location":"security/backup-restore-your-cluster/#what-happen-if-you-delete-your-cloudformation-stack","title":"What happen if you delete your Cloudformation stack?","text":"<p>Your backup vault won't be deleted if you have active backups in it. In case of accidental termination of your primary cloudformation template, you will still be able to recover your data by restoring the EFS and/or EC2. To delete your AWS Backup entry, you first need to manually remove all backups present in your vault.</p>"},{"location":"security/instance_access/","title":"Instance Access","text":"<p>Administrators can access instances using AWS Systems Manager Session Manager or using SSH to connect via the network bastion.</p>"},{"location":"security/instance_access/#instance-access-using-session-manager","title":"Instance Access Using Session Manager","text":"<p>Go to the EC2 console, right click on the instance and select Connect. Select Session Manager and click Connect. This will open a terminal in a browser tab as ssm-user which has sudo access on the instance.</p> <p>If Session Manager cannot connect to the instance it may be a problem with the amazon-ssm-agent on the instance. In that case you will have to connect to the instance using SSH via the bastion.</p>"},{"location":"security/instance_access/#instance-access-using-ssh","title":"Instance Access Using SSH","text":"<p>Only administrators that have the private key of the EC2 KeyPair can connect to the bastion using SSH. The bastion host is in a private subnet behind a Network Load Balancer. You can get the DNS name of the loadbalancer by looking at the BastionDnsName output of the CloudFormation stack. You can also get the name using the AWS CLI.</p> <p><code>bastionDns=$(aws cloudformation describe-stacks --stack-name &lt;stack-name&gt; --query 'Stacks[*].Outputs[?OutputKey==`BastionDnsName`].OutputValue' --output text)</code></p> <p>You should connect to the bastion using agent forwarding so that you can ssh from the bastion to other instances in the VPC.</p> <p><code>ssh -A -i privatekey.pem ec2-user@$bastionDns</code></p> <p>On Windows you can use Pageant for agent forwarding.  On linux systems the following command will load your private key.</p> <pre><code>ssh-add privatekey.pem\nssh -A ec2-user@$bastionDns\n</code></pre> <p>Once you are on the bastion you can ssh to any instance in the VPC.</p> <pre><code>ssh -A ec2-user@$bastionDns\nssh proxy.soca.local\n</code></pre> <p>You can configure ssh to automatically connect to another instance in the VPC via the bastion by adding the following lines to you ~/.ssh/config.</p> <pre><code>Host soca-*\n     ForwardAgent yes\n     User ec2-user\n     ProxyJump ec2-user@&lt;BastionDnsName&gt;:22\n\nHost soca-proxy\n     Hostname proxy.soca.local\n</code></pre> <p>With that added to your SSH config you can connect to the proxy instance with one command.</p> <pre><code>ssh-add &lt;pem-file&gt;\nssh soca-proxy\n</code></pre>"},{"location":"security/integrate-cognito-sso/","title":"Enable Oauth2 authentication with Cognito","text":"<p>On this page, we will see how you can automatically authenticate your users to Scale-Out Computing on AWS using without having them to enter their password.</p>"},{"location":"security/integrate-cognito-sso/#what-is-cognito-oauth2","title":"What is Cognito / Oauth2","text":"<p>With Amazon Cognito, your users can sign-in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory using SAML. Amazon Cognito User Pools provide a secure user directory that scales to hundreds of millions of users. As a fully managed service, User Pools are easy to set up without any worries about server infrastructure. User Pools provide user profiles and authentication tokens for users who sign up directly and for federated users who sign in with social and enterprise identity providers.</p> <p>Additionally, read this link  if you are not already familiar with <code>Oauth2</code> workflow.</p>"},{"location":"security/integrate-cognito-sso/#how-it-works","title":"How it works ?","text":"<ul> <li>1: Mary has an account on her corporate LDAP or Active Directory. This account has an username (e.g mary), an email (e.g mary@company.com) and other parameters (cost center, location ... ). She uses her account to log in to her corporate network.</li> <li>2: Mary wants to access the web UI of SOCA (we assume she already has an active account on SOCA. If not, refer to this page to learn how to manage user account on SOCA)<ul> <li>2.1: She can access the application by entering her SOCA LDAP username/password</li> <li>2.2: She can be automatically logged in using Amazon Cognito</li> </ul> </li> <li>3: Assuming SSO is enabled, SOCA will forward the access request Cognito which will use Mary's Corporate LDAP as a Federated identity to determine if she is a valid user. This is the authentication part.</li> <li>4: Mary's Corporate LDAP will check her account (e.g based on Kerberos ticket) and return a SAML token. This is the authorization part.</li> <li>5: Based on the authorization results, SOCA will automatically logs Mary in or reject her request</li> </ul> <p>What if Mary does not have an account on SOCA?</p> <p>Assuming a user has a valid credential on corporate LDAP/AD but not on SOCA, Cognito will redirect the user to the default SOCA login portal.</p> <p>How do you determine if a user has an account on SOCA?</p> <p>To verify if a corporate user is active and can log in, SOCA checks whether or not this user has an account. By default, we determine the user account name is the first part of an email address.  For example, if the email returned by your corporate LDAP for a given user is <code>myuser@company.com</code>, we will assume this user the SOCA account is <code>myuser</code>. If this mapping does not apply to your company, you can change it by editing <code>/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/auth.py</code>.</p>"},{"location":"security/integrate-cognito-sso/#create-your-cognito-user-pool","title":"Create your Cognito User Pool","text":"<p>Info</p> <p>This setup is different for each customer, refer fo the official AWS Documentaton for Cognito if needed.</p> <p>First, you need to configure your Cognito User Pool with your existing SAML provider. You can leave most settings by default as we won't be using any user password, custom login UI etc.. This step is optional if you already have a User Pool configured.</p>"},{"location":"security/integrate-cognito-sso/#integrate-your-adldap-with-cognito","title":"Integrate your AD/LDAP with Cognito","text":"<p>Once your User Pool is created, go to <code>Federation &gt; Identity Provider</code> and choose whatever IDP you want to use (most likely SAML if you are looking to integrate your corporate LDAP)</p> <p></p> <p>SAML integration vary based on your own corporate settings, reach out to your local IT if needed. Here are some extra links if you need more documentation related to SAML:</p> <ul> <li>SAML Authentication Flow</li> <li>Adding SAML Identity Providers to a User Pool </li> <li>Create a SAML Provider on Cognito</li> </ul> <p>Warning</p> <p>The only required parameters for Scale-Out Computing on AWS is the <code>email</code> attribute. </p>"},{"location":"security/integrate-cognito-sso/#create-a-cognito-client","title":"Create a Cognito Client","text":"<p>On Cognito interface, click <code>User Pools &gt; Federated Identities</code> then <code>General Settings &gt; App Clients</code> and finally click <code>Add Another App Client</code>.</p> <p>Note your client name, client id and client secret and leave all other parameters by default.</p> <p></p>"},{"location":"security/integrate-cognito-sso/#configure-your-cognito-client","title":"Configure your Cognito Client","text":"<p>Now visit <code>App Integration &gt; App client setting</code> and configure your application with the Identity Provider you just created (<code>Enabled Identity Provider</code>).  You also want to specify the callback url(s) for all domains you are planning to use. The default callback URL you must specify is <code>https://&lt;your_soca_elb_name&gt;/oauth</code> but you can add any other URL if you have multiple environments (e.g: dev/test)</p> <p>Important Info about Callback</p> <p>Update the callback URLs with your custom domain if you are using one. If you don't do that, you will get a Cognito error with \"Redirect URI mismatch\"</p> <p></p>"},{"location":"security/integrate-cognito-sso/#configure-soca","title":"Configure SOCA","text":"<p>Edit <code>/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/parameters.cfg</code> and update the <code>[cognito]</code> section as shown below:</p> <pre><code>[cognito]\n## Cognito\nenable_sso=\"true\" # Set this flag to \"true\"\ncognito_oauth_authorize_endpoint=\"https://&lt;YOUR_COGNITO_POOL&gt;.auth.&lt;YOUR_REGION&gt;.amazoncognito.com/oauth2/authorize\"\ncognito_oauth_token_endpoint=\"https://&lt;YOUR_COGNITO_POOL&gt;.auth.&lt;YOUR_REGION&gt;.amazoncognito.com/oauth2/token\"\ncognito_jws_keys_endpoint=\"https://cognito-idp.&lt;YOUR_REGION&gt;.amazonaws.com/&lt;YOUR_REGION&gt;_&lt;YOUR_POOL_ID&gt;/.well-known/jwks.json\"\ncognito_app_secret=\"&lt;YOUR_APP_SECRET&gt;\"\ncognito_app_id=\"&lt;YOUR_APP_ID&gt;\"\ncognito_root_url=\"&lt;YOUR_WEB_URL&gt;\"\ncognito_callback_url=\"&lt;YOUR_CALLBACK_URL&gt;\"\n</code></pre> <p>Important</p> <p>Make sure to use double quotes for all variables (eg. enable_sso=\"true\" and not enable_sso='true')</p>"},{"location":"security/integrate-cognito-sso/#restart-the-web-ui","title":"Restart the Web UI","text":"<p>Simply restart the Web UI by running:</p> <pre><code>/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh stop\n/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start\n</code></pre> <p>Now try to access <code>https://&lt;YOUR_SOCA_DNS&gt;/</code>, you should be automatically logged in.</p> <p>Note that we keep <code>/login</code> as fallback authentication mechanism to LDAP, so make sure your users access <code>https://&lt;YOUR_SOCA_DNS&gt;/</code> and not <code>https://&lt;YOUR_SOCA_DNS&gt;/login</code> if they want to be automatically logged in with SSO</p>"},{"location":"security/internet-proxy/","title":"Internet Proxy","text":"<p>Scale-Out Computing on AWS filters all outbound internet access using a proxy server.</p>"},{"location":"security/internet-proxy/#default-proxy-rules","title":"Default Proxy Rules","text":"<p>All internet access is restricted unless explicitly allowed.</p> <p>The rules can be customized before deployment by updating the following file:</p> <p>source/proxy/soca.conf</p> <p>This file can also be modified on the proxy at /etc/squid/soca.conf</p> <p>After updating the proxy configuration you must restart the squid service:</p> <p><code>systemctl restart squid; systemctl status -l squid</code></p>"},{"location":"security/limit-concurrent-job-and-instances/","title":"Restrict number of concurrent jobs and/or instances","text":""},{"location":"security/limit-concurrent-job-and-instances/#restrict-number-of-concurrent-running-jobs","title":"Restrict number of concurrent running jobs","text":"<p>Configure <code>max_running_jobs</code> to limit the number of jobs running in parallel for a given queue Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code></p> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    max_running_jobs: 5 \n    ... \n  test:\n    queues: [\"test\"]\n    max_running_jobs: 30\n</code></pre> <p>In this example, only 5 jobs can be running at the same time in the \"normal\" queue, and 30 in the \"test\" queue. If a job cannot start because of this parameter, an <code>error_message</code> will be visible when running \"qstat -f\" or via the web interface</p>"},{"location":"security/limit-concurrent-job-and-instances/#restrict-number-of-provisioned-instances","title":"Restrict number of provisioned instances","text":"<p>Configure <code>max_provisioned_instances</code> to limit the number of instances that can be provisioned for a given queue</p> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    max_provisioned_instances: 10 \n    ... \n  test:\n    queues: [\"test\"]\n    max_provisioned_instances: 20\n</code></pre> <p>In this example, you cannot have more than 10 instances provisioned for \"normal\" queue, and 20 for the \"test\" queue. If a job cannot start because of this parameter, an <code>error_message</code> will be visible when running \"qstat -f\" or via the web interface</p> <p>Info</p> <p>Limit apply to all type of instance. Configure <code>allowed_instance_types</code> / <code>excluded_instance_types</code> if you also want to limit the type of EC2 instance than can be provisioned).</p>"},{"location":"security/manage-queue-acls/","title":"Manage Access Lists at queue level","text":"<p>You can manage ACLs for each queue by configuring both <code>allowed_users</code> or <code>excluded_users</code>. </p> <p>These parameters can be configured as:</p> <ul> <li>List of allowed/excluded users: <code>allowed_users: [\"user1\", \"user2\"]</code></li> <li>List of LDAP groups: <code>allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"]</code></li> <li>List of username and LDAP groups: <code>allowed_users: [\"user1\", \"cn=mynewgroup,ou=Group,dc=soca,dc=local\", \"user2\"]</code></li> </ul>"},{"location":"security/manage-queue-acls/#restrict-queue-for-some-users","title":"Restrict queue for some users","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    allowed_users: [] # empty list = all users can submit job \n    excluded_users: [] # empty list = no restriction, [\"*\"] = only allowed_users can submit job\n    ... \n  test:\n    queues: [\"high\", \"low\"]\n    allowed_users: [] \n    excluded_users: [\"user1\"] \n</code></pre></p> <p>In this example, <code>user1</code> can submit a job to \"normal\" queue but not on \"high\" or \"low\" queues.</p> <pre><code># Job submission does not work on \"high\" queue because user1 is on the excluded_users list pattern\nqsub -q high -- /bin/sleep 60\nqsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n\n# Job submission is ok on \"normal\" queue\nqsub -q normal -- /bin/sleep 60\n19.ip-30-0-2-29\n</code></pre> <p><code>allowed_users</code> overrides <code>excluded_users</code></p> <p>Job will go through if a user is present is both <code>allowed_users</code> and <code>excluded_users</code> lists.</p>"},{"location":"security/manage-queue-acls/#restrict-the-queue-for-everyone-except-allowed_users","title":"Restrict the queue for everyone except <code>allowed_users</code>","text":"<p><code>excluded_users: [\"*\"]</code> will prevent anyone to use the queue except for the list of <code>allowed_users</code>. </p> <p>In the example below, <code>user1</code> is the only user authorized to submit job.</p> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    allowed_users: [\"user1\"]\n    excluded_users: [\"*\"]\n</code></pre>"},{"location":"security/manage-queue-acls/#manage-acls-using-ldap-groups","title":"Manage ACLs using LDAP groups","text":"<p>SOCA will consider <code>allowed_users</code> or <code>excluded_users</code> as LDAP group if you did not specify them as list.</p> <p>Create a text file <code>mynewgroup.ldif</code> and add the following content (note we are adding our <code>user1</code> as a group member)</p> <pre><code>dn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nobjectClass: top\nobjectClass: posixGroup\ncn: mynewgroup\ngidNumber: 6000\nmemberUid: user1\n</code></pre> <p>Create the group using <code>ldapadd</code> command</p> <pre><code>~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif\nadding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\"\n</code></pre> <p>Run <code>ldapsearch</code> command to confirm your group has been created correctly and your <code>user1</code> is part of it</p> <pre><code>~ ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL\ndn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nobjectClass: top\nobjectClass: posixGroup\ncn: mynewgroup\ngidNumber: 6000\nmemberUid: user1\n</code></pre> <p>Let's configure our queue to reject all users:</p> <pre><code>allowed_users: []\nexcluded_users: [\"*\"]\n</code></pre> <p>Confirm <code>user1</code> can't submit any job:</p> <pre><code>qsub -q high -- /bin/sleep 60\nqsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml\n</code></pre> <p>Edit your <code>allowed_users</code> and specify your LDAP group:</p> <pre><code>allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"]\nexcluded_users: [\"*\"]\n</code></pre> <p>Verify <code>user1</code> can submit job: <pre><code>qsub -q high -- /bin/sleep 60\n22.ip-30-0-2-29\n</code></pre></p> <p>Let's now assume you have a <code>user2</code>. Confirm this user can't submit job</p> <pre><code>qsub -q high -- /bin/sleep 60\nqsub: user2 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml\n</code></pre> <p>Create a new ldif file (add_new_user.ldif) and add <code>user2</code> to your group <pre><code>dn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nchangetype: modify\nadd: memberUid\nmemberUid: user2\n</code></pre></p> <p>Execute the command <pre><code>~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f add_new_user.ldif\nmodifying entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\n</code></pre></p> <p>Confirm both users are part of the group: <pre><code>ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL\ndn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nobjectClass: top\nobjectClass: posixGroup\ncn: mynewgroup\ngidNumber: 6000\nmemberUid: user1\nmemberUid: user2\n</code></pre></p> <p>Finally, confirm <code>user2</code> is now authorized to submit job: <pre><code>qsub -q high -- /bin/sleep 60\n23.ip-30-0-2-29\n</code></pre></p> <p>On the other side, you can also prevent users from a LDAP group to use the queue by specifying the ldap group as \"excluded_users\" <pre><code>allowed_users: []\nexcluded_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"]\n</code></pre></p>"},{"location":"security/manage-queue-acls/#check-the-logs","title":"Check the logs","text":"<p>Scheduler hooks are located on /var/spool/pbs/server_logs/</p>"},{"location":"security/manage-queue-acls/#code","title":"Code","text":"<p>The hook file can be found under <code>/apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_acls.py</code> on your Scale-Out Computing on AWS cluster)</p>"},{"location":"security/manage-queue-acls/#disable-the-hook","title":"Disable the hook","text":"<p>You can disable the hook by running the following command on the scheduler host (as root):</p> <pre><code>user@host: qmgr -c \"delete hook check_queue_acls event=queuejob\"\n</code></pre>"},{"location":"security/manage-queue-instance-types/","title":"Restrict provisioning of specific instance type","text":"<p>You can manage the EC2 instance types allowed for each queue by configuring both  <code>allowed_instance_types</code> and <code>excluded_instance_types</code>.</p> <p>This allows you to restrict which instance types or family of instances are allowed to be used for jobs on a per queue basis by either white listing instance types or blocking them.</p> <p>Default settings</p> <p>By default, users can provision any type of instance. There are no restrictions configured out of the box.</p> <p>These parameters can be configured as:</p> <ul> <li>List of allowed EC2 instance types for a queue: <code>allowed_instance_types: [\"c5.4xlarge\", \"r5.2xlarge\"]</code></li> <li>List of excluded EC2 instance types for a queue: <code>excluded_instance_types: [\"f1.16xlarge\", \"i3.2xlarge\"]</code></li> <li>Allow EC2 instance types by specific type or by instance family: <code>allowed_instance_types: [\"c5\", \"r5.2xlarge\"]</code></li> <li>Exclude EC2 instance types by specific type or by instance family: <code>excluded_instance_types: [\"f1.16xlarge\", \"i3\"]</code></li> </ul> <p>Instance family specification uses the exact name of the instance family.</p> <p>If you add <code>c5</code> to the allowed instance list c5 instances will be allowed.  c5n instances will be blocked unless <code>c5n</code> is added to the <code>allowed_instance_types</code> to allow c5n instances to run.</p>"},{"location":"security/manage-queue-instance-types/#allow-only-compute-optimized-ec2-instances","title":"Allow only compute optimized EC2 instances","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    allowed_instance_types: [\"c5\", \"c5n\"] \n    excluded_instance_types: []\n    ... \n  test:\n    queues: [\"test\"]\n    allowed_instance_types: [\"c5.4xlarge\"] \n    excluded_instance_types: [] \n</code></pre></p> <p>In this example, only EC2 instance types in the c5 and c5n families can be used for jobs submitted to the normal queue.  For the test queue only c5.4xlarge instance type will be allowed. </p> <pre><code># Job submission to queue \"normal\" using instance type i3.2xlarge is blocked\nqsub -q normal -l instance_type=i3.2xlarge -- /bin/echo test\nqsub: i3.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n\n# Job submission to queue \"normal\" using instance from c5 family is allowed.\nqsub -q normal -l instance_type=c5.2xlarge -- /bin/echo test\n15.ip-110-0-12-28\n\n# Job submission to \"test\" queue only allowed if using c5.4xlarge instance type\nqsub -q test -l instance_type=c5.2xlarge -- /bin/echo test\nqsub: c5.2xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n\nqsub -q test -l instance_type=c5.4xlarge -- /bin/echo test\n16.ip-110-0-12-28\n</code></pre> <p>Instance types in <code>excluded_instance_types</code> will be blocked even if they appear in <code>allowed_instance_types</code></p> <p>If an instance type or family appears in both the <code>excluded_instance_types</code>list as well as <code>allowed_instance_types</code> for a queue, the <code>excluded_instance_types</code> setting takes priority.</p>"},{"location":"security/manage-queue-instance-types/#block-users-from-using-specific-ec2-instance-types","title":"Block users from using specific EC2 instance types","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\"]\n    allowed_instance_types: [] \n    excluded_instance_types: [\"f1\",\"g4.16xlarge\",\"g3\"]\n    ... \n  test:\n    queues: [\"test\"]\n    allowed_instance_types: [] \n    excluded_instance_types: [\"f1\",\"g4dn\"]\n</code></pre> In this example the normal queue will not allow instances in the f1 and g3 family as well as g4.16xlarge instanct types.  The test queue will not allow instances in the f1 and g4 family.</p> <pre><code># Job submission to queue \"normal\" using instance type in f1 family is blocked\nqsub -q normal -l instance_type=f1.2xlarge -- /bin/echo test\nqsub: f1.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n\n# Job submission to queue \"test\" using instance type in g4dn is blocked\nqsub -q normal -l instance_type=g4dn.xlarge -- /bin/echo test\nqsub: g4dn.xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n</code></pre>"},{"location":"security/manage-queue-instance-types/#check-the-logs","title":"Check the logs","text":"<p>Scheduler hooks are located on /var/spool/pbs/server_logs/</p>"},{"location":"security/manage-queue-instance-types/#code","title":"Code","text":"<p>The hook file can be found under <code>/apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_instance_types.py</code> on your Scale-Out Computing on AWS cluster)</p>"},{"location":"security/manage-queue-instance-types/#disable-the-hook","title":"Disable the hook","text":"<p>You can disable the hook by running the following command on the scheduler host (as root):</p> <pre><code>user@host: qmgr -c \"delete hook check_queue_instance_types event=queuejob\"\n</code></pre>"},{"location":"security/manage-queue-restricted-parameters/","title":"Prevent user to change specific parameters","text":"<p>When submitting a job, users can override the default parameters configured for the queue (click here to see a list of all parameters supported by SOCA).</p> <p>For security, compliance or cost reasons, you may want prevent users to override these default parameters by configuring <code>restricted_parameters</code> on your <code>queue_mapping.yml</code></p>"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-choose-a-different-instance-type","title":"Prevent user to choose a different instance type","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code></p> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\", \"low\"]\n    instance_type: \"c5.large\"\n    restricted_parameters: [\"instance_type\"]\n    ...\n</code></pre> <p>In this example, a job will be rejected if a user try to specify the <code>instance_type</code> parameter when using the <code>normal</code> or <code>low</code> queues. In this particular case, any job sent to the <code>normal</code> or <code>low</code> queue will be forced to use <code>c5.large</code> instance, which is the default instance type configured by HPC admins. </p> <pre><code>qsub -q normal -l instance_type=m5.24xlarge -- /bin/echo test\nqsub: instance_type is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n</code></pre> <p>Need to whitelist more than one instance type/family?</p> <p>Read the documentation if you want to limit users to a list of multiple instance types</p>"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-provision-additional-storage","title":"Prevent user to provision additional storage","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code></p> <p><pre><code>queue_type:\n  compute:\n    queues: [\"normal\", \"low\"]\n    scratch_size: \"200\"\n    restricted_parameters: [\"scratch_size\"]\n    ... \n</code></pre> In this example, a job will be rejected if a user try to specify the <code>scratch_size</code> parameter when using the <code>normal</code> or <code>low</code> queues. In this particular case, any job sent to the <code>normal</code> or <code>low</code> queue will be forced to use a 200 GB EBS disk as <code>/scratch</code> partition. Users are no longer able to provision more storage than what's allocated to them.</p> <pre><code>qsub -q normal -l scratch_size=550 -- /bin/echo test\nqsub: scratch_size is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n</code></pre>"},{"location":"security/manage-queue-restricted-parameters/#combine-multiple-restrictions","title":"Combine multiple restrictions","text":"<p>Considering <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code></p> <pre><code>queue_type:\n  compute:\n    queues: [\"normal\", \"low\"]\n    scratch_size: \"200\"\n    restricted_parameters:[\"instance_type\", \"fsx_lustre_bucket\", \"scratch_size\"]\n    ...\n</code></pre> <p>In this example, a job will be rejected if a user try to change either <code>instance_type</code>, <code>fsx_lustre_bucket</code> or <code>scratch_size</code> parameters.</p> <pre><code>qsub -q normal -l fsx_lustre_bucket=mybucket -- /bin/echo test\nqsub: fsx_lustre_bucket is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml\n</code></pre>"},{"location":"security/manage-queue-restricted-parameters/#check-the-logs","title":"Check the logs","text":"<p>Scheduler hooks are located on /var/spool/pbs/server_logs/</p>"},{"location":"security/manage-queue-restricted-parameters/#code","title":"Code","text":"<p>The hook file can be found under <code>/apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_restricted_parameters.py</code> on your Scale-Out Computing on AWS cluster)</p>"},{"location":"security/manage-queue-restricted-parameters/#disable-the-hook","title":"Disable the hook","text":"<p>You can disable the hook by running the following command on the scheduler host (as root):</p> <pre><code>user@host: qmgr -c \"delete hook check_queue_restricted_parameters event=queuejob\"\n</code></pre>"},{"location":"security/network-debug/","title":"Network Debug","text":"<p>Scale-Out Computing on AWS configures all security groups (firewalls) and the internet proxy using the principal of least privilege. This means that all network traffic is restricted unless specifically allowed by the security groups (firewalls) and internet proxy. If you suspect that traffic is being blocked that should be allowed then you can debug network traffic using CloudWatch VPC flow logs and proxy logs.</p>"},{"location":"security/network-debug/#proxy-access-log","title":"Proxy Access Log","text":"<p>Connect to the proxy server using Systems Manager or SSH and search <code>/var/log/squid/access.log</code> for the string DENIED. The log will show the time, the source IP address and the destination.</p>"},{"location":"security/network-debug/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>The easiest way to debug network traffic that is blocked by security groups is to search the VPC flow logs using CloudWatch Insights.</p> <p>Go to the CloudWatch Logs Insights console. Select the region where your cluster is deployed and select it's VPC Flow Logs group. Update the query to select the packets you are interested in and run the query. The following shows example of the query syntax.</p> <pre><code>fields @timestamp, srcAddr, srcPort, dstAddr, dstPort, action\n| sort @timestamp desc\n| filter action=\"REJECT\"\n#| filter srcPort=\"443\"\n#| filter dstPort=\"443\"\n#| filter dstAddr like \"10.0.\"\n#| filter dstAddr=\"10.0.169.50\"\n#| filter srcAddr=\"10.0.22.9\" or dstAddr=\"10.0.175.175\"\n</code></pre>"},{"location":"security/update-soca-dns-ssl-certificate/","title":"Change your DNS name and SSL certificate","text":"<p>By default, Scale-Out Computing on AWS will use a non-friendly DNS name and create a unique certificate to enable access through your HTTPS endpoint. Because it's a self-signed certificate, browsers won't recognized it and you will get a security warning on your first connection. </p> <p></p> <p>In this page, we will see how you can update Scale-Out Computing on AWS to match your company domain name.</p>"},{"location":"security/update-soca-dns-ssl-certificate/#create-a-new-dns-record-for-scale-out-computing-on-aws","title":"Create a new DNS record for Scale-Out Computing on AWS","text":"<p>For this example, let's assume I want to use <code>https://demo.soca.dev</code>. First locate the DNS associated to your ALB endpoint using the AWS console.</p> <p></p> <p>Create a new <code>CNAME</code> record which point to your ALB endpoint. Once done, validate your DNS is working properly using the <code>nslookup</code> command.</p> <pre><code>user@host: nslookup demo.soca.dev\nNon-authoritative answer:\ndemo.soca.dev   canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com.\nName:   soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com\nAddress: 52.40.2.185\nName:   soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com\nAddress: 54.68.240.4\nName:   soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com\nAddress: 52.27.180.89\n</code></pre>"},{"location":"security/update-soca-dns-ssl-certificate/#upload-your-ssl-certificate-to-acm","title":"Upload your SSL certificate to ACM","text":"<p>Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...)</p> <p>To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\".</p> <p></p> <p>Enter your private key, certificate and certificate chain (optional), then click Import.</p> <p></p> <p>Once the import is complete, note your certificate identifier.</p> <p></p>"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-alb-with-the-new-certificate","title":"Update your ALB with the new certificate","text":"<p>Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button.</p> <p></p> <p>Change the default certificate to point to your new certificate and save your change.</p> <p></p>"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-default-domain-for-dcv","title":"Update your default domain for DCV","text":"<p>Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". </p> <p>Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save</p> <p></p>"},{"location":"security/update-soca-dns-ssl-certificate/#validate-everything","title":"Validate everything","text":"<p>Now that you have your friendly DNS and SSL certificate configured, it's time to test.</p> <p>Visit your new DNS (<code>https://demo.soca.dev</code> in my case) and make sure you can access Scale-Out Computing on AWS correctly.</p> <p></p> <p>Make sure your browser is detecting your new SSL certificate correctly.</p> <p></p> <p>Finally, create a new DCV session and verify the endpoint is using your new DNS name</p> <p></p>"},{"location":"security/yum_repo_mirror/","title":"Create Private Yum Repos","text":"<p>If security requires no internet access then you need a way to install and update packages. In this scenario you should deploy the MirrorRepos stack before deploying SOCA.</p> <p>Create an S3 bucket in the same region where you will deploy SOCA. Download the SOCA package and change to the MirrorRepos directory. Then deploy the stack using the S3 bucket you created.</p> <pre><code>cd  Solution-for-scale-out-computing-on-aws/source/MirrorRepos\nmake STACK_NAME=MirrorRepos S3_MIRROR_REPO_BUCKET=&lt;bucket-name&gt; S3_MIRROR_REPO_FOLDER=repositories upload create\n</code></pre> <p>This stack uses Amazon CodeBuild to copy the source code that SOCA uses and the CentOS 7 repos into your S3 bucket where it can be used by your SOCA cluster. You can monitor the status of the process by going to the outputs of your stack and clicking on the  MirrorCentos7ReposBuildUrl output. When the build completes the S3 path of the mirror will be at the end of the logs and you will use that as the RepositoryBucket and RepositoryFolder parameters of the SOCA stack when you deploy it. This will configure SOCA to use your private S3 mirror instead of public repositories.</p>"},{"location":"storage/","title":"About","text":"<p>This section list all storage options available on SOCA (EFS, EBS, FSx ...)</p> <p>Refer to the left sidebar for more detailed resources</p>"},{"location":"storage/backend-storage-options/","title":"Understand backend storage","text":"<p>Scale-Out Computing on AWS gives you the flexibility to customize your storage backend based on your requirements</p> <ul> <li>You can customize the root partition size</li> <li>You can provision a local scratch partition</li> <li>You can deploy standard SSD (gp2) or IO Optimized SSD (io1) volumes</li> <li>Scale-Out Computing on AWS automatically leverages instance store disk(s) as scratch partition when applicable</li> <li>In term of performance: Instance Store &gt; EBS SSD IO &gt; EBS SSD Standard &gt; EFS</li> </ul> <p>Refer to this link to learn more about EBS volumes.</p>"},{"location":"storage/backend-storage-options/#efs-shared-partitions","title":"EFS (shared) partitions","text":""},{"location":"storage/backend-storage-options/#data-partition","title":"/data partition","text":"<p>/data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = <code>/data/home/&lt;USERNAME&gt;</code>). This partition is persistent. Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)</p>"},{"location":"storage/backend-storage-options/#apps-partition","title":"/apps partition","text":"<p>/apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent. Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)</p> <p>The corresponding EFS file system is deployed with bursting Throughput mode which depends on burst credits. /apps has cron jobs that monitor the status of the cluster, scale-up and scale-down the cluster, and is also running the web application. If the cluster stays up and running for 1+ months and not much additional applications are installed under /apps, then EFS file system might run out of burst credits which could impact the web application. Starting v2.6.0, the solution deploys CloudWatch Alarms to monitor the burst credits for /apps EFS file system. When burst credits are close to be depleted, a lambda function is triggered to change the Throughput mode to provisioned at 5 MiB/sec. This increases the monthly cost of EFS by ~$30/month (for us-west-2 and us-east-1 regions). After some time, the file system would have earned enough burst credits, so the lambda function changes Throughput mode back to bursting as it is more cost effective. Click here to learn more about EFS Throughput modes</p>"},{"location":"storage/backend-storage-options/#fsx","title":"FSx","text":"<p>Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs.</p>"},{"location":"storage/backend-storage-options/#instance-local-partitions","title":"Instance (local) partitions","text":"<p>Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing <code>ComputeNode.sh</code> script to match your requirements.</p>"},{"location":"storage/backend-storage-options/#root-partition","title":"Root partition","text":"<p>By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition.  To expand the size of the volume, submit a simulation using <code>-l root_size=&lt;SIZE_IN_GB&gt;</code> parameter.</p> <pre><code>user@host:qsub -l root_size=25 -- /bin/sleep 600\n</code></pre> <p>Result: Root partition is now 25GB</p> <pre><code>user@host: lsblk\nNAME          MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\nnvme0n1       259:0    0  25G  0 disk\n\u251c\u2500nvme0n1p1   259:1    0  25G  0 part /\n\u2514\u2500nvme0n1p128 259:2    0   1M  0 part\n\nuser@host: df -h /\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p1   25G  2.2G   23G   9% /\n</code></pre>"},{"location":"storage/backend-storage-options/#scratch-partition","title":"Scratch Partition","text":"<p>Info</p> <ul> <li>It's recommended to provision <code>/scratch</code> directory whenever your simulation is I/O intensive.</li> <li><code>/scratch</code> is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME</li> <li><code>/scratch</code> is automatically created when Instance supports local ephemeral storage</li> </ul>"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-ssd-disk","title":"Request a /scratch partition with SSD disk","text":"<p>During job submission, specify <code>-l scratch_size=&lt;SIZE_IN_GB&gt;</code> to provision a new EBS disk (<code>/dev/sdj</code>) mounted as <code>/scratch</code></p> <pre><code>user@host: qsub -l scratch_size=150 -- /bin/sleep 600\n</code></pre> <p>Result: a 150 GB /scratch partition is available on all nodes</p> <pre><code>user@host: lsblk\nNAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nnvme1n1       259:0    0  150G  0 disk /scratch\nnvme0n1       259:1    0   10G  0 disk\n\u251c\u2500nvme0n1p1   259:2    0   10G  0 part /\n\u2514\u2500nvme0n1p128 259:3    0    1M  0 part\n\nuser@host: df -h /scratch\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1    148G   61M  140G   1% /scratch\n</code></pre> <p>To verify the type of your EBS disk, simply go to your AWS bash &gt; EC2 &gt; Volumes and verify your EBS type is \"gp2\" (SSD).  Refer to this link for more information about the various EBS types available.</p> <p></p>"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-io-optimized-disk","title":"Request a /scratch partition with IO optimized disk","text":"<p>To request an optimized SSD disk, use <code>-l scratch_iops=&lt;IOPS&gt;</code> along with <code>-l scratch_size=&lt;SIZE_IN_GB&gt;</code>. Refer to this link to get more details about burstable/IO EBS disks. <pre><code>user@host: qsub -l scratch_iops=6000 -l scratch_size=200 -- /bin/sleep 600\n</code></pre></p> <p>Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission.</p> <p></p>"},{"location":"storage/backend-storage-options/#instance-store-partition","title":"Instance store partition","text":"<p>Free storage is always good</p> <p>You are not charged for instance storage (included in the price of the instance)</p> <p>Some instances come with default instance storage. An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted.</p> <p>Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify <code>-l scratch_size</code> parameter for your job. In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s).</p>"},{"location":"storage/backend-storage-options/#when-node-has-1-instance-store-volume","title":"When node has 1 instance store volume","text":"<p>For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk.</p> <pre><code>user@host: qsub -l instance_type=c5d.9xlarge -- /bin/sleep 600\n</code></pre> <p>Result: Default /scratch partition has been provisioned automatically using local instance storage</p> <pre><code>user@host: lsblk\nNAME          MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nnvme1n1       259:0    0 838.2G  0 disk /scratch\nnvme0n1       259:1    0    10G  0 disk\n\u251c\u2500nvme0n1p1   259:2    0    10G  0 part /\n\u2514\u2500nvme0n1p128 259:3    0     1M  0 part\n\nuser@host: df -h /scratch\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1    825G   77M  783G   1% /scratch\n</code></pre>"},{"location":"storage/backend-storage-options/#when-node-has-more-than-1-instance-store-volumes","title":"When node has more than 1 instance store volumes","text":"<p>In this special case, <code>ComputeNode.sh</code> script will create a raid0 partition using all instance store volumes available.</p> <p>For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb).</p> <pre><code>user@host: qsub -l instance_type=m5dn.12xlarge -- /bin/sleep 600\n</code></pre> <p>Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes)</p> <pre><code>user@host: lsblk\nNAME          MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT\nnvme1n1       259:0    0 838.2G  0 disk\n\u2514\u2500md127         9:127  0   1.7T  0 raid0 /scratch\nnvme2n1       259:1    0 838.2G  0 disk\n\u2514\u2500md127         9:127  0   1.7T  0 raid0 /scratch\nnvme0n1       259:2    0    10G  0 disk\n\u251c\u2500nvme0n1p1   259:3    0    10G  0 part  /\n\u2514\u2500nvme0n1p128 259:4    0     1M  0 part\n\nuser@host: df -h /scratch\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/md127      1.7T   77M  1.6T   1% /scratch\n</code></pre>"},{"location":"storage/backend-storage-options/#combine-custom-scratch-and-root-size","title":"Combine custom scratch and root size","text":"<p>You can combine parameters as needed. For example, <code>qsub -l root_size=150 -l scratch_size=200 -l nodes=2</code> will provision 2 nodes with 150GB / and 200GB SSD /scratch</p>"},{"location":"storage/backend-storage-options/#change-the-storage-parameters-at-queue-level","title":"Change the storage parameters at queue level","text":"<p>Edit <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> to configure default storage settings at a queue level:</p> <pre><code>queue_type:\ncompute:\n# /root will be 30 GB and /scratch will be a standard 100GB SSD\nqueues: [\"queue1\", \"queue2\", \"queue3\"]\ninstance_ami: \"ami-082b5a644766e0e6f\"\ninstance_type: \"c5.large\"\nscratch_size: \"100\"\nroot_size: \"30\"\n# .. Refer to the doc for more supported parameters\nmemory:\n# /scratch will be a SSD with provisioned IO\nqueues: [\"queue4\"]\ninstance_ami: \"ami-082b5a644766e0e6f\"\ninstance_type: \"r5.large\"\nscratch_size: \"300\"\nscratch_iops: \"5000\"\ninstancestore:\n# /scratch will use the default instance store\nqueues: [\"queue5\"]\ninstance_ami: \"ami-082b5a644766e0e6f\"\ninstance_type: \"m5dn.12large\"\nroot_size: \"300\"\n</code></pre>"},{"location":"storage/launch-job-with-fsx/","title":"Launch a job with FSx for Lustre","text":""},{"location":"storage/launch-job-with-fsx/#what-is-fsx","title":"What is FSx","text":"<p>Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. </p> <p>Please note the following when using FSx on Scale-Out Computing on AWS</p> <ul> <li>FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS)</li> <li>You can launch an ephemeral FSx filesystem for your job</li> <li>You can connect to an existing FSx filesystem</li> <li>You can dynamically adjust the storage capacity of your FSx filesystem</li> <li>Exported files (if any) from FSx to S3 will be stored under <code>s3://&lt;YOUR_BUCKET_NAME&gt;/&lt;CLUSTER_ID&gt;-fsxoutput/job-&lt;JOB_ID&gt;/</code> by default (you can change it if needed)</li> </ul> <p>Scale-Out automatically determines the actions to be taken based on the <code>fsx_lustre</code> value you specified during job submission</p> <ul> <li> <p>If value is <code>yes/true/on</code>, a standard FSx for Lustre will be provisioned</p> </li> <li> <p>If value starts with <code>s3://</code> or is a string, SOCA will try to mount the S3 bucket automatically as part of the FSx deployment</p> </li> <li> <p>If value starts with <code>fs-xxx</code>, SOCA will try to mount an existing FSx automatically</p> </li> </ul>"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx","title":"How to provision an ephemeral FSx","text":"<p>To provision an FSx for Lustre without S3 backend, simply specify <code>-l fsx_lustre=True</code> at job submission.</p> <p>If <code>-l fsx_lustre_capacity</code> is not set, the default storage provisioned will be 1.2 TB. The FSx will be mounted under \"/fsx\" by default, you can change this value by referring at the section at the end of this doc.</p>"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx-with-s3-backend","title":"How to provision an ephemeral FSx with S3 backend","text":""},{"location":"storage/launch-job-with-fsx/#pre-requisite","title":"Pre-requisite","text":"<p>S3 Backend</p> <p>This section is only required if you are planning to use S3 as a data backend for FSx</p> <p>You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role. The Scheduler IAM role can be found on the IAM bash and is named <code>&lt;SOCA_AWS_STACK_NAME&gt;-Security-&lt;UUID&gt;-SchedulerIAMRole-&lt;UUID&gt;</code>. To create an inline policy, select your IAM role, click \"Add Inline Policy\":</p> <p></p> <p>Select \"JSON\" tab</p> <p></p> <p>Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"AllowAccessFSxtoS3\",\n\"Effect\": \"Allow\",\n\"Action\": \"s3:*\",\n\"Resource\": [\n\"arn:aws:s3:::&lt;YOUR_BUCKET_NAME&gt;\",\n\"arn:aws:s3:::&lt;YOUR_BUCKET_NAME&gt;/*\"\n]\n}\n]\n}\n</code></pre></p> <p>To validate your policy is effective, access the scheduler host and run the following commmand:</p> <pre><code>## Example when IAM policy is not correct\nuser@host: aws s3 ls s3://&lt;YOUR_BUCKET_NAME&gt;\n\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\n## Example when IAM policy is valid (output will list content of your bucket)\nuser@host: aws s3 ls s3://&lt;YOUR_BUCKET_NAME&gt;\n2019-11-02 04:26:27       2209 dataset1.txt\n2019-11-02 04:26:39      10285 dataset2.csv\n</code></pre> <p>Warning</p> <p>This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes  will not  have access to the S3 bucket.</p>"},{"location":"storage/launch-job-with-fsx/#setup","title":"Setup","text":"<p>For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using <code>-l fsx_lustre=s3://&lt;YOUR_BUCKET_NAME&gt;</code>. The bucket will then be mounted on all nodes provisioned for the job under <code>/fsx</code> mountpoint.</p> <pre><code>user@host: qsub -l fsx_lustre=s3://&lt;YOUR_BUCKET_NAME&gt; -- /bin/sleep 600\n</code></pre> <p>This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job:</p> <p></p> <p>Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under <code>/fsx</code></p> <pre><code>user@host: df -h /fsx\nFilesystem             Size  Used Avail Use% Mounted on\n200.0.170.60@tcp:/fsx  1.1T  4.4M  1.1T   1% /fsx\n\n## Verify the content of your bucket is accessible\nuser@host: ls -ltr /fsx\ntotal 1\n-rwxr-xr-x 1 root root  2209 Nov  2 04:26 dataset1.txt\n-rwxr-xr-x 1 root root 10285 Nov  2 04:26 dataset2.csv\n</code></pre> <p>You can change the ImportPath / ExportPath by using the following syntax: <code>-l fsx_lustre=&lt;BUCKET&gt;+&lt;EXPORT_PATH&gt;+&lt;IMPORT_PATH&gt;</code>.</p> <p>If <code>&lt;IMPORT_PATH&gt;</code> is not set, value defaults to the bucket root level. </p> <p>The default <code>&lt;EXPORT_PATH&gt;</code> is <code>&lt;BUCKET&gt;/&lt;CLUSTER_ID&gt;-fsxoutput/&lt;JOBID&gt;</code></p> <p>Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories. </p>"},{"location":"storage/launch-job-with-fsx/#how-to-connect-to-a-permanentexisting-fsx","title":"How to connect to a permanent/existing FSx","text":"<p>If you already have a running FSx, you can mount it using <code>-l fsx_lustre</code> variable.</p> <pre><code>user@host: qsub -l fsx_lustre=&lt;MY_FSX_DNS&gt; -- /bin/sleep 60\n</code></pre> <p>To retrieve your FSx DNS, select your filesystem and select \"Network &amp; Security\"</p> <p></p> <p>Warning</p> <ul> <li>Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS</li> <li>Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG</li> <li>If you specify both \"fsx_lustre\" and \"fsx_lustre\", only \"fsx_lustre\" will be mounted.</li> </ul>"},{"location":"storage/launch-job-with-fsx/#change-fsx-capacity","title":"Change FSx capacity","text":"<p>Use <code>-l fsx_lustre_size=&lt;SIZE_IN_GB&gt;</code> to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600</p> <pre><code>user@host: qsub  -l fsx_lustre_size=3600 -l fsx_lustre=s3://&lt;YOUR_S3_BUCKET&gt; -- /bin/sleep 600\n</code></pre> <p>This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation.</p> <p></p>"},{"location":"storage/launch-job-with-fsx/#how-to-change-the-mountpoint","title":"How to change the mountpoint","text":"<p>By default Scale-Out Computing on AWS mounts fsx on <code>/fsx</code>. If you need to change this value, edit <code>scripts/ComputeNode.sh</code> update the value of <code>FSX_MOUNTPOINT</code>.</p> <pre><code>...\nif [[ $SOCA_AWS_fsx_lustre != 'false' ]]; then\necho \"FSx request detected, installing FSX Lustre client ... \"\nFSX_MOUNTPOINT=\"/fsx\" ## &lt;-- Update mountpoint here\nmkdir -p $FSX_MOUNTPOINT\n...\n</code></pre>"},{"location":"storage/launch-job-with-fsx/#learn-about-the-other-storage-options-on-scale-out-computing-on-aws","title":"Learn about the other storage options on Scale-Out Computing on AWS","text":"<p>Click here to learn about the other storage options offered by Scale-Out Computing on AWS.</p>"},{"location":"storage/launch-job-with-fsx/#troubleshooting-and-most-common-errors","title":"Troubleshooting and most common errors","text":"<p>Like any other parameter, FSx options can be debugged using <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/&lt;QUEUE_NAME&gt;.log</code></p> <pre><code>[Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. \nUpdate IAM policy as described on the documentation]\n</code></pre> <p>Resolution: Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above</p> <p><pre><code>[Error while trying to create ASG: fsx_lustre_size must be: 1200, 2400, 3600, 7200, 10800]\n</code></pre> Resolution: fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600</p>"},{"location":"tutorials/","title":"About","text":"<p>This section covers SOCA tutorial such as integration with Simple Email Services for job notifications, AMI creation, troubleshooting guide and much more!</p> <p>Refer to the left sidebar for more detailed resources</p>"},{"location":"tutorials/access-soca-cluster/","title":"How to access Scale-Out Computing on AWS","text":"<p>Info</p> <p>Backend storage on Scale-Out Computing on AWS is persistent. You will have access to the same filesystem ($HOME, /data and /apps) whether you access your cluster using SSH, Web Remote Desktop or Native Remote Desktop</p>"},{"location":"tutorials/access-soca-cluster/#ssh-access","title":"SSH access","text":"<p>To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click  \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format.</p> <p></p> <p>SSH to an instance in a Private Subnet</p> <p>If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this:</p> <pre><code>$ ssh-add -K ~/Keys/my_key_region.pem\nIdentity added: /Users/username/Keys/my_key_region.pem (/Users/username/Keys/my_key_region.pem)\n\n$ ssh-add -L\n&lt;you should see your ssh key here&gt;\n</code></pre> <p>Now use <code>-A</code> with ssh and this will forward the key with your ssh login:</p> <pre><code>$ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444\n</code></pre> <p>Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: <pre><code>$ ssh &lt;USERNAME&gt;@&lt;PRIVATE_IP&gt;\n</code></pre></p>"},{"location":"tutorials/access-soca-cluster/#graphical-access-using-windowslinux-virtual-desktop","title":"Graphical access using Windows/Linux virtual desktop","text":"<p>Refer to this page to learn how to launch your own Windows/Linux session and access SOCA via your virtual desktop</p>"},{"location":"tutorials/install-soca-cluster/","title":"Install your Scale-Out Computing on AWS cluster","text":""},{"location":"tutorials/install-soca-cluster/#1-click-installer","title":"1-Click installer","text":"<p>You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work. This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we recommend downloading building your own SOCA (see below) for your production. Always refers to the Github repository for the latest SOCA version.</p> <p>1-Click Install</p>"},{"location":"tutorials/install-soca-cluster/#download-scale-out-computing-on-aws","title":"Download Scale-Out Computing on AWS","text":""},{"location":"tutorials/install-soca-cluster/#option-1-build-your-own-version","title":"Option 1: Build your own version","text":"<p>Scale-Out Computing on AWS is open-source and available on Github (https://github.com/awslabs/scale-out-computing-on-aws. To get started, simply clone the repository:</p> <pre><code># Clone using HTTPS\nuser@host: git clone https://github.com/awslabs/scale-out-computing-on-aws .\n\n# Clone using SSH\nuser@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git .\n</code></pre> <p>Build your release</p> <p>Once you have cloned your repository, install dependencies with <code>pip</code> and then execute <code>source/manual_build.py</code> using either python2 or python3. In the following example we use python3:</p> <p>IAM permissions required</p> <p>Your IAM user invoked by <code>awscli</code> must have the permission to list and upload to S3</p> <pre><code>user@host: pip3 install -r source/requirements.txt\nuser@host: python3 source/manual_build.py\n====== Scale-Out Computing on AWS Build ======\n\n&gt; Generated unique ID for build: r6l1\n &gt; Creating temporary build folder ...\n &gt; Copying required files ...\n &gt; Creating archive for build id: r6l1\n\n====== Uploading to S3 ======\n\n&gt; Please enter the AWS region youd like to build SOCA in: us-east-1\n &gt; Please enter the name of an S3 bucket you own: your-bucket\n &gt; Uploading required files ...'\n\n[+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/install-with-existing-resources.template to s3://your-bucket/soca-installer-r6l1/install-with-existing-resources.template\n...\n[+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/templates/Security.template to s3://your-bucket/soca-installer-r6l1/templates/Security.template'\n\n====== Upload COMPLETE ======\n\n====== Installation Instructions ======\n1. Click on the following link:\nhttps://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review?&amp;templateURL=https://your-bucket.s3.amazonaws.com/soca-installer-r6l1/scale-out-computing-on-aws.template&amp;param_S3InstallBucket=your-bucket&amp;param_S3InstallFolder=soca-installer-r6l1\n2. The 'Install Location' parameters are pre-filled for you, fill out the rest of the parameters.\n\nFor more information: https://awslabs.github.io/scale-out-computing-on-aws/install-soca-cluster/\nPress Enter key to close ..\n</code></pre> <p>This command builds and uploads the required files to Amazon S3, then outputs a 1-Click url to launch the SOCA CloudFormation Stack.</p> <p>Info</p> <p>You can use the same bucket to host multiple Scale-Out Computing on AWS clusters. Each build generates a unique ID and uses that as the S3 key.</p>"},{"location":"tutorials/install-soca-cluster/#option-2-download-the-latest-release-targz","title":"Option 2: Download the latest release (.tar.gz)","text":"<p>Download the tarball from https://github.com/awslabs/scale-out-computing-on-aws/releases</p> <p>Upload to S3</p> <p>Go to your Amazon S3 console and click \"Create Bucket\"</p> <p></p> <p>Choose a name and a region then click  \"Create\"</p> <p></p> <p>Avoid un-necessary charge</p> <p>It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer )</p> <p>Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder  (<code>r6l1</code> in this example) to upload the content of the folder to S3.</p> <p></p> <p>Info</p> <p>You can use the same bucket to host multiple Scale-Out Computing on AWS clusters</p> <p>Locate the install template</p> <p>On your S3 bucket, click on the folder you just uploaded.</p> <p></p> <p>Your install template is located under <code>&lt;S3_BUCKET_NAME&gt;/&lt;BUILD_ID&gt;/scale-out-computing-on-aws.template</code>. Click on the object to retrieve the \"Object URL\"</p> <p></p> <p>Want to use your existing AWS resources?</p> <p>Refer to <code>install-with-existing-resources.template</code> if you want to use Scale-Out Computing on AWS with your existing resources.  Check out the web installer to verify your setup</p>"},{"location":"tutorials/install-soca-cluster/#install-scale-out-computing-on-aws","title":"Install Scale-Out Computing on AWS","text":"<p>Clicking on the link will open the CloudFormation console and pre-fill the Install Location parameters:</p> <p></p> <p>Under stack details, choose the stack name (do not use uppercase or it will break your ElasticSearch cluster). </p> <p>Requirements</p> <ul> <li>No uppercase in stack name</li> <li>Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix)</li> <li>Not supported on regions with less than 3 AZs (Northern California / us-west-1)</li> </ul> <ul> <li> <p>Environment Parameters: Choose your Linux Distribution, instance type for your master host, VPC CIDR, your IP which will be whitelisted for port 22, 80 and 443 as well as the root SSH keypair you want to use</p> </li> <li> <p>LDAP Parameters: Create a default LDAP user</p> </li> </ul> <p></p> <p>Marketplace AMIs</p> <p>If you choose to use the CentOS 7 image, you must subscribe to CentOS 7 in the AWS Marketplace, to allow the installer to access the AMI during installation.</p> <p>This solution supports a heterogeneous environment. After installation, administrators and users can specify a custom AMI per job and queue. </p> <p>Disable Rollback on Failure if needed</p> <p>If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section)</p> <p>Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes.</p> <p></p> <p>CREATE_FAILED</p> <p>If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" </p>"},{"location":"tutorials/install-soca-cluster/#post-install-verifications","title":"Post Install Verifications","text":"<p>Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then  select your base stack and click \"Outputs\"</p> <p></p> <p>Output tabs give you information about the SSH IP for the master, link to the web interface or ElasticSearch.</p> <p></p> <p>Even though Cloudformation resources are created, your environment might not be completely ready.  To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below:</p> <pre><code>38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@&lt;IP&gt;\n ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION *************\n    Hold on, cluster is not ready yet.\n    Please wait ~30 minutes as Scale-Out Computing on AWS is being installed.\n    Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH.\n *********************************************************\nConnection Closed.\n</code></pre> <p>If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted.</p> <pre><code>38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@&lt;IP&gt;\nLast login: Mon Oct  7 21:37:21 2019 from &lt;IP&gt;\n\n   _____  ____   ______ ___\n  / ___/ / __ \\ / ____//   |\n\\__ \\ / / / // /    / /| |\n___/ // /_/ // /___ / ___ |\n/____/ \\____/ \\____//_/  |_|\nCluster: soca-cluster-v1\n&gt; source /etc/environment to load Scale-Out Computing on AWS paths\n\n[ec2-user@ip-20-0-5-212 ~]$\n</code></pre> <p>At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation</p> <p></p>"},{"location":"tutorials/install-soca-cluster/#what-if-ssh-port-22-is-blocked-by-your-it","title":"What if SSH port (22) is blocked by your IT?","text":"<p>Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session.</p> <p>First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button</p> <p></p> <p>Select \"Session Manager\" and click Connect</p> <p></p> <p>You now have access to a secure shell directly within your browser</p> <p></p>"},{"location":"tutorials/install-soca-cluster/#enable-termination-protection","title":"Enable Termination Protection","text":"<p>This step is optional yet higly recommended. AWS CloudFormation allows you to protect a stack from being accidently deleted. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack, including its status, will remain unchanged. To enable \"Termination Protect\" select your Primary template and click \"Stack Action\" button then \"Edit Termination Protection\". Choose \"Enabled\" and click Save. Choose \"Disabled\" if you want to be able to delete the stack again.</p>"},{"location":"tutorials/install-soca-cluster/#important-services","title":"Important Services","text":"<p>Note</p> <p>All services on SOCA will automatically restart if you restart your scheduler instance</p> <p>Run the following command (as root) if you want to restart any service: </p> <ul> <li>Scheduler: <code>service pbs start</code></li> <li>SSSD: <code>service sssd start</code></li> <li>OpenLDAP: <code>service openldap start</code></li> <li>Web UI <code>/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start</code></li> <li>NFS partitions <code>mount -a</code> (mount configuration is available on <code>/etc/fstab</code>)</li> </ul>"},{"location":"tutorials/install-soca-cluster/#operational-metrics","title":"Operational Metrics","text":"<p>This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products.  Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy. </p> <p>To opt out of this feature, modify the <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/cloudformation_builder</code> and set <code>allow_anonymous_data_collection</code> variable to <code>False</code></p> <p>When enabled, the following information is collected and sent to AWS:</p> <pre><code>   - Solution ID: The AWS solution identifier\n- Base Operating System: The operating system selected for the solution deployment\n- Unique ID (UUID): Randomly generated, unique identifier for each solution deployment\n- Timestamp: Data-collection timestamp\n- Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region\n- Keep Forever: If instances are running when no job is running\n- EFA Support: If EFA support was selected\n- Spot Support: If Spot support was invoked for new auto-scaling stacks\n- Stack Creation Version: The version of the stack that is created or deleted\n- Status: The status of the stack (stack_created or stack_deleted)\n- Scratch Disk Size: The size of the scratch disk selected for each solution deployment\n- Region: The region where the stack is deployed\n- FSxLustre: If the job is using FSx for Lustre\n</code></pre>"},{"location":"tutorials/install-soca-cluster/#whats-next","title":"What's next ?","text":"<p>Learn how to access your cluster, how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.</p>"},{"location":"tutorials/integration-ec2-job-parameters/","title":"Job customization for EC2","text":"<p>Scale-Out Computing on AWS made job submission on EC2 very easy  and is fully integrated with EC2. Below is a list of parameters you can specify when you request your simulation to ensure the hardware provisioned will exactly match your simulation requirements. </p> <p>Info</p> <p>If you don't specify them, your job will use the default values configured for your queue (see <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code>)</p> <p>You can use the web-based simulator to generate your qsub command very easily.</p>"},{"location":"tutorials/integration-ec2-job-parameters/#compute","title":"Compute","text":""},{"location":"tutorials/integration-ec2-job-parameters/#base_os","title":"base_os","text":"<ul> <li>Description: Reference to the base OS of the AMI you are using</li> <li>Allowed Values: <code>amazonlinux2</code> <code>centos7</code> <code>rhel7</code></li> <li>Default: If not specified, value default to the OS of the install AMI</li> <li>Examples: <ul> <li><code>-l base_os=centos7</code>: Instances provisioned will be deployed against CentOS manifest</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support","title":"ht_support","text":"<p>Disabled by default</p> <ul> <li>Description: Enable support for hyper-threading</li> <li>Allowed Values: <code>yes</code> <code>true</code> <code>no</code> <code>false</code> (case insensitive) </li> <li>Examples:<ul> <li><code>-l ht_support=True</code>: Enable hyper-threading for all instances</li> <li><code>-l ht_support=False</code>: Disable hyper-threading for all instances (default)</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#instance_ami","title":"instance_ami","text":"<ul> <li>Description: Reference to a custom AMI you want to use</li> <li>Default: If not specified, value default to the AMI specified during installation</li> <li>Examples:<ul> <li><code>-l instance_ami=ami-abcde123</code>: Capacity provisioned for the job will use the specific AMI</li> </ul> </li> </ul> <p>Info</p> <p>If your are planning to use an AMI which is not using the same OS as the scheduler, you will need to specify <code>base_os</code> parameter</p>"},{"location":"tutorials/integration-ec2-job-parameters/#instance_type","title":"instance_type","text":"<ul> <li>Description: The type of instance to provision for the simulation</li> <li>Examples:<ul> <li><code>-l instance_type=c5.large</code>: Provision a c5.large for the simulation</li> <li><code>-l instance_type=c5.large+m5.large</code>: Provision c5.large and m5.large (if needed) for the simulation.</li> </ul> </li> </ul> <p>Info</p> <p>You can specify multiple instances type using \"+\" sign.  When using more than 1 instance type, AWS will prioritize the capacity based on the order (eg: launch c5.large first and switch to m5.large if AWS can't provision c5.large anymore)</p>"},{"location":"tutorials/integration-ec2-job-parameters/#nodes","title":"nodes","text":"<ul> <li>Description:The number of EC2 instance to provision</li> <li>Examples:<ul> <li><code>-l nodes=5</code>: Provision 5 EC2 instances</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#force_ri","title":"force_ri","text":"<ul> <li>Description: Restrict a job to run on Reserved Instance</li> <li>Allowed Values: <code>True</code> <code>False</code></li> <li>Default: <code>False</code></li> <li>Examples: <ul> <li><code>-l force_ri=False</code>: Job can use RI, On-Demand or Spot</li> <li><code>-l force_ri=True</code>: Job will only use Reserved Instance. Job will stay in the queue if there is not enough reserved instance available</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_count","title":"spot_allocation_count","text":"<ul> <li>Description: Specify the number of SPOT instances to launch when provisioning both OD (On Demand) and SPOT instances</li> <li>Allowed Values: Integer</li> <li>Examples:<ul> <li><code>-l nodes=10 -l spot_price=auto -l spot_allocation_count=8</code>: Provision 10 instances, 2 OD and 8 SPOT with max spot price capped to OD price</li> <li><code>-l nodes=10 -l spot_price=1.4 -l spot_allocation_count=5</code>: Provision 10 instances, 5 OD and 5 SPOT with max spot price set to $1.4 </li> <li><code>-l nodes=10 -l spot_price=auto</code>: Only provision SPOT instances</li> <li><code>-l nodes=10</code>: Only provision OD instances</li> </ul> </li> </ul> <p>Note</p> <p>This parameter is ignored if <code>spot_price</code> is not specified <code>spot_allocation_count</code> must be lower that the total number of nodes you are requesting (eg: you can not do <code>-l nodes=5 -l spot_allocation_count=15</code>)</p>"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_strategy","title":"spot_allocation_strategy","text":"<ul> <li>Description: Choose allocation strategy when using multiple SPOT instances type</li> <li>Allowed Valuess: <code>capacity-optimized</code> or <code>lowest-price</code> or <code>diversified</code> (only for SpotFleet deployments)</li> <li>Default Value: <code>lowest-price</code></li> <li>Examples:<ul> <li><code>-l spot_allocation_strategy=capacity-optimized</code>: AWS will provision compute nodes based on capacity availabilities</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#spot_price","title":"spot_price","text":"<ul> <li>Description: Enable support for SPOT instances</li> <li>Allowed Values: any float value or <code>auto</code></li> <li>Examples:<ul> <li><code>-l spot_price=auto</code>: Max price will be capped to the On-Demand price</li> <li><code>-l spot_price=1.4</code>: Max price you are willing to pay for this instance will be $1.4 an hour.</li> </ul> </li> </ul> <p>Note</p> <p><code>spot_price</code> is capped to On-Demand price (e.g: Assuming you are provisioning a t3.medium, AWS will default spot price to 0.418 (OD price) even though you specified <code>-l spot_price=15</code>)</p>"},{"location":"tutorials/integration-ec2-job-parameters/#subnet_id","title":"subnet_id","text":"<ul> <li>Description: Reference to a subnet ID to use</li> <li>Default: If not specified, value default to one of the three private subnets created during installation</li> <li>Examples:<ul> <li><code>-l subnet_id=sub-123</code>: Will provision capacity on sub-123 subnet</li> <li><code>-l subnet_id=sub-123+sub-456+sub-789</code>: + separated list of private subnets. Specifying more than 1 subnet is useful when requesting large number of instances</li> <li><code>-l subnet_id=2</code>: SOCA will provision capacity in 2 private subnets chosen randomly</li> </ul> </li> </ul> <p>Note</p> <p>If you specify more than 1 subnet and have <code>placement_group</code> set to True, SOCA will automatically provision capacity and placement group on the first subnet from the list</p> <p>Note</p> <p>Capacity provisioning is limited to private subnets.</p>"},{"location":"tutorials/integration-ec2-job-parameters/#storage","title":"Storage","text":""},{"location":"tutorials/integration-ec2-job-parameters/#ebs","title":"EBS","text":""},{"location":"tutorials/integration-ec2-job-parameters/#keep_ebs","title":"keep_ebs","text":"<p>Disabled by default</p> <ul> <li>Description: Retain or not the EBS disks once the simulation is complete</li> <li>Allowed Values: <code>yes</code> <code>true</code> <code>false</code> <code>no</code> (case insensitive)</li> <li>Default Value: <code>False</code></li> <li>Example: <ul> <li><code>-l keep_ebs=False</code>: (Default) All EBS disks associated to the job will be deleted</li> <li><code>-l keep_ebs=True</code>: Retain EBS disks after the simulation has terminated (mostly for debugging/troubleshooting procedures)</li> </ul> </li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#root_size","title":"root_size","text":"<ul> <li>Description: Define the size of the local root volume</li> <li>Unit: GB</li> <li>Example: <code>-l root_size=300</code>: Provision a 300 GB SSD disk for <code>/</code> (either <code>sda1</code> or <code>xvda1</code>)</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_size","title":"scratch_size","text":"<ul> <li>Description: Define the size of the local root volume</li> <li>Unit: GB</li> <li>Example: <code>-l scratch_size=500</code>: Provision a 500 GB SSD disk for <code>/scratch</code></li> </ul> <p>Info</p> <p>scratch disk is automatically mounted on all nodes associated to the simulation under <code>/scratch</code></p>"},{"location":"tutorials/integration-ec2-job-parameters/#instance_store","title":"instance_store","text":"<p>Info</p> <ul> <li>SOCA automatically mount instance storage when available. </li> <li>For instances having more than 1 volume, SOCA will create a raid device</li> <li>In all cases, instance store volumes will be mounted on <code>/scratch</code></li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_iops","title":"scratch_iops","text":"<ul> <li>Description: Define the number of provisioned IOPS to allocate for your <code>/scratch</code> device</li> <li>Unit: IOPS</li> <li>Example: <code>-l scratch_iops=3000</code>: Your EBS disks provisioned for <code>/scratch</code> will have 3000 dedicated IOPS</li> </ul> <p>Info</p> <p>It is recommended to set the IOPs to 3x storage capacity of your EBS disk</p>"},{"location":"tutorials/integration-ec2-job-parameters/#fsx-for-lustre","title":"FSx for Lustre","text":""},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre","title":"fsx_lustre","text":""},{"location":"tutorials/integration-ec2-job-parameters/#with-no-s3-backend","title":"With no S3 backend","text":"<ul> <li>Example: <code>-l fsx_lustre=True</code>: Create a new FSx for Lustre and mount it accross all nodes</li> </ul> <p>Info</p> <ul> <li>FSx partitions are mounted as <code>/fsx</code>. This can be changed if needed</li> <li>If <code>fsx_lustre_size</code> is not specified, default to 1200 GB</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#with-s3-backend","title":"With S3 backend","text":"<ul> <li>Example: <code>-l fsx_lustre=my-bucket-name</code> or <code>-l fsx_lustre=s3://my-bucket-name</code> : Create a new FSx for Lustre and mount it across all nodes</li> </ul> <p>Info</p> <ul> <li>FSx partitions are mounted as <code>/fsx</code>. This can be changed if needed</li> <li>You need to give IAM permission first</li> <li>If not specified, SOCA automatically prefix your bucket name with  <code>s3://</code></li> <li>If <code>fsx_lustre_size</code> is not specified, default to 1200 GB</li> <li>You can configure custom ImportPath and ExportPath</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#mount-existing-fsx","title":"Mount existing FSx","text":"<ul> <li>Description: Mount an existing FSx to all compute nodes if <code>fsx_lustre</code> points to a FSx filesystem's DNS name</li> <li>Example: <code>-l fsx_lustre=fs-xxxx.fsx.region.amazonaws.com</code></li> </ul> <p>Info</p> <ul> <li>FSx partitions are mounted as <code>/fsx</code>. This can be changed if needed </li> <li>Make sure your FSx for Luster configuration is correct (use SOCA VPC and correct IAM roles)</li> <li>Make sure to use the Filesytem's DNS name</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_size","title":"fsx_lustre_size","text":"<ul> <li>Description: Create an ephemeral FSx for your job and mount the  S3 bucket specified </li> <li>Unit: GB</li> <li>Example: <code>-l fsx_lustre_size=3600</code>: Provision a 3.6TB EFS disk</li> </ul> <p>Info</p> <p>If <code>fsx_lustre_size</code> is not specified, default to 1200 GB (smallest size supported)</p> <p>Pre-Requisite</p> <p>This parameter is ignored unless you have specified <code>fsx_lustre=True</code></p>"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_deployment_type","title":"fsx_lustre_deployment_type","text":"<ul> <li>Description: Choose what type of FSx for Lustre you want to deploy</li> <li>Allowed Valuess: <code>SCRATCH_1</code> <code>SCRATCH_2</code> <code>PERSISTENT_1</code> (case insensitive)</li> <li>Default Value: <code>SCRATCH_1</code></li> <li>Example: <code>-l fsx_lustre_deployment_type=scratch_2</code>: Provision a FSx for Lustre with SCRATCH_2 type</li> </ul> <p>Info</p> <p>If <code>fsx_lustre_size</code> is not specified, default to 1200 GB (smallest size supported)</p> <p>Pre-Requisite</p> <p>This parameter is ignored unless you have specified <code>fsx_lustre=True</code></p>"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_per_unit_throughput","title":"fsx_lustre_per_unit_throughput","text":"<ul> <li>Description: Select the baseline disk throughput available for that file system </li> <li>Allowed Values: <code>50</code> <code>100</code> <code>200</code></li> <li>Unit: MB/s</li> <li>Example: <code>-l fsx_lustre_per_unit_throughput=250</code>: </li> </ul> <p>Info</p> <p>Per Unit Throughput is only avaible when using <code>PERSISTENT_1</code> FSx for Lustre</p> <p>Pre-Requisite</p> <p>This parameter is ignored unless you have specified <code>fsx_lustre=True</code> </p>"},{"location":"tutorials/integration-ec2-job-parameters/#network","title":"Network","text":""},{"location":"tutorials/integration-ec2-job-parameters/#efa_support","title":"efa_support","text":"<ul> <li>Description: Enable EFA support</li> <li>Allowed Values: yes, true, True </li> <li>Example: <code>-l efa_support=True</code>: Deploy an EFA device on all the nodes</li> </ul> <p>Info</p> <p>You must use an EFA compatible instance, otherwise your job will stay in the queue</p>"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support_1","title":"ht_support","text":"<p>Disabled by default</p> <ul> <li>Description: Enable support for hyper-threading</li> <li>Allowed Values: <code>yes</code> <code>true</code> (case insensitive) </li> <li>Example: <code>-l ht_support=True</code>: Enable hyper-threading for all instances</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#placement_group","title":"placement_group","text":"<p>Enabled by default</p> <ul> <li>Description: Disable placement group</li> <li>Allowed Values: <code>yes</code> <code>true</code> (case insensitive) </li> <li>Example: <code>-l placement_group=True</code>: Instances will use placement groups</li> </ul> <p>Info</p> <ul> <li>Placement group is enabled by default as long as the number of nodes provisioned is greated than 1</li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#others","title":"Others","text":""},{"location":"tutorials/integration-ec2-job-parameters/#system_metrics","title":"system_metrics","text":"<p>Default to False</p> <ul> <li>Description: Send host level metrics to your ElasticSearch backend</li> <li>Allowed Values: <code>yes</code> <code>no</code> <code>true</code> <code>false</code> (case insensitive) </li> <li>Example: <code>-l system_metrics=False</code></li> </ul> <p>Warning</p> <p>Enabling system_metrics generate a lot of data (especially if you are tracking 1000s of nodes). If needed, you can add more storage to your AWS ElasticSearch cluster </p>"},{"location":"tutorials/integration-ec2-job-parameters/#anonymous_metrics","title":"anonymous_metrics","text":"<p>Default to the value specified during SOCA installation</p> <ul> <li>Description: Send anonymous operational metrics to AWS</li> <li>Allowed Values: <code>yes</code> <code>true</code> <code>no</code> <code>false</code> (case insensitive) </li> <li>Example: <code>-l anonymous_metrics=True</code></li> </ul>"},{"location":"tutorials/integration-ec2-job-parameters/#how-to-use-custom-parameters","title":"How to use custom parameters","text":"<p>Example</p> <p>Here is an example about how to use a custom AMI at job or queue level. This example is applicable to all other parameters (simply change the parameter name to the one you one to use). </p>"},{"location":"tutorials/integration-ec2-job-parameters/#for-a-single-job","title":"For a single job","text":"<p>Use <code>-l instance_ami</code> parameter if you want to only change the AMI for a single job</p> <pre><code>$ qsub -l instance_ami=ami-082b... -- /bin/echo Hello\n</code></pre> <p>Priority</p> <p>Job resources have the highest priorities. Your job will always use the AMI specified at submission time even if it's different thant the one configure at queue level.</p>"},{"location":"tutorials/integration-ec2-job-parameters/#for-an-entire-queue","title":"For an entire queue","text":"<p>Edit <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> and update the default <code>instance_ami</code> parameter if you want all jobs in this queue to use your new AMI:</p> <pre><code>queue_type:\ncompute:\nqueues: [\"queue1\", \"queue2\", \"queue3\"] instance_ami: \"&lt;YOUR_AMI_ID&gt;\" # &lt;- Add your new AMI \ninstance_type: ...\nroot_size: ...\nscratch_size: ...\nefa: ...\n....\n</code></pre>"},{"location":"tutorials/integration-ec2-job-parameters/#view-examples","title":"View Examples","text":""},{"location":"tutorials/job-configuration-generator/","title":"Job Submission Generator","text":"<p>Automatic parameter selection</p> <ul> <li>You can manually specify parameters at job submission using the command below. If needed, all parameters can also be automatically configured at queue level. </li> <li>Job will use the default parameters configured for its queue unless the parameters are explicitly specified during submission (job parameters override queue parameters).</li> <li>Refer to this page for additional examples.</li> </ul>  Your Command  user@host: qsub {{qsub_instance_ami}}  {{qsub_instance_type}}  {{qsub_subnet_id}}  {{qsub_spot_price}} {{qsub_efa_support}} {{qsub_placement_group}} {{qsub_root_size}} {{qsub_scratch_size}} {{qsub_scratch_iops}} {{qsub_fsx_lustre}} {{qsub_fsx_lustre_size}} {{qsub_ht_support}} {{qsub_spot_allocation_count}} {{qsub_spot_allocation_strategy}} {{qsub_nodes}} {{qsub_base_os}} {{qsub_keep_ebs}} {{qsub_force_ri}} myscript.sh   Job Parameters  Compute parameters:  Documentation Must be a number greater than 0 Documentation Documentation Image name must start with \"ami-\" Documentation Must be centos7, rhel7 or amazonlinux2 {{base_os_error}} Documentation Subnet name must start with \"sub-\" Documentation Spot Price must be a float (eg 1.2) or auto (match OD price) Documentation Must be a number {{spot_allocation_error}} {{spot_allocation_error_price}} Documentation Must be either lowest-cost (default) or capacity-optimized {{spot_allocation_strategy_price}}  Storage parameters:  Documentation Root Size must be a number Documentation Scratch Size must be a number Documentation Provisioned IO/s must be a number Documentation Documentation Size must be a number Flags:   I want to use EFA Documentation  I do not want to use Placement Group (enabled by default)    Documentation  I want to enable HyperThreading (disabled by default)   Documentation  I want to retain my EBS disks (disabled by default)   Documentation  I want my job to only run on Reserved instances  Documentation"},{"location":"tutorials/job-licenses-flexlm/","title":"Job with FlexLM licenses requirements","text":"<p>In this page, we will see how Scale-Out Computing on AWS manages job and capacity provisioning based on license availabilities.</p> <p>Example configuration</p> <p>Test settings used for all examples:</p> <ul> <li>License Server Hostname: <code>licenses.soca.dev</code></li> <li>License Server port: <code>5000</code></li> <li>License Daemon port: <code>5001</code></li> <li>Feature to check: <code>Audio_System_Toolbox</code></li> <li>Scale-Out Computing on AWS cluster name: <code>rctest</code></li> </ul>"},{"location":"tutorials/job-licenses-flexlm/#firewall-configuration","title":"Firewall Configuration","text":"<p>Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers.</p> <p>FlexLM server installed on Scheduler host</p> <p>No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically whitelist all traffic between the scheduler and the compute nodes.</p> <p>Warning</p> <p>FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to whitelist both of them.</p> <p>Allow traffic from your license server IP to Scale-Out Computing on AWS</p> <p>Assuming my license server IP is 10.0.15.18, simply go to the EC2 console, locate your <code>Scheduler</code> and <code>ComputeNode</code> security groups (filter by your cluster name) associated to your Scale-Out Computing on AWS cluster and whitelist both SERVER and DAEMON ports:</p> <p></p> <p>Allow traffic from Scale-Out Computing on AWS to your license server</p> <p>Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to whitelist the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster. </p>"},{"location":"tutorials/job-licenses-flexlm/#upload-your-lmutil","title":"Upload your lmutil","text":"<p>lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py</code> with the location of your file.</p> <pre><code>arg = parser.parse_args()\nlmstat_path = \"PATH_TO_LMUTIL\"\nif lmstat_path == \"PATH_TO_LMUTIL\":\n    print('Please specify a link to your lmutil binary (edit line 19 of this file')\n    sys.exit(1)\n</code></pre> <p>Note</p> <p>You do not need to install FlexLM server manager. Only <code>lmutil</code> binary is required.</p> <p>lmutil and RHEL based distro</p> <p>FlexLM may requires 32 bits lib depending your system. If launching <code>lmutil</code> returns an <code>ELF</code> version mismatch, simply install <code>yum install redhat-lsb</code> (or equivalent)</p>"},{"location":"tutorials/job-licenses-flexlm/#how-to-retrieve-number-of-licenses-available","title":"How to retrieve number of licenses available","text":"<p>Scale-Out Computing on AWS includes a script (<code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py</code>) which output the number of FlexLM available for a given feature. This script takes the following arguments:</p> <ul> <li>-s: The license server hostname</li> <li>-p: The port used by your flexlm deamon</li> <li>-f: The feature name (case sensitive)</li> <li>(Optional) -m: Reserve licenses number for non HPC usage</li> </ul> <p>Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: <pre><code>license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\n26\n</code></pre> Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: <pre><code>license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11\n</code></pre></p> <p>Info</p> <p><code>license_check.py</code> is simply a <code>lmutil</code> wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed <pre><code>lmutil lmstat -a -c 5000@licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\"\n</code></pre></p>"},{"location":"tutorials/job-licenses-flexlm/#integration-with-scale-out-computing-on-aws","title":"Integration with Scale-Out Computing on AWS","text":"<p>IMPORTANT</p> <p>The name of the resource must be <code>*_lic_*</code>. We recommend using <code>&lt;application&gt;_lic_&lt;feature_name&gt;</code></p> <p>Update your <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml</code> and create a new resource. This file must follow the YAML syntax. </p> <pre><code># There is no requirements for section names, but I recommend having 1 section = 1 application\n\nmatlab:\nmatlab_lic_audiosystemtoolbox: \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\"\n# Example for other daemons/features\ncomsol:\ncomsol_lic_acoustic: \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\"\ncomsol_lic_cadimport: \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\"\n\nsynopsys:\nsynopsys_lic_testbenchruntime: \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\"\nsynopsys_lic_vcsruntime: \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\"\nsynopsys_lic_vipambaaxisvt: \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\"\n</code></pre> <p>This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements.</p> <p>Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit <code>/var/spool/pbs/sched_priv/sched_config</code> and add a new <code>server_dyn_res</code></p> <pre><code>server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\"\n</code></pre> <p>On the same file, add your new resource under <code>resources</code> section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount.</p> <pre><code>resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\"\n</code></pre> <p>Finally, edit <code>/var/spool/pbs/server_priv/resourcedef</code> and add your new resource with <code>type=long</code></p> <pre><code>...\nht_support type=string\nbase_os type=string\nfsx_lustre_bucket type=string\nfsx_lustre_size type=string\nfsx_lustre_dns type=string\nmatlab_lic_audiosystemtoolbox type=long\n</code></pre> <p>Once done, restart the scheduler using <code>service pbs restart</code></p>"},{"location":"tutorials/job-licenses-flexlm/#test","title":"Test","text":"<p>For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available <pre><code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\n3\n</code></pre></p> <p>Let's try to submit a job which require 5 licenses</p> <pre><code>qsub -l matlab_lic_audiosystemtoolbox=5 -- /bin/sleep 600\n31.ip-20-0-2-69\n</code></pre> <p>Let's check the log files under <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/log/&lt;QUEUE_NAME&gt;</code>. Scale-Out Computing on AWS will ignore this job due to the lack of licenses available</p> <pre><code> [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}]\n [157] [INFO] [Next User is mickael]\n [157] [INFO] [Next Job for user is ['31']]\n [157] [INFO] [Checking if we have enough resources available to run job_31]\n ....\n [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5]\n [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox]\n</code></pre> <p>If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): <pre><code> [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}]\n [157] [INFO] [Checking if we have enough resources available to run job_31]\n ....\n [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5]\n # Next job in in queue\n [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}]\n [157] [INFO] [Checking if we have enough resources available to run job_32]\n ....\n [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5]\n # Next job in in queue\n [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}]\n [157] [INFO] [Checking if we have enough resources available to run job_33]\n ....\n [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5]\n [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox]\n</code></pre></p> <p>Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs</p> <p>Let say you have 10 licenses available and you submit <code>job1</code> and <code>job2</code> which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. </p> <p>Shortly after you submit <code>job3</code> which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start).  In that case we skip <code>job3</code> until both <code>job1</code> and <code>job2</code> and in running state.</p> <p>Invalid Resource</p> <p>You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS) <pre><code>qsub -l matlab_lic_fakeresource=3 -- /bin/sleep 60\nqsub: Unknown resource Resource_List.matlab_lic_fakeresource\n</code></pre></p>"},{"location":"tutorials/job-start-stop-email-notification/","title":"Automatic emails when your job start/stop","text":"<p>In this page, I will show you how to configure email notification when your job start/stop. For this example, I will use Simple Email Service (SES), but you can use any SMTP provider.</p> <p>Info</p> <p>Note1: By default, the Scale-Out Computing on AWS admin user you created during the installation does not have any associated email address. If you want to use this account you must edit LDAP and add the \"mail\" attribute. </p> <p>Note2: All qmgr command must be executed on the scheduler host</p>"},{"location":"tutorials/job-start-stop-email-notification/#configure-ses-sender-domain","title":"Configure SES sender domain","text":"<p>Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing.</p> <p></p> <p>Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated.</p> <p></p>"},{"location":"tutorials/job-start-stop-email-notification/#configure-recipients-addresses","title":"Configure Recipients addresses","text":"<p>By default SES limits you to send email to unique recipients which you will need verify manually</p> <p></p> <p>If you want to be able to send email to any addresses, you need to request production access.</p> <p></p>"},{"location":"tutorials/job-start-stop-email-notification/#notification-code","title":"Notification code","text":"<p>Create a hook file (note: this file can be found under <code>/apps/soca/$SOCA_CONFIGURATION/cluster_hooks/job_notifications.py</code> on your Scale-Out Computing on AWS cluster)</p> <p>Edit the following section to match your SES settings <pre><code>ses_sender_email = '&lt;SES_SENDER_EMAIL_ADDRESS_HERE&gt;'\nses_region = '&lt;YOUR_SES_REGION_HERE&gt;'\n</code></pre></p>"},{"location":"tutorials/job-start-stop-email-notification/#create-the-hooks","title":"Create the hooks","text":"<p>Once your script is created, configure your scheduler hooks by running the following commands: <pre><code>user@host: qmgr -c \"create hook notify_job_start event=runjob\"\nuser@host: qmgr -c \"create hook notify_job_complete event=execjob_end\"\nuser@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/job_notifications.py\"\nuser@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/job_notifications.py\"\n</code></pre></p> <p>Note: If you make any change to the python file, you must re-run the <code>import hook</code> command</p>"},{"location":"tutorials/job-start-stop-email-notification/#test","title":"Test","text":"<p>Let's submit a test job which will last 5 minutes</p> <pre><code>qsub -N mytestjob -- /bin/sleep 300\n</code></pre> <p>Now let's verify if I received the alerts correctly.</p> <p>Job start:</p> <p></p> <p>5 minutes later:</p> <p></p>"},{"location":"tutorials/job-start-stop-email-notification/#addupdate-email","title":"Add/Update email","text":"<p>Run <code>ldapsearch -x uid=&lt;USER&gt;</code> command to verify if your user has a valid <code>mail</code> attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute.</p> <pre><code>user@host: ldapsearch -x uid=mickael\n## mickael, People, soca.local\ndn: uid=mickael,ou=People,dc=soca,dc=local\nobjectClass: top\nobjectClass: person\nobjectClass: posixAccount\nobjectClass: shadowAccount\nobjectClass: inetOrgPerson\nobjectClass: organizationalPerson\nuid: mickael\nuidNumber: 5001\ngidNumber: 5001\ncn: mickael\nsn: mickael\nloginShell: /bin/bash\nhomeDirectory: /data/home/mickael\n</code></pre> <p>To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content</p> <pre><code>dn: uid=mickael,ou=People,dc=soca,dc=local\nchangetype: modify\nadd: mail\nmail: mickael@soca.dev\n</code></pre> <p>Then execute the <code>ldapadd</code> as root <pre><code>user@host: ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif\nmodifying entry \"uid=mickael,ou=People,dc=soca,dc=local\"\n</code></pre></p> <p>Finally re-run the ldapsearch command and validate your user now has <code>mail</code> attribute</p> <pre><code>user@host: ldapsearch -x uid=mickael\ndn: uid=mickael,ou=People,dc=soca,dc=local\nobjectClass: top\nobjectClass: person\nobjectClass: posixAccount\nobjectClass: shadowAccount\nobjectClass: inetOrgPerson\nobjectClass: organizationalPerson\nuid: mickael\nuidNumber: 5001\ngidNumber: 5001\ncn: mickael\nsn: mickael\nloginShell: /bin/bash\nhomeDirectory: /data/home/mickael\nmail: mickael@soca.dev\n</code></pre>"},{"location":"tutorials/job-start-stop-email-notification/#check-the-logs","title":"Check the logs","text":"<p>Scheduler hooks are located: </p> <ul> <li>/var/spool/pbs/server_logs/ for notify_job_start on the Scheduler</li> <li>/var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)</li> </ul>"},{"location":"tutorials/launch-always-on-instances/","title":"Launch AlwaysOn nodes","text":""},{"location":"tutorials/launch-always-on-instances/#why-alwayson-instances","title":"Why AlwaysOn instances?","text":"<p>By default, Scale-Out Computing on AWS provisions on-demand capacity when there are jobs in the queue. This mean any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready.</p> <p>If you want to avoid this penalty, you can provision \"AlwaysOn instance\". Please note you will be charged until you manually terminate it.</p>"},{"location":"tutorials/launch-always-on-instances/#how-to-launch-an-alwayson-instance","title":"How to launch an AlwaysOn instance?","text":"<p>On your scheduler host, sudo as root and run <code>source /etc/environment</code> to load Scale-Out Computing on AWS shell and then execute <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py</code></p> <pre><code>[root@ip-40-0-22-232 ~]# python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py -h\nusage: add_nodes.py [-h] --desired_capacity [DESIRED_CAPACITY] --instance_type\n                    [INSTANCE_TYPE] --job_name [JOB_NAME] --job_owner\n                    [JOB_OWNER] --queue [QUEUE] [--efa_support EFA_SUPPORT]\n[--ht_support HT_SUPPORT] [--keep_forever KEEP_FOREVER]\n[--terminate_when_idle [TERMINATE_WHEN_IDLE]]\n[--base_os BASE_OS] [--fsx_lustre FSX_LUSTRE]\n[--fsx_lustre_size FSX_LUSTRE_SIZE] --instance_ami\n                    [INSTANCE_AMI] [--job_id [JOB_ID]]\n[--job_project [JOB_PROJECT]]\n[--placement_group PLACEMENT_GROUP]\n[--root_size [ROOT_SIZE]] [--scratch_iops [SCRATCH_IOPS]]\n[--scratch_size [SCRATCH_SIZE]]\n[--spot_allocation_count [SPOT_ALLOCATION_COUNT]]\n[--spot_allocation_strategy [SPOT_ALLOCATION_STRATEGY]]\n[--spot_price [SPOT_PRICE]] [--keep_ebs]\n[--subnet_id SUBNET_ID] [--tags [TAGS]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n--desired_capacity [DESIRED_CAPACITY]\nNumber of EC2 instances to deploy\n  --instance_type [INSTANCE_TYPE]\nInstance type you want to deploy\n  --job_name [JOB_NAME]\nJob Name for which the capacity is being provisioned\n  --job_owner [JOB_OWNER]\nJob Owner for which the capacity is being provisioned\n  --queue [QUEUE]       Queue to map the capacity\n  --efa_support EFA_SUPPORT\n                        Support for EFA\n  --ht_support HT_SUPPORT\n                        Enable Hyper Threading\n  --keep_forever KEEP_FOREVER\n                        Whether or not capacity will stay forever\n  --terminate_when_idle [TERMINATE_WHEN_IDLE]\nIf instances will be terminated when idle for N\n                        minutes\n  --base_os BASE_OS     Specify custom Base OK\n  --fsx_lustre FSX_LUSTRE\n                        Mount existing FSx by providing the DNS\n  --fsx_lustre_size FSX_LUSTRE_SIZE\n                        Specify size of your FSx\n  --instance_ami [INSTANCE_AMI]\nAMI to use\n  --job_id [JOB_ID]     Job ID for which the capacity is being provisioned\n  --job_project [JOB_PROJECT]\nJob Owner for which the capacity is being provisioned\n  --placement_group PLACEMENT_GROUP\n                        Enable or disable placement group\n  --root_size [ROOT_SIZE]\nSize of Root partition in GB\n  --scratch_iops [SCRATCH_IOPS]\nSize of /scratch in GB\n  --scratch_size [SCRATCH_SIZE]\nSize of /scratch in GB\n  --spot_allocation_count [SPOT_ALLOCATION_COUNT]\nWhen using mixed OD and SPOT, choose % of SPOT\n  --spot_allocation_strategy [SPOT_ALLOCATION_STRATEGY]\nlowest-price or capacity-optimized or diversified\n                        (supported only for SpotFleet)\n--spot_price [SPOT_PRICE]\nSpot Price\n  --keep_ebs            Do not delete EBS disk\n  --subnet_id SUBNET_ID\n                        Launch capacity in a special subnet\n  --tags [TAGS]         Tags, format must be {'Key':'Value'}\n</code></pre> <p>To enable \"AlwaysOn\" instance, there are two alternative methods either using --keep_forever or --terminate_when_idle options.</p>"},{"location":"tutorials/launch-always-on-instances/#using-keep_forever-option","title":"Using keep_forever option","text":"<p>Use <code>--keep_forever true</code> and <code>alwayson</code> queue. If you do not want to use <code>alwayson</code> queue, make sure the queue you have created has been configured correctly to support AlwaysOn (see instructions)</p> <p>See example below (note: you can use additional parameters if needed)</p> <pre><code> python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py --instance_type=c5.large \\\n--desired_capacity=1 \\\n--keep_forever true \\\n--job_owner mickael \\\n--job_name always_on_capacity \\\n--queue alwayson\n</code></pre> <p>When the capacity is available, simply run a job and specify <code>alwayson</code> as queue name</p>"},{"location":"tutorials/launch-always-on-instances/#terminate-an-alwayson-instance-launched-with-keep_forever","title":"Terminate an AlwaysOn instance launched with keep_forever","text":"<p>Simply go to your CloudFormation console, locate the stack following the naming convention: <code>soca-&lt;cluster_name&gt;-keepforever-&lt;queue_name&gt;-uniqueid</code> and terminate it.</p> <p></p>"},{"location":"tutorials/launch-always-on-instances/#using-terminate_when_idle-option","title":"Using terminate_when_idle option","text":"<ol> <li>Use <code>--terminate_when_idle N</code> where N represents the number of minutes when the instance(s) where be terminated after all running jobs on the instances exit,</li> <li>Use <code>--keep_forever false</code>, and </li> <li>Use <code>alwayson</code> queue. If you do not want to use <code>alwayson</code> queue, make sure the queue you have created has been configured correctly to support AlwaysOn (see instructions)</li> </ol> <p>See example below (note: you can use additional parameters if needed)</p> <pre><code> python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py --instance_type=c5.large \\\n--desired_capacity=1 \\\n--terminate_when_idle 5 \\\n--keep_forever false \\\n--job_owner mickael \\\n--job_name always_on_capacity \\\n--queue alwayson\n</code></pre> <p>When the capacity is available, simply run a job and specify <code>alwayson</code> as queue name. </p> <p>The instance(s) launched with <code>--terminate_when_idle</code> will be terminated automatically once all jobs running on the instance exit then the instance is detected as idle (no jobs running) for the specified number of minutes (5 in the example above).</p>"},{"location":"tutorials/launch-your-first-job/","title":"Launch your first job","text":""},{"location":"tutorials/launch-your-first-job/#submit-your-job","title":"Submit your job","text":"<p>Things to know before you start</p> <ul> <li>Jobs start on average 5 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned). You can reduce this cold-start by pre-configuring your AMI</li> <li>Nodes are ephemeral and tie to a given job id. If needed, you can launch 'AlwaysOn' instances that will be running 24/7.</li> <li>If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path</li> <li>Use the web-based simulator to generate your qsub/script command.</li> </ul> <p>Web Based Job Submission</p> <p>In addition of regular qsub, SOCA supports web based job submission as well as via HTTP REST API</p> <p>To get started, create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **)</p> <pre><code>#!/bin/bash\n## BEGIN PBS SETTINGS: Note PBS lines MUST start with #\n#PBS -N **your_job_name**\n#PBS -V -j oe -o **your_job_name**.qlog\n#PBS -P **your_project**\n#PBS -q **your_queue**\n#PBS -l nodes=**number_of_nodes_for_this_job**\n## END PBS SETTINGS\n## BEGIN ACTUAL CODE\n** your code goes here **\n## END ACTUAL CODE\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#run-your-job","title":"Run your job","text":"<p>Run <code>qsub job_submit.que</code> to submit your job to the queue.  </p> <pre><code>user@host:~$ qsub job_submit.que\n3323.ip-10-10-10-28\n</code></pre> <p>If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run <code>qstat -f 3323</code> (or <code>qstat -f 3323 -x</code> is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission)</p>"},{"location":"tutorials/launch-your-first-job/#delete-a-job-from-the-queue","title":"Delete a job from the queue","text":"<p>Run <code>qdel &lt;job_id&gt;</code> to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes.  </p>"},{"location":"tutorials/launch-your-first-job/#custom-aws-scheduler-resources-optional","title":"Custom AWS scheduler resources (optional)","text":"<p>Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job.  Syntax is as follow:  </p> <ul> <li>In a script: <code>#PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2</code></li> <li>Using qsub: <code>qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh</code></li> </ul> <p>Info</p> <p>If you don't specify them, your job will use the default values configured for your queue (see <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code>) </p>"},{"location":"tutorials/launch-your-first-job/#specify-an-ec2-instance-type-optional","title":"Specify an EC2 Instance Type (optional)","text":"<p>Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value <code>#PBS -l [existing_parameters...],instance_type=**instance_type_value**</code></p>"},{"location":"tutorials/launch-your-first-job/#specify-a-license-restriction-optional","title":"Specify a license restriction (optional)","text":"<p>License Mapping</p> <p>Please refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet.</p> <p>If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. <code>#PBS -l [existing_parameters...],synopsys_lic_vcsruntimenet=2</code> </p>"},{"location":"tutorials/launch-your-first-job/#manage-your-application-logs","title":"Manage your application logs","text":"<p>PBS will automatically generate a .qlog file once the job is complete as shown below. <code>#PBS -V -j oe -o **your_job_name**.qlog</code> If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code  </p>"},{"location":"tutorials/launch-your-first-job/#my-job-is-queued-what-next-aws-orchestration","title":"My job is queued. What next? (AWS orchestration)","text":"<p>First, let's make sure your jobs have been sent to the queue. You can run default <code>qstat</code> or use <code>aligoqstat</code> which is a custom wrapper developed for Scale-Out Computing on AWS.  </p> <p>Web Based</p> <p></p> <p>CLI</p> <p></p> <p>As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run <code>qstat -f **job_id** | grep Resource</code>.</p> <p>Web Based</p> <p></p> <p>CLI</p> <p></p> <p>If you see <code>stack_id</code> or <code>compute_node</code> resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned).  </p> <p> </p> <p>Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id**):  </p> <p> </p> <p>Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running <code>pbsnodes -a</code> Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...)</p> <pre><code>user@host:~$ pbsnodes -a\n#Host Ready\nip-90-0-118-49\n     Mom = ip-90-0-118-49.us-west-2.compute.internal\n     ntype = PBS\n     state = free\n     pcpus = 16\n     jobs = 1.ip-90-0-24-214/0\n     resources_available.arch = linux\n     resources_available.availability_zone = us-west-2a\n     resources_available.compute_node = job1\n     resources_available.host = ip-90-0-118-49\n     resources_available.instance_type = c5.4xlarge\n     resources_available.mem = 31890060kb\n     resources_available.ncpus = 16\n     resources_available.subnet_id = subnet-055c0dcdd6ddbb020\n     resources_available.vnode = ip-90-0-118-49\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\n     resources_assigned.ncpus = 1\n     resources_assigned.vmem = 0kb\n     queue = normal\n     resv_enable = True\n     sharing = default_shared\n     last_state_change_time = Sun Sep 29 23:30:05 2019\n\n# Host not ready yet\nip-90-0-188-37\n     Mom = ip-90-0-188-37.us-west-2.compute.internal\n     ntype = PBS\n     state = state-unknown,down\n     resources_available.availability_zone = us-west-2c\n     resources_available.compute_node = job2\n     resources_available.host = ip-90-0-188-37\n     resources_available.instance_type = r5.xlarge\n     resources_available.subnet_id = subnet-0d046c8668ccfdcb0\n     resources_available.vnode = ip-90-0-188-37\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\n     resources_assigned.ncpus = 0\n     resources_assigned.vmem = 0kb\n     queue = normal\n     comment = node down: communication closed\n     resv_enable = True\n     sharing = default_shared\n     last_state_change_time = Sun Sep 29 23:28:05 2019` \n</code></pre> <p>Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. </p> <p> </p> <p>The web ui will also reflect this change.</p> <p> </p>"},{"location":"tutorials/launch-your-first-job/#examples","title":"Examples","text":"<p>Job Submission Simulator</p> <p>Use the web-based simulator to generate your qsub/script command.</p> <p>How to set a parameter</p> <ul> <li>In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2</li> <li>Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh</li> </ul> <p>Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content:</p> <pre><code>#!/bin/bash\n# Will output the hostname of the host where the script is executed\n# If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job\necho `hostname`\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue","title":"Run a simple script on 1 node using default settings on 'normal' queue","text":"<pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=1\n## END PBS SETTINGS\ncd $HOME\n./script.sh &gt;&gt; my_output.log 2&gt;&amp;1\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue_1","title":"Run a simple script on 1 node using default settings on 'normal' queue","text":"<pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=1\n## END PBS SETTINGS\ncd $HOME\n./script.sh &gt;&gt; my_output.log 2&gt;&amp;1\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-mpi-script-on-3-nodes-using-custom-ec2-instance-type","title":"Run a simple MPI script on 3 nodes using custom EC2 instance type","text":"<p>This job will use a 3 c5.18xlarge instances <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=3,instance_type=c5.18xlarge\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5.18xlarge is 36 cores so -np is 36 * 3 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh &gt; my_output.log\n</code></pre></p>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-3-nodes-using-custom-license-restriction","title":"Run a simple script on 3 nodes using custom License Restriction","text":"<p>This job will only start if we have at least 4 Comsol Acoustic licenses available <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5.18xlarge is 36 cores so -np is 36 * 3 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh &gt; my_output.log\n</code></pre></p>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami","title":"Run a simple script on 5 nodes using custom AMI","text":"<p>This job will use a user-specified AMI ID <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5.18xlarge is 36 cores so -np is 36 * 5 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre></p>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami-using-a-different-os","title":"Run a simple script on 5 nodes using custom AMI using a different OS","text":"<p>This job will use a user-specified AMI ID which use a operating system different than the scheduler <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5.18xlarge is 36 cores so -np is 36 * 5 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre></p>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-25-per-hour","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than $2.5 per hour","text":"<p>This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance</p> <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# m5.24xlarge is 48 cores so -np is 48 * 5 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-od-price","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than OD price","text":"<pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=auto\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# m5.24xlarge is 48 cores so -np is 48 * 5 hosts\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#submit-a-job-with-efa","title":"Submit a job with EFA","text":"<p>Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5n.18xlarge is 36 cores so -np is 36 * 5\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre></p>"},{"location":"tutorials/launch-your-first-job/#use-50-c5xlarge-for-your-job-and-fallback-to-m5xlarge-and-r5xlarge-if-capacity-is-not-available","title":"Use 50 c5.xlarge for your job and fallback to m5.xlarge and r5.xlarge if capacity is not available","text":"<p>AWS honors the instance order, so it will try to provision 50 c5.large first and fallback to m5.xlarge/r5.xlarge if needed (in case your account has instance limitation or AWS can't allocate more than X instance type on a given AZ/region). Ultimately, you may end up with the following configuration (but not limited to):</p> <ul> <li>50 c5.xlarge</li> <li>30 c5.xlarge, 20 m5.xlarge</li> <li>20 c5.xlarge, 20 m5.xlarge, 10 r5.xlarge</li> <li>Or any other combination. The only certain know is that you will get 50 instances</li> </ul> <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=50,instance_type=c5.xlarge+m5.xlarge+r5.xlarge\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5n.18xlarge is 36 cores so -np is 36 * 5\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#use-multiple-spot-instance-type","title":"Use multiple SPOT instance type","text":"<pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=5,instance_type=c5.xlarge+m5.xlarge+r5.xlarge, spot_price=auto\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5n.18xlarge is 36 cores so -np is 36 * 5\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#provision-50-instances-10-on-demand-and-40-spot","title":"Provision 50 instances (10 On-Demand and 40 SPOT)","text":"<pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n#PBS -l nodes=50,instance_type=c5.large,spot_allocation_count=40\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5n.18xlarge is 36 cores so -np is 36 * 5\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/launch-your-first-job/#multi-lines-parameters","title":"Multi-lines parameters","text":"<p>Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id</p> <pre><code>#!/bin/bash\n#PBS -N my_job_name\n#PBS -V -j oe -o my_job_name.qlog\n#PBS -P project_a\n#PBS -q normal\n## Resources can be specified on multiple lines\n#PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes\n#PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde\n## END PBS SETTINGS\ncd $PBS_O_WORKDIR\ncat $PBS_NODEFILE | sort | uniq &gt; mpi_nodes\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/\nexport PATH=$PATH:/apps/openmpi/4.0.1/bin/\n# c5n.18xlarge is 36 cores so -np is 36 * 5\n/apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh &gt; my_output.log\n</code></pre>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/","title":"Import custom AMI to provision capacity faster","text":"<p>By default, SOCA provisions a vanilla AMI and installs all required packages in ~3 to 5 minutes.  If this cold time is not acceptable for your workload, you can launch AlwaysOn instance or pre-bake your AMI with all required libraries.</p> <p>Also see the Custom AMIs section to see how you can use preconfigured EC2 Image Builder pipelines to build custom AMIs.</p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-1-locate-your-base-ami","title":"Step 1: Locate your base AMI","text":"<p>Run <code>cat /etc/environment | grep SOCA_INSTALL_AMI</code> on your scheduler host</p> <pre><code>$ ssh -i &lt;key&gt; ec2-user@&lt;ip&gt;\nLast login: Wed Oct  2 20:06:47 2019 from &lt;ip&gt;\n\n   _____  ____   ______ ___\n  / ___/ / __ \\ / ____//   |\n\\__ \\ / / / // /    / /| |\n___/ // /_/ // /___ / ___ |\n/____/ \\____/ \\____//_/  |_|\nCluster: soca-uiupdates\n&gt; source /etc/environment to SOCA paths\n\n[ec2-user@ip-30-0-1-28 ~]$ cat /etc/environment | grep SOCA_INSTALL_AMI\nexport SOCA_INSTALL_AMI=ami-082b5a644766e0e6f\n[ec2-user@ip-30-0-1-28 ~]$\n</code></pre>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-2-launch-a-temporary-ec2-instance","title":"Step 2: Launch a temporary EC2 instance","text":"<p>Launch a new EC2 instance using the <code>SOCA_INSTALL_AMI</code> image </p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-3-pre-configure-your-ami","title":"Step 3: Pre-configure your AMI","text":"<p>Important</p> <p>Step 3 is only required if you want to reduce the time required for your compute node to boot. You can skip this section if you just want to install your customization on your AMI and let SOCA handles PBS/Gnome/System packages installation. </p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#31-pre-install-system-packages","title":"3.1 Pre-Install system packages","text":"<p>You can pre-install the packages listed on https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg. You will need to run <code>yum install</code> for:</p> <ul> <li>SYSTEM_PKGS</li> <li>SCHEDULER_PKGS</li> <li>OPENLDAP_SERVER_PKGS</li> <li>SSSD_PKGS</li> </ul> <p>Easy Install</p> <ul> <li>Copy the content of the <code>config.cfg</code> on your filesystem (say <code>/root/config.cfg</code>)</li> <li>Run <code>source /root/config.cfg</code></li> <li>Run the following commands:<ul> <li><code>yum install -y $(echo ${SYSTEM_PKGS[*]})</code></li> <li><code>yum install -y $(echo ${SCHEDULER_PKGS[*]})</code></li> <li><code>yum install -y $(echo ${OPENLDAP_SERVER_PKGS[*]})</code></li> <li><code>yum install -y $(echo ${SSSD_PKGS[*]})</code></li> </ul> </li> </ul> <p>Here is an example of how you can install packages listed in an array in bash.</p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#32-pre-install-the-scheduler","title":"3.2: Pre-Install the scheduler","text":"<p>To reduce the launch time of your EC2 instance, it's recommended to pre-install OpenPBS.  First, refer to https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg and note all OpenPBS related variables as you will need to use them below (see highlighted lines):</p> <pre><code># Sudo as Root\nsudo su -\n\n# Define OpenPBS variable\nexport OPENPBS_URL=&lt;variable_from_config.txt&gt; # ex https://github.com/openpbs/openpbs/archive/v20.0.1.tar.gz\nexport OPENPBS_TGZ=&lt;variable_from_config.txt&gt; # ex v20.0.1.tar.gz\nexport OPENPBS_VERSION=&lt;variable_from_config.txt&gt; # ex 20.0.1\n# Run the following command to install OpenPBS\ncd ~\nwget $OPENPBS_URL\ntar zxvf $OPENPBS_TGZ\ncd openpbs-$OPENPBS_VERSION\n./autogen.sh\n./configure --prefix=/opt/pbs\nmake -j6\nmake install -j6\n/opt/pbs/libexec/pbs_postinstall\nchmod 4755 /opt/pbs/sbin/pbs_iff /opt/pbs/sbin/pbs_rcp\nsystemctl disable pbs\n</code></pre> <p>Installation Path</p> <p>Make sure to install OpenPBS under <code>/opt/pbs</code></p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#33-optional-pre-install-gnome","title":"3.3: (Optional) Pre-Install Gnome","text":"<ul> <li> <p>If you are using <code>RHEL</code>, run <code>yum groupinstall \"Server with GUI\" -y</code></p> </li> <li> <p>If you are using <code>Centos</code>, run <code>yum groupinstall \"GNOME Desktop\" -y</code></p> </li> <li> <p>If you are using <code>Amazon Linux</code>, run <code>yum install -y $(echo ${DCV_AMAZONLINUX_PKGS[*]})</code></p> </li> </ul>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#34-reboot-your-ec2-machine","title":"3.4: Reboot your EC2 machine","text":""},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#35-make-sure-you-do-not-have-any-libvirt-of-firewalldiptables","title":"3.5: Make sure you do not have any libvirt of firewalld/iptables","text":"<p>Post reboot, some distribution may automatically start libvirt or firewall. If that's the case you must delete them otherwise PBS won't be able to contact the master scheduler. To find if you have a running libvirt, run <code>ifconfig</code> and check if you have <code>virbr0</code> interface such as:</p> <pre><code>ifconfig\nens5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 9001\ninet 10.10.2.19  netmask 255.255.255.0  broadcast 10.10.2.255\n        inet6 fe80::8b1:6aff:fe8a:5ad8  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 0a:b1:6a:8a:5a:d8  txqueuelen 1000  (Ethernet)\nRX packets 81  bytes 11842 (11.5 KiB)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 92  bytes 12853 (12.5 KiB)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\ninet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\n        loop  txqueuelen 1000  (Local Loopback)\nRX packets 8  bytes 601 (601.0 B)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 8  bytes 601 (601.0 B)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nvirbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500\ninet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255\n        ether 52:54:00:ea:5a:b9  txqueuelen 1000  (Ethernet)\nRX packets 0  bytes 0 (0.0 B)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 0  bytes 0 (0.0 B)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n</code></pre> <p>If that's the case, disable <code>libvirt</code> by running <pre><code>/bin/systemctl disable libvirtd.service\nip link set virbr0 down\nbrctl delbr virbr0\n</code></pre></p> <p>Then, make sure you do not have iptables (<code>iptables -L</code>) running. If needed, disable <code>firewalld</code> by running <code>/bin/systemctl disable firewalld</code></p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-4-create-your-ami","title":"Step 4: Create your AMI","text":"<p>Once you are done, go back to EC2 console, locate your instance and click \"Actions &gt; Image &gt; Create Image\" </p> <p>Choose an AMI name and click 'Create Image'. </p> <p>Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar </p> <p>Stop your temporary EC2 instance</p> <p>Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore.</p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-5-test-your-new-ami","title":"Step 5: Test your new AMI","text":"<pre><code># Test 1: Submit a job with a vanilla AMI\n$ qsub -l instance_type=c5.9xlarge -- /bin/date # Test 2: Submit a job with a pre-configured AMI\n$ qsub -l instance_type=c5.9xlarge -l instance_ami=ami-0e05219e578020c64 -- /bin/date </code></pre> <p>Results:</p> <ul> <li>Test1 (Vanilla): 3 minutes 45 seconds to provision EC2 capacity, register node on SOCA and start the job</li> <li>Test2 (Pre-Configured): 1 minute 44 seconds to provision EC2 capacity, register host on SOCA and start the job</li> </ul>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-6-update-default-ami-optional","title":"Step 6: Update default AMI (Optional)","text":""},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#single-job","title":"Single job","text":"<p>As you are planning to use a custom AMI, you will be required to specify <code>-l instance_ami=&lt;IMAGE_ID&gt;</code> at job submission. It's recommended to go with the \"Entire Queue\" option below if you do not want to manually specify this resource each time you submit a job</p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-queue","title":"Entire queue","text":"<p>Edit <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code> and update the default AMI</p> <pre><code>queue_type:\ncompute:\nqueues: [\"queue1\", \"queue2\", \"queue3\"] instance_ami: \"&lt;YOUR_AMI_ID&gt;\" # &lt;- Add your new AMI \ninstance_type: ...\n</code></pre> <p>Any jobs running in the queue configured on the <code>queue_mapping</code> will now use your pre-configured AMI by default. You do not need to specify <code>-l instance_ami</code> at job submission anymore.</p>"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-cluster","title":"Entire cluster","text":"<p>If you want to change the default AMI to use regardless of queue/job, open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save</p> <p></p>"},{"location":"tutorials/troubleshoot-job-queue/","title":"Debug why your jobs are not starting","text":""},{"location":"tutorials/troubleshoot-job-queue/#jobs-in-dynamic-queue","title":"Jobs in dynamic queue","text":"<p>First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job.</p>"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-log","title":"Verify the log","text":"<p>If your job is not starting, first verify the queue log under <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/&lt;queue_name&gt;.log</code></p> <p>If the log is not created or you don't see any update on it even though you submitted a job, try to run the <code>dispatcher.py</code> command manually. On the scheduler, list all crontabs as root <code>crontab -</code> and refer to \"Automatic Host Provisioning\" section:</p> <pre><code>## Automatic Host Provisioning\n*/3 * * * * source /etc/environment;  /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t compute\n*/3 * * * * source /etc/environment;  /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t desktop\n*/3 * * * * source /etc/environment;  /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t test\n</code></pre> <p>Run the command manually (ex <code>source /etc/environment;  /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATIONcluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t compute</code>) and look for any error. Common errors include malformed yaml files.</p>"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-job-resource","title":"Verify the job resource","text":"<p>This guide assume you have created your queue correctly</p> <p>Run <code>qstat -f &lt;job_id&gt; | grep -i resource</code> and try to locate <code>compute_node</code> or <code>stack_id</code> resource. When your job is launched, these resources does not exist. The script <code>dispatcher.py</code>. running as a crontab and executed every 3 minutes will create these resources automatically.</p> <p>Example of job having all resources configured correctly <pre><code># Job with Scale-Out Computing on AWS resources\nbash-4.2$ qstat -f 2 | grep -i resource\n    Resource_List.instance_type = m5.large\n    Resource_List.ncpus = 3\nResource_List.nodect = 3\nResource_List.nodes = 3\nResource_List.place = scatter\n    Resource_List.select = 3:ncpus=1:compute_node=job2 Resource_List.stack_id = soca-fpgaami-job-2\n</code></pre></p> <p>Please note these resources are created by <code>dispatcher.py</code> so allow a maximum of 3 minutes between job is submitted and resources are visibles on <code>qstat</code> output <pre><code># Job without Scale-Out Computing on AWS resources created yet\nbash-4.2$ qstat -f 2 | grep -i resource\n    Resource_List.instance_type = m5.large\n    Resource_List.ncpus = 3\nResource_List.nodect = 3\nResource_List.nodes = 3\nResource_List.place = scatter\n    Resource_List.select = 3:ncpus=1\n</code></pre></p> <p>If you see a <code>compute_node</code> different than <code>tbd</code> as well as <code>stack_id</code>, that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see  a new stack being created using the following naming convention: <code>soca-&lt;cluster_name&gt;-job-&lt;job_id&gt;</code></p>"},{"location":"tutorials/troubleshoot-job-queue/#retrieve-node-logs","title":"Retrieve node logs","text":"<p>On the master host, access <code>/apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/</code>. This folder contains the output of all logs for all hosts provisioned by SOCA</p> <pre><code># Retrieve logs for the most recent (2 weeks) jobs\nls -ltr /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/ | tail -n 5\ndrwxr-xr-x   3 root root  6144 Jul 21 17:02 19607\ndrwxr-xr-x   3 root root  6144 Jul 21 17:16 19608\ndrwxr-xr-x   3 root root  6144 Jul 21 17:21 19609\ndrwxr-xr-x   6 root root  6144 Jul 21 17:40 19575\ndrwxr-xr-x  10 root root  6144 Jul 21 17:44 19606\n\n# Filter for a specific job id. Each nodes provisioned for this job will show up on the directory\nls -ltr /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/19606 | tail -n 5\ndrwxr-xr-x 2 root root 6144 Jul 21 17:47 ip-10-10-99-2\ndrwxr-xr-x 2 root root 6144 Jul 21 17:47 ip-10-10-102-78\ndrwxr-xr-x 2 root root 6144 Jul 21 17:48 ip-10-10-101-45\ndrwxr-xr-x 2 root root 6144 Jul 21 17:48 ip-10-10-85-64\ndrwxr-xr-x 2 root root 6144 Jul 21 17:48 ip-10-10-77-184\n\n# For each hosts, you will be able to retrieve the install logs and do any troubleshooting\nls -ltr /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/19606/ip-10-10-85-64\n-rw-r--r-- 1 root root 77326 Jul 21 17:47 ComputeNode.sh.log\n-rw-r--r-- 1 root root   864 Jul 21 17:48 ComputeNodePostReboot.log\n-rw-r--r-- 1 root root    12 Jul 21 17:48 ComputeNodeUserCustom.log\n-rw-r--r-- 1 root root 73009 Jul 21 17:48 ComputeNodeUserCustomization.log\n-rw-r--r-- 1 root root 77995 Jul 21 17:48 ComputeNodeConfigureMetrics.log\n</code></pre>"},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-not-create_complete","title":"If CloudFormation stack is NOT \"CREATE_COMPLETE\"","text":"<p>Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors</p> <p></p> <p>In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using  <code>-l root_size=75</code></p>"},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-create_complete","title":"If CloudFormation stack is \"CREATE_COMPLETE\"","text":"<p>First, make sure CloudFormation has created a new \"Launch Template\" for your job.</p> <p></p> <p>Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab.</p> <p>Here is an example of capacity being provisioned correctly</p> <p></p> <p>Here is an example of capacity provisioning errors:</p> <p></p> <p>If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run <code>pbsnodes -a</code>. Verify the capacity assigned to your job ID (refer to <code>resources_available.compute_node</code>) is in <code>state = free</code>.</p> <pre><code>pbsnodes -a\nip-60-0-174-166\n     Mom = ip-60-0-174-166.us-west-2.compute.internal\n     Port = 15002\n     pbs_version = 18.1.4\n     ntype = PBS\n     state = free\n     pcpus = 1\n     resources_available.arch = linux\n     resources_available.availability_zone = us-west-2c\n     resources_available.compute_node = job2\n     resources_available.host = ip-60-0-174-166\n     resources_available.instance_type = m5.large\n     resources_available.mem = 7706180kb\n     resources_available.ncpus = 1\n     resources_available.subnet_id = subnet-0af93e96ed9c4377d\n     resources_available.vnode = ip-60-0-174-166\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\n     resources_assigned.ncpus = 0\n     resources_assigned.vmem = 0kb\n     queue = normal\n     resv_enable = True\n     sharing = default_shared\n     last_state_change_time = Sat Oct 12 17:37:28 2019\n</code></pre> <p>If host is not in <code>state = free</code> after 10 minutes, SSH to the host, sudo as root and check the log file located under <code>/root</code> as well as <code>/var/log/message | grep cloud-init</code></p>"},{"location":"web-interface/","title":"About","text":""},{"location":"web-interface/#web-user-interface","title":"Web User Interface","text":"<p>Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as:</p> <ul> <li>Start/Stop virtual desktops (Windows/Linux) sessions in 1 click</li> <li>Download private key in both PEM or PPK format</li> <li>Check the queue and job status in real-time</li> <li>Add/Remove LDAP users </li> <li>Access the analytic dashboard</li> <li>Access your filesystem</li> <li>Create Application profiles and let your users submit job directly via the web interface</li> <li>Understand why your jobs are stuck in the queue</li> <li>And more .. (refer to the left sidebar for additional resources)</li> </ul>"},{"location":"web-interface/#http-rest-api","title":"HTTP Rest API","text":"<p>Users can submit/retrieve/delete jobs remotely via an HTTP REST API</p>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/","title":"Control your job with HTTP/REST API","text":"<p>API Documentation</p> <p>You can interact with your SOCA cluster (create users, groups, queue, submit jobs, view jobs etc ..) via a simple REST interface. Documentation and examples can be found on <code>https://your_soca_url/api/doc</code> (note: Your SOCA must have a valid SSL certificate</p> <p>Submit a job via web</p> <p>In addition of REST API, SOCA also support job management via a web based interface. Click here to learn more</p>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step1-retrieve-your-api-key","title":"Step1: Retrieve your API key","text":"<p>To retrieve your API key, navigate to \"My API Key\" section on the left sidebar. </p> <p>Important</p> <p>Your API key is unique and linked to your account. Do not share it with anyone. If you think your key has been compromised, reset it immediately via the link on the same page</p>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step2-prepare-your-job-input","title":"Step2: Prepare your job input","text":"<p>For this example, let's assume we want to submit a simple \"Hello World\" job as shown below:</p> <pre><code>#!/bin/bash\n#PBS -N testjob\n#PBS -V -j oe -o testjob_output.qlog\n#PBS -P myproject\n#PBS -q normal\n#PBS -l nodes=1,instance_type=c5.large\n/bin/echo \"Hello World\"\n</code></pre> <p>To be able to send this job to SOCA via HTTP, you must first encode this file using <code>base64.</code> There are multiple ways to create a base64 hash, on Linux/Mac, the easiest method is to use <code>base64</code> encode function:</p> <pre><code>$ base64 job_submit.sh\nIyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\n</code></pre> <p>You can verify the hash is correct by running the <code>base64</code> decode function (this should return your original input file)</p> <pre><code>$ echo \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo\" | base64 --decode\n#!/bin/bash\n#PBS -N testjob\n#PBS -V -j oe -o testjob_output.qlog\n#PBS -P myproject\n#PBS -q normal\n#PBS -l nodes=1,instance_type=c5.large\n/bin/echo \"Hello World\"\n</code></pre>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step3-send-the-job-to-soca","title":"Step3: Send the job to SOCA","text":"<p>To be able to submit the POST request, you will need to specify three headers:</p> <ul> <li>X-SOCA-USER and set the value to <code>&lt;YOUR_SOCA_USER&gt;</code></li> <li>X-SOCA-TOKEN and set the value to <code>&lt;YOUR_SOCA_TOKEN&gt;</code></li> <li>Content-Type and set the value to <code>multipart/form-data</code></li> </ul> <p>Once you have your headers configured, submit a HTTP/POST request and pass your hash as form data via <code>payload</code> parameter:</p> <pre><code>curl -X POST \\\nhttps://&lt;YOUR_SOCA_URL&gt;/api/scheduler/job \\\n-H 'X-SOCA-TOKEN: &lt;YOUR_SOCA_TOKEN&gt;' \\\n-H 'X-SOCA-USER: &lt;YOUR_SOCA_USER&gt;' \\\n-F payload=IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\n</code></pre> <p>If your hash is a valid PBS job file, SOCA will return the job id associated to your request (11313 in this example)</p> <pre><code>{\"success\": true, \"message\": \"11313\"}\n</code></pre>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step4-get-job-info","title":"Step4: Get job info","text":"<p>To retrieve information about a given job (assuming you are the job owner), simply submit a HTTP/GET request and pass <code>job_id</code> as parameter</p> <pre><code>curl -X GET \\\nhttps://&lt;YOUR_SOCA_URL&gt;/api/scheduler/job?job_id=11313 \\\n-H 'X-SOCA-TOKEN: &lt;YOUR_SOCA_TOKEN&gt;' \\\n-H 'X-SOCA-USER: &lt;YOUR_SOCA_USER&gt;'\n</code></pre> <p>This command will return a JSON object with all information regarding your job.</p> <pre><code>{\n\"success\": true,\n\"message\": {\n\"Job_Name\": \"testjob\",\n\"Job_Owner\": \"&lt;YOUR_SOCA_USER&gt;@ip-10-10-0-75.us-west-2.compute.internal\",\n\"job_state\": \"Q\",\n\"queue\": \"normal\",\n\"server\": \"ip-10-10-0-75\",\n\"Checkpoint\": \"u\",\n\"ctime\": \"Thu May 21 02:47:05 2020\",\n\"Error_Path\": \"ip-10-10-0-75.us-west-2.compute.internal:/data/home/&lt;YOUR_SOCA_USER&gt;/soca_job_output/testjob_pGep6UiWpK/testjob.e11313\",\n\"Hold_Types\": \"n\"\n....\n}\n}\n</code></pre>"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step4-delete-job","title":"Step4: Delete job","text":"<p>To delete a job, simply submit a <code>HTTP/DELETE</code> request and specify <code>job_id</code> parameter:</p> <pre><code>curl -X DELETE \\\nhttps://&lt;YOUR_SOCA_URL&gt;/api/scheduler/job?job_id=11313 \\\n-H 'X-SOCA-TOKEN: &lt;YOUR_SOCA_TOKEN&gt;' \\\n-H 'X-SOCA-USER: &lt;YOUR_SOCA_USER&gt;'\n</code></pre> <p>If the command is valid, you will receive a validation message:</p> <pre><code> {\"success\": true, \"message\": \"Job deleted\"}\n</code></pre> <p>You can verify the Job has been removed from the queue using the same HTTP/GET request:</p> <pre><code> curl -X GET \\\nhttps://&lt;YOUR_SOCA_URL&gt;/api/scheduler/job?job_id=11313\\\n-H 'X-SOCA-TOKEN: &lt;YOUR_SOCA_TOKEN&gt;' \\\n-H 'X-SOCA-USER: &lt;YOUR_SOCA_USER&gt;'\n</code></pre> <p>This time the output will return an error:</p> <pre><code>{\"success\": false, \"message\": \"Unable to retrieve Job ID (job may have terminated and is no longer in the queue)\"}\n</code></pre>"},{"location":"web-interface/create-virtual-desktops-images/","title":"Create Virtual Desktop Images for Windows/Linux","text":"<p>Feature in preview</p> <p>This feature is only available on beta builds</p> <p>By default, SOCA only provide base DCV image for Windows, meaning no applications are pre-installed. Customers can create their own bundle using Amazon Images (AMI) and let their user choose what type of software they want to see pre-installed on their graphical sessions</p>"},{"location":"web-interface/create-virtual-desktops-images/#windows","title":"Windows","text":"<p>Important</p> <p>An image is a complete snapshot of your EC2 host. Make sure you do not have any confidential data hosted on it before creating the image</p> <p>First, launch a simple Windows graphical session and install some applications. In this example, I have installed Creo</p> <p></p> <p>To prepare your image, you first need to configure your system. Click the Start button and search for <code>Powershell</code></p> <p></p> <p>On the Powershell terminal, execute <code>C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule</code></p> <p></p> <p>Go back to the Windows session list and retrieve the instance ID associated to your session (mouse hover the question mark icon)</p> <p></p> <p>On your EC2 console and select your instance</p> <p></p> <p>Click \"Actions\" -&gt; \"Image\" -&gt; \"Create Image\"</p> <p></p> <p>Choose a name and a description, make sure to check \"No Reboot\" then click Create Image</p> <p></p> <p>Navigate to the AMI tab and verify if your image status is \"available\"</p> <p></p> <p>My Image is taking forever to be created</p> <p>To check the progress of your image, navigate to \"Snapshots\" section and refer to the <code>Progress</code> column for all EBS volumes created by your image </p> <p>Info</p> <p>You will not be able to use your image until the status is available. </p> <p>Creating an AMI may take a couple of hours depending the size of the image</p> <p>AMI is region specific</p> <p>AMI has 3 syllabes and is pronounced \"Ay-Em-I\".</p> <p>Once your AMI is ready, login to your SOCA web interface and go to \"AMI management\" under \"Admin\" section</p> <p>Fill out the form (specify the AMI ID, operating system, minimum storage requirement as well as pick a friendly label)</p> <p></p> <p>What is root disk size?</p> <p>Root disk size is the minimum storage required for your AMI to boot up. In other words, if you created your AMI with 150GB root disk, then you won't be able to launch any instance based on this AMI unless the associated EBS disk is greater or equal than the base storage. SOCA will automatically honor the minimum storage required by the image if users choose a lower value. </p> <p>Once done, click 'Register AMI in SOCA', you will get a success message if everything is configured correctly</p> <p></p> <p>Now go back to the \"Windows Session\" section, this time you should be able to see your newly create image under <code>Software Stack</code></p> <p></p> <p>You can now launch your session with a pre-configured image.</p>"},{"location":"web-interface/create-virtual-desktops-images/#linux","title":"Linux","text":"<p>Refer to this article to learn how to create Linux images</p>"},{"location":"web-interface/create-virtual-desktops/","title":"Create Virtual Desktops (Windows/Linux)","text":"<p>Feature in preview</p> <p>This feature is only available on beta builds</p> <p>SOCA lets you deploy Windows and/or Linux desktops using NICE DCV technology. To get started, select either \"Linux Desktop\" or \"Windows Desktop\" in the left sidebar.</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#create-your-windowslinux-desktop","title":"Create your Windows/Linux desktop","text":"<p>To launch your Windows/Linux desktop, you first have to pick a name, choose how much storage you want to allocate as well as specify other options such as hibernation support, compute type, software stack or even subnet id.</p> <p></p> <p>Need help?</p> <p>Hover your mouse on the parameter name to display help section.</p> <p></p> <p>Once you are ready, click \"Launch My Session\" button. Your session interface will be changed to pending state.</p> <p></p> <p>Your virtual desktop will be ready within 10-15 minutes. Startup time is based on the image selected, the operating system as well as the instance type. Windows tends to boot faster as some components such as the scheduler or EFS are not installed.</p> <p>Nvidia drivers for GPU instances</p> <p>SOCA automatically install Nvidia GRID (G3/G4) drivers. Tesla drivers (P3) have to be installed separately by the customers.</p> <p>Once your virtual desktop is active, your session interface will display two ways to connect to it:</p> <p></p> <p>You can either access your desktop directly via your browser (Option1) or use the NICE DCV native application (Option2). A link to download the application is provided on the website. For better performance, we recommend using the Option 2.</p>"},{"location":"web-interface/create-virtual-desktops/#example-of-linux-desktop","title":"Example of Linux desktop","text":""},{"location":"web-interface/create-virtual-desktops/#example-of-windows-desktop","title":"Example of Windows desktop","text":"<p>Important</p> <p>Unlike Linux desktops, Windows desktops are not connected to the scheduler and you cannot submit jobs directly from your Windows system.</p> <p>To submit a job via Windows, you first need to access the web ui then submit a job via HTTP endpoint or via the HTTP REST API</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#stophibernate-your-desktop","title":"Stop/Hibernate your desktop","text":"<p>What does hibernate means?</p> <p>When you hibernate an instance, your desktop state is saved in memory. When you restart it, all your applications will automatically resume. On the other hand, stopping a virtual desktop is the same as powering off your laptop. Note not all EC2 instances support hibernation.</p> <p>To stop/hibernate your session, click the \"Stop\" (or \"Hibernate\") button in the top bar.</p> <p></p> <p>A popup will appear, asking you to confirm your action. Click \"Stop/Hibernate my session\" button to temporarily turn off your virtual desktop.</p> <p></p> <p>Your desktop is now stopped (you are not charged for compute price while your instance is stopped). To restart your session simply click \"Restart your session\" button.</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#configure-your-desktop-auto-startstop-time","title":"Configure your desktop auto start/stop time","text":"<p>To change the schedule of your desktop, click the \"Schedule\" button in the top bar.</p> <p></p> <p>SOCA will honors your session schedule and automatically start/stop your desktop based on your own schedule. Desktop will be stopped outside of scheduled hours only if they are idle. You can configure three type of schedule for your desktop:</p> <ul> <li>Run all day (SOCA will ensures your desktop is up and running from 12 AM to 12 PM)</li> <li>Stopped all day (SOCA will ensures your desktop is stopped from 12 AM to 12 PM)</li> <li>Custom schedule (SOCA will honors your own schedule)</li> </ul> <p></p> <p>Info</p> <p>If you manually start your desktop during off hours, your session will be up and running until it becomes idle. Idle time can be customized by cluster admins.</p> <p>Schedule and Timezone</p> <p>By default, SOCA is configured to use UTC timezone and this may be a problem if you are currently using a different timezone. To fix that, you can adjust your local timezone on <code>config.py</code> (then restart the web ui)</p>"},{"location":"web-interface/create-virtual-desktops/#update-your-hardware-requirements-on-the-fly","title":"Update your hardware requirements on the fly","text":"<p>You can upgrade/downgrade the hardware of your desktop only when your session is stopped and if you have disabled hibernation. To do so, click </p> <p></p> <p>Choose the new type of instance you want to resize your desktop to then click \"Change Instance Type\".</p> <p></p> <p>Hardware upgrade/downgrade are instant. Your desktop will use the updated config as soon as you restart it.</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#terminate-your-desktop","title":"Terminate your desktop","text":"<p>To terminate your desktop, click the \"Kill\" button from the menu bar.</p> <p></p> <p>You will be prompted for a confirmation message. Terminate a session may cause data loss if you are using ephemeral storage, so make sure to have uploaded all your data back to SOCA filesystem first.</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#windows-only-retrieve-local-admin-password","title":"(Windows only) Retrieve local admin password","text":"<p>As of today, Windows desktops are not configured to OpenLDAP, meaning you will use a local Windows account unique to each session. To retrieve the password for the local Administrator account, click \"Get Password\" icon.</p> <p></p> <p>This will open a new window where you can retrieve the password.</p> <p></p>"},{"location":"web-interface/create-virtual-desktops/#create-custom-windowslinux-images","title":"Create custom Windows/Linux images","text":"<p>SOCA images are Windows or Linux desktops with pre-installed applications. Refer to this page to learn more about images management</p>"},{"location":"web-interface/create-virtual-desktops/#how-idle-time-is-calculated","title":"How idle time is calculated?","text":"<p>When you have enabled <code>DCV_LINUX_HIBERNATE_IDLE_SESSION</code>, <code>DCV_LINUX_STOP_IDLE_SESSION</code>, <code>DCV_WINDOWS_HIBERNATE_IDLE_SESSION</code> or <code>DCV_WINDOWS_STOP_IDLE_SESSION</code> configured, SOCA will automatically try to hibernate/stop your idle instance if they are outside of regular schedule hours.</p> <p>Idle time is calculated based on:</p> <ul> <li>Last time the user accessed the virtual desktop</li> <li>Current CPUs usage.</li> </ul> <p>Session will only be stopped/hibernated if current CPUs is below a threshold configured via <code>DCV_IDLE_CPU_THRESHOLD</code> (default to 15%). This setting avoid desktop being stopped/hibernated while compute intensive tasks are still running on, even if the user did not access the desktop for a while.</p>"},{"location":"web-interface/create-virtual-desktops/#common-configuration","title":"Common configuration","text":"<pre><code>#  General\nTIMEZONE = \"UTC\"  # Change to match your local timezone if needed. See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for all TZ\n\nDCV_FORCE_INSTANCE_HIBERNATE_SUPPORT = False  # If True, users can only provision instances that support hibernation\nDCV_TOKEN_SYMMETRIC_KEY = os.environ[\"SOCA_DCV_TOKEN_SYMMETRIC_KEY\"]  # used to encrypt/decrypt and validate DCV session auth\nDCV_BLACKLIST_INSTANCE_TYPE = ['metal', 'nano', 'micro', 'p3', 'p2']  # This instance type won't be visible on the dropdown menu\nDCV_IDLE_CPU_THRESHOLD = 15  # SOCA will NOT hibernate/stop an instance if current CPU usage % is over this value\nALLOW_DOWNLOAD_FROM_PORTAL = True  # Give user ability to download files from the web portal\n\n# DCV Linux\nDCV_LINUX_SESSION_COUNT = 4\nDCV_LINUX_ALLOW_INSTANCE_CHANGE = True  # Allow user to change their instance type if their DCV session is stopped\nDCV_LINUX_HIBERNATE_IDLE_SESSION = 1  # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable\nDCV_LINUX_STOP_IDLE_SESSION = 1  # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable\nDCV_LINUX_TERMINATE_STOPPED_SESSION = 0  # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable\n\n# DCV Windows\nDCV_WINDOWS_SESSION_COUNT = 4\nDCV_WINDOWS_ALLOW_INSTANCE_CHANGE = True  # Allow user to change their instance type if their DCV session is stopped\nDCV_WINDOWS_HIBERNATE_IDLE_SESSION = 1  # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable\nDCV_WINDOWS_STOP_IDLE_SESSION = 1  # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable\nDCV_WINDOWS_TERMINATE_STOPPED_SESSION = 0  # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable\nDCV_WINDOWS_AUTOLOGON = True  # enable or disable autologon. If disabled user will have to manually input Windows password\n DCV_WINDOWS_AMI = {\"graphics\": {\"us-east-1\": \"ami-035a352d4d53371dc\",\n                                   ....\n                                    \"ap-south-1\": \"ami-09c1d03de366041a4\"},\n                       \"non-graphics\": {\"us-east-1\": \"ami-021660b17250fbc9b\",\n                                        ....\n                                        \"ap-south-1\": \"ami-08e852f6df553818a\"}}\n</code></pre>"},{"location":"web-interface/create-your-own-queue/","title":"Create your own queue","text":"<p>Things to know before you start</p> <p>By default, Scale-Out Computing on AWS creates 4 queues: high, normal (default), low and alwayson.</p>"},{"location":"web-interface/create-your-own-queue/#queue-with-automatic-instance-provisioning","title":"Queue with automatic instance provisioning","text":""},{"location":"web-interface/create-your-own-queue/#create-the-queue","title":"Create the queue","text":""},{"location":"web-interface/create-your-own-queue/#via-the-web-ui","title":"Via the web UI","text":"<p>As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Automatic Provisioning\"</p> <p></p>"},{"location":"web-interface/create-your-own-queue/#via-command-line","title":"Via command-line","text":"<p>On your scheduler host, run <code>qmgr</code> as root and enter the following commands: <pre><code># Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only\nQmgr:create queue &lt;queue_name&gt;\n\n# Set the queue to execution\nQmgr:set queue &lt;queue_name&gt; queue_type = Execution\n\n# Set default compute node - See below for more information\nQmgr:set queue &lt;queue_name&gt; default_chunk.compute_node = tbd\n\n# Enable / Start the queue\nQmgr:set queue &lt;queue_name&gt; enabled = True\nQmgr:set queue &lt;queue_name&gt; started = True\n\n# Exit\nQmgr:exit\n</code></pre></p> <p>What is compute_node=tbd</p> <p>On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes</p>"},{"location":"web-interface/create-your-own-queue/#configure-automatic-host-provisioning","title":"Configure automatic host provisioning","text":"<p>If you want to enable automatic host provisioning, edit this file: <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml</code></p>"},{"location":"web-interface/create-your-own-queue/#option1-i-want-to-use-the-same-settings-as-an-existing-queue","title":"Option1: I want to use the same settings as an existing queue","text":"<p>In this case, simply update the array with your new queue</p> <pre><code>queue_type:\ncompute:\nqueues: [\"queue1\", \"queue2\", \"queue3\"] # &lt;- Add your queue to the array\ninstance_ami: \"ami-1234567\"\ninstance_type: \"c5.large\"\n...\n</code></pre>"},{"location":"web-interface/create-your-own-queue/#option2-i-want-to-configure-specific-settings","title":"Option2: I want to configure specific settings","text":"<p>In this case, you will first need to create a new section on the YAML file (see example with memory)</p> <pre><code>queue_type:\ncompute:\nqueues: [\"queue1\"]\ninstance_ami: \"ami-1234567\"\ninstance_type: \"c5.large\"\nscratch_size: \"100\"\nmemory: # &lt;- Add new section\nqueues: [\"queue2\"]\ninstance_ami: \"ami-9876543\"\ninstance_type: \"r5.24xlarge\"\nscratch_size: \"600\"\n</code></pre> <p>Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created  </p> <pre><code>*/3 * * * * source /etc/environment;  /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t memory\n</code></pre>"},{"location":"web-interface/create-your-own-queue/#automatic-host-provisioning-logs","title":"Automatic Host provisioning logs","text":"<p>All logs queues are stored under <code>/apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/&lt;queue_name&gt;</code></p>"},{"location":"web-interface/create-your-own-queue/#queue-with-alwayson-instances","title":"Queue with AlwaysOn instances","text":"<p>Important</p> <ul> <li>Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation </li> <li>In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack</li> </ul>"},{"location":"web-interface/create-your-own-queue/#create-the-queue_1","title":"Create the queue","text":""},{"location":"web-interface/create-your-own-queue/#via-the-web-ui_1","title":"Via the web UI","text":"<p>As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Always On\"</p>"},{"location":"web-interface/create-your-own-queue/#via-command-line_1","title":"Via command-line","text":"<p>On your scheduler host, run <code>qmgr</code> as root and enter the following commands:</p> <pre><code> # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only\nQmgr:create queue &lt;queue_name&gt;\n\n# Set the queue to execution\nQmgr:set queue &lt;queue_name&gt; queue_type = Execution\n\n# Enable / Start the queue\nQmgr:set queue &lt;queue_name&gt; enabled = True\nQmgr:set queue &lt;queue_name&gt; started = True\n\n# Exit\nQmgr:exit\n</code></pre>"},{"location":"web-interface/create-your-own-queue/#start-provisioning-some-capacity","title":"Start provisioning some capacity","text":"<p>Run <code>python3 apps/soca/cluster_manager/add_nodes.py</code> and enable <code>--keep_forever True</code> flag</p> <p><pre><code># Launch 1 c5.large always on\npython3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py --instance_type c5.large \\\n--desired_capacity 1 \\\n--queue &lt;queue_name&gt; \\\n--job_name instancealwayson \\\n--job_owner mcrozes \\\n--keep_forever True\n\n IMPORTANT:\n You specified --keep-forever flag. This instance will be running 24/7 until you MANUALLY terminate the Cloudformation Stack                            </code></pre> If you need help with this script, run <code>python3 add_nodes.py -h</code></p>"},{"location":"web-interface/create-your-own-queue/#delete-alwayson-capacity","title":"Delete AlwaysOn capacity","text":"<p>Simply go to your CloudFormation console, locate the stack following the naming convention: soca-cluster-name-keepforever-queue_name-uniqueid and terminate it. </p>"},{"location":"web-interface/create-your-own-queue/#delete-a-queue","title":"Delete a queue","text":"<p>Via the web ui, go to \"Queue Management\" then navigate to the \"Delete Queue\" tab</p> <p></p>"},{"location":"web-interface/disable-api/","title":"Disable API","text":"<p>If required, SOCA administrators can disable web API or views by using <code>@disabled</code> decorator</p>"},{"location":"web-interface/disable-api/#disable-an-api","title":"Disable an API","text":"<p>First, let's confirm a user can submit a job via the <code>/api/scheduler/job</code> endpoint:</p> <pre><code> curl -k -X POST \\\n&gt;    -H \"X-SOCA-TOKEN: xxx\" \\\n&gt;    -H \"X-SOCA-USER: mickael\" \\\n&gt;    -F payload=\"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\"  \\\n&gt;    https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job\n{\"success\": true, \"message\": \"0\"}\n</code></pre> <p>Edit <code>/apps/soca/$SOCA_CONFIGURATION/cluter_web_ui/api/v1/scheduler/pbspro/job.py</code> and import the new decorator</p> <pre><code>from decorators import disabled\n</code></pre> <p>Locate  the API you want to disable and replace the current decorator with <code>@disabled</code></p> <p>Before: <pre><code>@private_api\ndef post(self):\n    // code     \n</code></pre></p> <p>After: <pre><code>@disabled\ndef post(self):\n   // code \n</code></pre></p> <p>Restart SOCA web interface via <code>socawebui.sh stop/start</code> and validate you cannot use the API anymore</p> <pre><code> curl -k -X POST \\\n&gt;    -H \"X-SOCA-TOKEN: xxx\" \\\n&gt;    -H \"X-SOCA-USER: mickael\" \\\n&gt;    -F payload=\"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\"  \\\n&gt;    https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job\n{\"success\": false, \"message\": \"This API has been disabled by your Administrator\"}\n</code></pre> <p>If you want to re-enable the API, simply configure the decorator back to its previous version (<code>@private_api</code>). Restart the web interface again and verify the API is now enabled: <pre><code> curl -k -X POST \\\n&gt;    -H \"X-SOCA-TOKEN: xxx\" \\\n&gt;    -H \"X-SOCA-USER: mickael\" \\\n&gt;    -F payload=\"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\"  \\\n&gt;    https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job\n{\"success\": true, \"message\": \"1\"}\n</code></pre></p>"},{"location":"web-interface/disable-api/#disable-a-view","title":"Disable a view","text":"<p>Process is very similar, locate the HTTP view you want to restrict. For example edit <code>/apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/views/remote_desktop.py</code></p> <p>Import the new decorator</p> <pre><code>from decorators import login_required, disabled\n</code></pre> <p>Then replace the current decorator of the view you want to restrict with <code>@disabled</code></p> <pre><code>@remote_desktop.route('/remote_desktop', methods=['GET'])\n@disabled\ndef index():\n    // code\n</code></pre> <p>Restart the Web UI. Accessing the view will now redirect you back to your homepage</p> <p></p>"},{"location":"web-interface/import-export-application-profiles/","title":"Import/Export application profiles","text":"<p>Feature in preview</p> <p>This feature is only available on beta builds</p> <p>Refer to this page to learn how you can easily share your application profiles between multiple SOCA clusters.</p>"},{"location":"web-interface/import-export-application-profiles/#export-an-existing-application","title":"Export an existing application","text":"<p>To share your application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab</p> <p></p> <p>Select the application you want to export from the dropdown menu then click \"Export\"</p> <p></p> <p>This will download a <code>json</code> file. Share this <code>json</code> file with whoever want to be able to use your application profile on their SOCA environment</p> <p></p>"},{"location":"web-interface/import-export-application-profiles/#import-an-existing-application","title":"Import an existing application","text":"<p>To import an application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab</p> <p></p> <p>Specify an application name,  upload a valid <code>json</code> then click Import</p> <p></p> <p>Your application will be imported successfully assuming the <code>json</code> provided is a valid SOCA application profile</p> <p></p> <p>Your application is now available on SOCA. You can edit it to make any change based on your own environment or start using it the way it is.</p>"},{"location":"web-interface/manage-ldap-users/","title":"Centralized user/group management","text":""},{"location":"web-interface/manage-ldap-users/#using-web-ui","title":"Using Web UI","text":"<p>Log in to the Web UI with an admin account and locate \"Users Management\" or \"Group Management\" sections on the left sidebar.</p> <p></p> <p>Info</p> <p>Users and Group management are limited to admins users </p>"},{"location":"web-interface/manage-ldap-users/#users","title":"Users","text":""},{"location":"web-interface/manage-ldap-users/#add-users","title":"Add users","text":"<p>To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking  \"Enable Sudo Access\" checkbox. If needed, you can also manually force UID/GID or choose a shell different than <code>/bin/bash</code>.</p> <p></p> <p>You will see a success message if the user is created correctly </p> <p>What is a SUDO user?</p> <p>Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users.</p> <p>Custom shell</p> <p>SOCA uses <code>/bin/bash</code> by default but admins can specify any available shells installed on the system ( list available on <code>/etc/shells</code>)</p>"},{"location":"web-interface/manage-ldap-users/#delete-users","title":"Delete users","text":"<p>To delete a user, navigate to 'Delete Users' section then select the user you want to delete and check the checkbox.</p> <p></p> <p>You will see a success message if the user is deleted correctly.</p> <p></p> <p>Info</p> <p>Deleting a user will only delete the LDAP user. Associated $HOME directory is still preserved on /data/home</p>"},{"location":"web-interface/manage-ldap-users/#reset-password-for-a-given-user","title":"Reset password for a given user","text":"<p>Users can change their own password via the web ui. If needed, admins can also temporarily unlock a user by resetting the password on his/her behalf.</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#manage-sudo-admin-permission","title":"Manage SUDO (admin permission)","text":"<p>Admins can grant/revoke SUDO permissions for any user:</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#groups","title":"Groups","text":""},{"location":"web-interface/manage-ldap-users/#create-a-new-group","title":"Create a new group","text":"<p>To create a new group, simply select \"Create a Group\" and select the user(s) you want to add to this group.</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#check-group-membership","title":"Check group membership","text":"<p>You can check group membership by going to \"Check group membership\" tab.</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#change-group-membership","title":"Change group membership","text":"<p>If needed, you can add/remove users from a given groups.</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#delete-group","title":"Delete group","text":"<p>Lastly, to delete a group, simply navigate to \"Delete Group\" tab.</p> <p></p>"},{"location":"web-interface/manage-ldap-users/#other-ldap-operations","title":"Other LDAP operations","text":"<p>Attention</p> <p>It's recommended to interact with OpenLDAP via the web ui interface.</p> <p>Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly.</p> <p>Scale-Out Computing on AWS LDAP Schema</p> <ul> <li>People: OU=People,DC=soca,DC=local</li> <li>Groups: OU=Group,DC=soca,DC=local</li> <li>Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster)</li> </ul> <p>Admin LDAP account credentials</p> <ul> <li>Bind DN (-D): cn=admin,dc=soca,dc=local </li> <li>Password (-y) /root/OpenLdapAdminPassword.txt</li> </ul> <p>For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content:</p> <pre><code>dn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nobjectClass: top\nobjectClass: posixGroup\ncn: mynewgroup\ngidNumber: 6000\nmemberUid: mytestuser\n</code></pre> <p>Run the following <code>ldapadd</code> command to add your new group: <pre><code>ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif\nadding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\"\n</code></pre></p> <p>Finally valid your group has been created correctly using <code>ldapsearch</code> <pre><code># Validate with Ldapsearch\n~ ldapsearch -x cn=mynewgroup\n#Extended LDIF\n#\n# LDAPv3\n# base DC=soca,DC=local (default) with scope subtree\n# filter: cn=mynewgroup\n# requesting: ALL\n#\n\n# mynewgroup, Group, soca.local\ndn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nobjectClass: top\nobjectClass: posixGroup\ncn: mynewgroup\ngidNumber: 6000\nmemberUid: mytestuser\n</code></pre></p> <p>Example for LDIF modify operation <pre><code>dn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nchangetype: modify\nadd: memberUid\nmemberUid: anotheruser\n</code></pre> Example for LDIF delete operation <pre><code>dn: cn=mynewgroup,ou=Group,dc=soca,dc=local\nchangetype: modify\ndelete: memberUid\nmemberUid:: anotheruser # you get the memberUid by running a simple ldapsearch first\n</code></pre></p>"},{"location":"web-interface/manage-ldap-users/#give-users-permissions-to-submit-job","title":"Give users permissions to submit job","text":"<p>By default, users can submit job to any queue, however you can set up ACL at queue level if needed</p>"},{"location":"web-interface/my-activity/","title":"My Activity","text":"<p>\"My Activity\" section let each user access their Kibana dashboard pre-configured with user and date filters.</p> <p>Important</p> <p>This page embed the Amazon ElasticSearch instance configured with your SOCA cluster. You must have created the job index first</p> <p></p>"},{"location":"web-interface/my-files/","title":"My Files","text":"<p>\"My Files\" section let each user access their filesystem via a web browser.</p>"},{"location":"web-interface/my-files/#create-a-new-folder","title":"Create a new folder","text":"<p>Click \"Create Folder\" to create a new folder un your current working directory.</p> <p></p> <p>Your folder will be created instantly and visible under the File Explorer section.</p> <p></p>"},{"location":"web-interface/my-files/#upload-files","title":"Upload files","text":"<p>Click \"Upload Files\" button then drag and drop (or select) the files you want to upload.</p> <p></p> <p>Info</p> <p>Maximum upload size (default 5GB) and upload timeout (default 30 minutes) can be configure via <code>config.py</code></p>"},{"location":"web-interface/my-files/#download-files","title":"Download files","text":""},{"location":"web-interface/my-files/#all-files-in-directory","title":"All files in directory","text":"<p>Click \"Download All\" button to download all files from the current working directory (SOCA will create a zip archive)</p> <p></p>"},{"location":"web-interface/my-files/#multiple-files","title":"Multiple files","text":"<p>Select the files you want to download by checking the assigned checkboxes them click \"Download Selected\" button.</p> <p></p>"},{"location":"web-interface/my-files/#single-file","title":"Single file","text":"<p>To download a single file, click on the file name or use the first icon.</p> <p></p>"},{"location":"web-interface/my-files/#delete-files","title":"Delete files","text":"<p>To delete a file, simply click the last icon located on the right of the file name.</p> <p></p> <p>This will open a confirmation window.</p> <p></p>"},{"location":"web-interface/my-files/#edit-files","title":"Edit files","text":"<p>To edit a file, click the 3rd icon located on the right of the file name.</p> <p></p> <p>This will open a text editor. The editor includes syntax highlights and auto-completion.</p> <p></p> <p>Once you are done with your changes, check the checkbox and click \"Save\" button.</p>"},{"location":"web-interface/my-files/#use-file-as-simulation-input","title":"Use file as simulation input","text":"<p>Refer to this page for more information</p>"},{"location":"web-interface/my-files/#files-created-via-the-filesystem-are-not-visible","title":"Files created via the filesystem are not visible","text":"<p>For better performance, SOCA cache the content of a directory by default for 2 minutes (this settings can be changed in <code>config.py</code> via DEFAULT_CACHE_TIME). If you have created a file via SSH/DCV and this file is not yet visible, simply force a cache refresh by clicking the grey button.</p> <p></p>"},{"location":"web-interface/my-files/#permissions-settings","title":"Permissions &amp; Settings","text":"<p>The web interface rely on POSIX permissions. In other words, creating a file/folder is the same as running \"touch\" or \"mkdir\".</p> <p>Web configuration parameters are listed on <code>/apps/soca/&lt;YOUR_CLUSTER&gt;/cluster_web_ui/config.py</code>. A restart is required if you change any of these settings.</p> <pre><code>    APPS_LOCATION = \"/apps/\"\nUSER_HOME = \"/data/home\" # Adjust if you use a different location\nCHROOT_USER = False  # if True, user can only access their $HOME directory (aka: USER_HOME/&lt;user&gt;)\nPATH_TO_RESTRICT = []  # eg: /apps/folder1 -&gt; users can't access anything under /apps/folder1\nDEFAULT_CACHE_TIME = 120  # 2 minutes. Change this value to optimize performance in case you have a large number of concurrent user\nMAX_UPLOAD_FILE = 5120  # 5 GB\nMAX_UPLOAD_TIMEOUT = 1800000  # 30 minutes\nMAX_SIZE_ONLINE_PREVIEW = 150000000  # in bytes (150mb by default), maximum size of file that can be visualized via the web editor\nMAX_ARCHIVE_SIZE = 150000000  # in bytes (150mb by default), maximum size of archive generated when downloading multiple files at once\nDAILY_BACKUP_COUNT = 15  # Keep 15 latest daily backups\n</code></pre>"},{"location":"web-interface/my-job-queue/","title":"My Job Queue","text":"<p>\"My Job Queue\" section let each user access their jobs information via the web interface.</p> <p></p> <p>SOCA automatically add a tag if capacity is being provisioned for a job in \"queued\" state.</p> <p></p>"},{"location":"web-interface/my-job-queue/#retrieve-job-information","title":"Retrieve job information","text":"<p>Click \"Job Info\" button to get information about your job.</p> <p></p>"},{"location":"web-interface/my-job-queue/#access-your-job-output","title":"Access your job output","text":"<p>Click \"Job Directory\" button to access the job output location.</p> <p></p>"},{"location":"web-interface/my-job-queue/#delete-a-job","title":"Delete a job","text":"<p>Click \"Kill\" button to remove a job from the queue.</p> <p></p>"},{"location":"web-interface/my-job-queue/#understand-why-your-job-cannot-start","title":"Understand why your job cannot start","text":"<p>There are multiple reasons why you job cannot start. To display useful information to the users, SOCA automatically add a visual tag to any jobs that won't be able to start due to misconfiguration.</p> <p></p> <p>Users can understand why their jobs are blocked by clicking the \"Job cannot start\" button. Some examples includes:</p>"},{"location":"web-interface/my-job-queue/#not-enough-software-licenses","title":"Not enough software licenses","text":""},{"location":"web-interface/my-job-queue/#typo-in-the-instance-type","title":"Typo in the instance type","text":""},{"location":"web-interface/my-job-queue/#invalid-configuration-request-efa-but-use-an-instance-without-efa-support","title":"Invalid configuration (request EFA but use an instance without EFA support)","text":""},{"location":"web-interface/my-job-queue/#and-more","title":"And more ...","text":"<ul> <li>Limit of running jobs exceeded</li> <li>Limit of provisioning instance exceeded</li> <li>AWS Service limit errors (eg: cannot provision more EBS volume)</li> <li>Can't start job when \"force_ri=True\" if you do not have enough Reserved Instance availables</li> <li>...</li> </ul>"},{"location":"web-interface/share-data-windows-sessions/","title":"How to easily share data with your virtual desktops","text":"<p>Feature in preview</p> <p>This feature is only available on beta builds</p>"},{"location":"web-interface/share-data-windows-sessions/#using-dcv-session-storage-windows-and-linux","title":"Using DCV session storage (Windows and Linux)","text":"<p>In addition of existing SCP/SFTP, you can now take advantage of DCV session-storage feature. Session Storage is a folder from where you can upload/download files directly from DCV.</p>"},{"location":"web-interface/share-data-windows-sessions/#using-the-web-browser","title":"Using the Web Browser","text":"<p>Click the  cloud icon (1) and select to \"Upload Files\" (2). Choose the file you want to upload from the explorer list.</p> <p></p> <p>To check the progress of your upload/download, click on the \"Notification bar\"</p> <p></p> <p>Once your upload is finish, locate the file on your filesystem</p> <ul> <li>On Linux, your files will be uploaded to $HOME/session-storage</li> <li>On Windows, your files will be uploaded to C:\\session-storage</li> </ul> <p></p> <p>Download</p> <p>Session Storage also let you download files directly from your DCV session.</p>"},{"location":"web-interface/share-data-windows-sessions/#using-the-native-application","title":"Using the native application","text":"<p>If you use the native application, Click \"Connection &gt; File Storage\"</p> <p></p> <p>Important</p> <p>Although session storage is very handy, it's recommended to use traditional SFTP/SCP or the Web UI to upload large file to upload large files.</p>"},{"location":"web-interface/share-data-windows-sessions/#share-data-between-windows-sessions","title":"Share data between Windows sessions","text":"<p>Unlike Linux desktop, Windows desktops do not share a common filesystem, meaning data hosted on your Windows Session #1 are not accessible to your Windows Session #2 out of the box. </p>"},{"location":"web-interface/share-data-windows-sessions/#on-the-machine-you-want-to-share","title":"On the machine you want to share","text":"<p>On the machine you want to share, first open a terminal, type \"ipconfig\" and note the IP of your session (130.0.157.1 in this example)</p> <p></p> <p>Right click on the folder you want to share. In my example I want to share the entire C: drive so I right click on C: and click Properties</p> <p></p> <p>Navigate to \"Sharing\" tab and click \"Advanced Sharing\"</p> <p></p> <p>Check \"Share this folder\" box and specify a name (my_first_session in this example)</p> <p></p> <p>You can also click \"Permissions\" to manage who can access your files (default to read-only). In this example I simply give \"Everyone\" Read/Write access. Please note \"Everyone\" still require users to be able to successfully authenticate to your machine.</p> <p></p> <p>Finally, you can verify if you disk is correctly shared</p> <p></p>"},{"location":"web-interface/share-data-windows-sessions/#on-the-machine-you-want-to-access-the-share","title":"On the machine you want to access the share","text":"<p>On the file explorer, right click \"Network\" tab and click \"Map Network Drive\"</p> <p></p> <p>If it's your first time, you will need to enable \"Network Sharing\", simply click \"Ok\"</p> <p></p> <p>This will open a new ribbon, click on it and click \"Turn on Discovery and File Sharing\"</p> <p></p> <p>You will be prompted for your sharing settings. You can use both settings but we usually recommend limiting to Private Network only</p> <p></p> <p>Now you have enabled file sharing, right click \"Network\" tab and click \"Map Network Drive\" again. This time you will be prompted with a new window asking you the location of your share Specify \\\\, then click \"Connect using different credentials\" <p></p> <p>Go back to SOCA and retrieve your Windows session you want to share the folder from. Click \"Get Password\" and note the password. Please note each Windows sessions have a unique password.</p> <p></p> <p>Go back to your Windows and then enter your SOCA username (or Administrator) and the password your just retrieved from SOCA.</p> <p></p> <p>You are done. You can now access your share from the file explorer</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/","title":"Submit your HPC job via a custom web interface","text":"<p>On this page, you will learn how to create an application profile and give your users the ability to submit HPC jobs via a simple web interface. </p> <p>Submit a job via REST API</p> <p>In addition of web based job submission, SOCA also supports job management via REST API. Click here to learn more</p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#no-coding-experience-no-problem","title":"No coding experience = No Problem","text":"<p>SOCA features a complete visual form builder experience with simple drag &amp; drop capabilities. HPC admins can build their own forms without any coding/HTML experience via an intuitive wysiwyg (What You See Is What You Get) solution.</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#build-the-job-script","title":"Build the job script","text":"<p>The first step is to identify the variables you want your users to configure. Let's take this simple job file for reference:</p> <pre><code>#PBS -N MyJobName\n#PBS -P MyJobProject\n#PBS -q queue_1\n#PBS -l instance_type=c5.18xlarge\n\n# CD into current working directory\ncd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc\nexport PATH=/apps/softwarename/v2020/\nexport LICENSE_SERVER=1234@licenseserver.internal\n\n# Run the solver\n/apps/softwarename/v2020/bin/solver --cpus 36 \\\n--input-file myfile.input \\\n--parameter1 value1\n\n# Once job is complete, archive output to S3\nBACKUP=1\nif [[ \"$BACKUP\" -eq 1 ]]; then\naws s3 sync . s3://mybucketname/\nfi\n</code></pre> <p>Replace the values/parameters you want your users to configure with %VARIABLE_NAME% such as:</p> <pre><code>#PBS -N %job_name%\n#PBS -P %job_project%\n# I do not want my user to be able to change the queue\n#PBS -q myqueue \n#PBS -l instance_type=%instance_type%\n\n# CD into current working directory\ncd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc\nexport PATH=/apps/softwarename/%version%/\nexport LICENSE_SERVER=1234@licenseserver.internal\n\n# Run the solver\n/apps/softwarename/%version%/bin/solver --cpus %cpus% \\\n--input-file %input_file% \\\n--parameter1 %parameter1%\n\n# Once job is complete, archive output to S3\nBACKUP=%backup_enabled%\nif [[ \"$BACKUP\" -eq 1 ]]; then\naws s3 sync . s3://%bucket_to_archive%/\nfi\n</code></pre> <p>In this example:</p> <ul> <li>%job_name% will be replaced by the actual job name specified by the user</li> <li>%job_project% will be replaced by the project associated to the job </li> <li>%version% will let the user decide what software version to use</li> <li>%cpus% , %input_file%, and %parameter1% are application specific parameters</li> <li>%backup_enabled% will determine if we want to archive the job output to S3</li> <li>%bucket_to_archive% will point to the user's personal S3 bucket</li> </ul>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#create-the-html-form","title":"Create the HTML form","text":"<p>Now that you have identified all variables, you must create their associated HTML components. As a HPC admin, navigate to \"Application Management\" tab and start to build the HTML form.</p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#job_name-job_project-bucket_to_archive-input_file-and-parameter1","title":"%job_name% , %job_project%, %bucket_to_archive%, %input_file% and %parameter1%","text":"<p>Drag \"Text Field\" component from the left section to add it to the form.</p> <p>Configure the widget and configure the Name settings (red) with the variable name associated (job_name in our example)</p> <p>In the example below, the value entered by the user for job_name will be sent to the job script and retrieved via %job_name% </p> <p></p> <p>Repeat the same operation for %job_project%, %bucket_to_archive%, %parameter1% and %input_file%</p> <p>Note</p> <p>%input_file% will automatically be configured with the path of the input file selected by the user</p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#instance_type","title":"%instance_type%","text":"<p>We want to enforce the instance type to be c5.18xlarge. To do that, you can simply hardcode the information on the job script or create a \"Hidden Input\" parameter. The red section references to the variable name and the green section is the variable value.</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#cpus","title":"%cpus%","text":"<p>For %cpus% variable, we recommend using the \"Number\" component</p> <p></p> <p>Specify a name which match your variable name (red), pick the default value (green) then choose the Min/Max/Step values allowed (blue)</p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#version","title":"%version%","text":"<p>Assuming your application hierarchy is as follow:</p> <pre><code>\u2514\u2500\u2500 /apps\n    \u2514\u2500\u2500 /softwarename\n        \u251c\u2500\u2500 v2020\n     \u00a0\u00a0 \u251c\u2500\u2500 v2019\n     \u00a0\u00a0 \u2514\u2500\u2500 v2018\n</code></pre> <p>This time, we recommend you using the \"Select\" component:</p> <p></p> <p>Similarly to the previous examples, check the \"Required\" checkbox, map the \"Name\" to your variable name (%version%) and add labels (green) and their associated values (blue)</p> <p>Note</p> <p>Use autocomplete if you have a large number of entry</p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#backup_enabled","title":"%backup_enabled%","text":"<p>%backup_enabled% is a boolean which enable (1) or disable (0) archival of the job output data to S3. This time use \"Radio Group\" component and configure the different values:</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#configure-the-job-script","title":"Configure the job script","text":"<p>Once your HTML form is done, simply click \"Step2\" and copy/paste your job script</p> <p>Select your interpreter</p> <p>Since this script is expected to be triggered by PBS, keep the default \"will use qsub\" option.</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#save-your-profile","title":"Save your profile","text":"<p>Finally, navigate to step3, choose an name, upload a thumbnail if needed (optional) and click \"Create this application\"</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#submit-a-test-job","title":"Submit a test job","text":"<p>To submit a job, first navigate to \"Submit Job (Web)\" on the left sidebar. Choose your input file and click \"Use as Simulation input\" icon</p> <p></p> <p>Select the application you want to run</p> <p></p> <p>Fill out the HTML form generated during the previous step with your own inputs.</p> <p>Real time cost estimate</p> <p>The cost of your simulation is calculated in real-time based on the resources you are specifying</p> <p>Nodes Count</p> <p>SOCA also determine the number of nodes to provision automatically based on the instance type and cpus requested. In this example, the instance is c5.18xlarge (36 cores) and the number of CPUs requested by the user is 72. SOCA automatically detect these values and determine the number of instances to provision is 2</p> <p></p> <p>Once done, click \"Submit Job\" and you job will be submitted to the queue.</p> <p>Inputs sanitization</p> <p>SOCA automatically sanitize your inputs when required (remove space, special characters etc ...). In this example, the job name specified was \"My Super Job\" and was corrected to \"MySuperJob\" due some scheduler limitations.</p> <p>You can verify the input file generated by clicking \"Job Directory\":</p> <p></p> <p>Then select \"View or Edit this File\" icon (3rd from the left)</p> <p></p> <p>This will open the submit file created by the web form. You can verify the output generated by your form is correct and/or troubleshoot any potential issue</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#delete-job","title":"Delete job","text":"<p>To delete a job, simply navigate to \"My Job Queue\" section and click on the \"Kill\" button</p> <p></p>"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#what-if-i-want-to-run-a-linux-scriptcommand","title":"What if I want to run a Linux script/command","text":"<p>If you want your job script to use regular bash interpreter (and not qsub), simple select \"This is a Linux script\". In other words, the output generated by your HTML world will be a simple bash script and SOCA will run <code>/bin/bash job_submit.sh</code> command.</p> <p></p>"},{"location":"workshops/","title":"About","text":"<p>Welcome to the AWS workshop and lab content portal for Scale-Out Computing on AWS! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of how to deploy and operate an elastic, multiuser environment for computationally intensive workflows on the AWS Cloud.</p> <p>The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs.  Prior expertise with AWS and HPC workloads is helpful but not required to complete the labs.</p>"},{"location":"workshops/DVCon/","title":"DVCon Workshop Overview","text":""},{"location":"workshops/DVCon/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","title":"Launch a turnkey scale-out compute environment in minutes on AWS","text":"<p>The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. </p> <p>In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation, a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management.</p> <p>Note</p> <p>This tutorial assumes familiarity with the Linux command line.</p>"},{"location":"workshops/DVCon/#lab-environment-at-a-glance","title":"Lab environment at a glance","text":"<p>At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions.</p> <p>The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access.</p> <p>Let's get started. Click the Next link in the bottom right corner to move on to the next module.</p>"},{"location":"workshops/DVCon/getting-started/","title":"Getting Started","text":"<p>To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.</p>"},{"location":"workshops/DVCon/getting-started/#accessing-your-aws-account","title":"Accessing your AWS account","text":"<p>At the beginning of the workshop, you will be given a 12-character access code. This access code grants you access to a temporary AWS account that you'll use for this workshop.</p>"},{"location":"workshops/DVCon/getting-started/#step-1-log-in","title":"Step 1. Log in","text":"<p>Go to https://dashboard.eventengine.run, and enter the access code in the Team Hash field.  Click Proceed.</p> <p></p>"},{"location":"workshops/DVCon/getting-started/#step-2-get-credentials","title":"Step 2. Get Credentials","text":"<ol> <li> <p>On the Team Dashboard, click SSH Key to download the SSH Keypair PEM file.  You'll use this file later to SSH into an EC2 instance.</p> </li> <li> <p>If your using a Mac, change permissions of the PEM file that you just downloaded.  This is an SSH security requirement.</p> <p><code>chmod 600 /path/to/file.pem</code></p> </li> <li> <p>Next, click AWS Console to begin the login process to the AWS account.</p> </li> </ol> <p></p>"},{"location":"workshops/DVCon/getting-started/#step-3-open-aws-console","title":"Step 3. Open AWS Console","text":"<p>Click Open AWS Console. For this workshop, you will not need the Credentials or CLI Snippets</p> <p></p> <p>Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next.</p>"},{"location":"workshops/DVCon/modules/01-deploy-env/","title":"Lab 1: Deploy Environment","text":"<p>We want you to have hands-on experience deploying your own cluster, and in this module we will have you walk through the process of launching a cluster in your temporary AWS account.  This cluster will be provisioned in the background. For the subsequent modules you'll access a pre-built cluster where EDA tools and licenses have been provisioned. </p>"},{"location":"workshops/DVCon/modules/01-deploy-env/#step-1-launch-stack","title":"Step 1: Launch stack","text":"<p>This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud.</p> <ol> <li> <p>Sign in to the AWS Management Console and click the Launch Stack link below to launch the scale-out-computing-on-aws AWS CloudFormation template.</p> <p>Launch Stack</p> </li> <li> <p>Verify the launch region is Oregon</p> <p></p> <p>Important</p> <p>The template must be launched in Oregon for this workshop.</p> </li> <li> <p>On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.  We recommend naming it \"dvcon\".</p> <p>Warning</p> <p>The stack name must be less than 20 characters and must be lower-case only.</p> </li> <li> <p>Under Parameters, modify the the last four parameters, which are marked with REQUIRED.  Leave all other fields with their default values.  These are variables passed the CloudFormation automation that deploys the environment.</p> Parameter Default Description Install Location Installer S3 Bucket <code>solutions-reference</code> The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder <code>scale-out-computing-on-aws/latest</code> The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances.  Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler.  Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address See description REQUIRED The public-facing IP address that is permitted to log into the environment.  You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com.  Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the <code>ee-default-keypair</code> provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) <li> <p>Choose Next.</p> </li> <li> <p>On the Configure Stack Options page, choose Next.</p> </li> <li> <p>On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of <code>CREATE_COMPLETE</code> in approximately 35 minutes.</p> <p>By now you've learned how to deploy Scale-Out Computing on AWS in an AWS account. For the purpose of this workshop, you'll login to a pre-built setup that has the following:</p> <pre><code>* Synopsys VCS software pre-installed,\n* A license server with valid licenses, and\n* Workshop test case\n</code></pre> <p>You can now move on to the next lab. Click Next.</p>"},{"location":"workshops/DVCon/modules/02-web-login/","title":"Lab 2: Login to SOCA Console and Launch Remote Desktop Session","text":"<p>The goal of this module is to login to SOCA console and start a remote desktop session from which you will run applications and submit jobs into the cluster.  You will use the cluster's management console to start and monitor the session.</p>"},{"location":"workshops/DVCon/modules/02-web-login/#step-1-login-to-soca-console","title":"Step 1: Login to SOCA console","text":"<ol> <li> <p>Click on the link below to login to SOCA console</p> <p>Login to SOCA console</p> <p></p> <p>Note</p> <p>Your web browser will warn you about a certificate problem with the site.  To open the webpage, you need to authorize the browser to trust the self-signed security certificate.  In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint.</p> </li> <li> <p>Log in using the web UI using the following credentials:</p> <p>username: user + table id (for example: user1, user12, user24, etc...)</p> <p>password: provided in the session</p> </li> </ol>"},{"location":"workshops/DVCon/modules/02-web-login/#step-2-launch-remote-desktop-server","title":"Step 2: Launch remote desktop server","text":"<p>Follow these instructions to start a full remote desktop experience in your new cluster:</p> <ol> <li> <p>Click Graphical Access on the left sidebar.</p> <p></p> </li> <li> <p>Select  1 day in the Session Validity popup menu.</p> </li> <li> <p>Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu.</p> </li> <li> <p>Click Launch my Session #1</p> </li> </ol> <p>After you click Launch my session, a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. </p> <p>You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server.</p> <p>Note</p> <p>You can monitor the deployment of the remote desktop server by observing the status of the corresponding job under \"My Job Queue\". </p> <p>Please wait till the desktop instance is ready before moving on to the next step.  Click Next once the job status on \"My Job Queue\" is RUNNING.</p>"},{"location":"workshops/DVCon/modules/03-login-compile/","title":"Lab 3: Login to Remote Desktop and Compile Design","text":"<p>The goal with this lab is to evaluate the remote visualization experience using a graphical EDA tool.</p>"},{"location":"workshops/DVCon/modules/03-login-compile/#step-1-log-into-your-session","title":"Step 1: Log into your session","text":"<p>By now your remote desktop session should be ready and you should see the following under Your Session #1:</p> <p></p> <ol> <li> <p>Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above.</p> <p>Note</p> <p>You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com</p> </li> <li> <p>Start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager.</p> </li> </ol>"},{"location":"workshops/DVCon/modules/03-login-compile/#step-2-copy-test-case","title":"Step 2: Copy test case","text":"<ol> <li> <p>Copy the test case to your home directory by typing <code>cp -r /data/NVDLA_export /fsx/`whoami`</code> at the command prompt and hit enter</p> <p>Note</p> <p>/data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsx is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA</p> </li> <li> <p>Change directory to test case <code>cd /fsx/`whoami`/NVDLA_export</code> and hit enter</p> </li> <li>Source environment settings by typing <code>. setup.sh</code> and hit enter </li> </ol>"},{"location":"workshops/DVCon/modules/03-login-compile/#step-3-compile-the-test-case","title":"Step 3: Compile the test case","text":"<ol> <li> <p>Change directory to verif/sim <code>cd verif/sim</code> and hit enter</p> </li> <li> <p>Compile the test case by typing <code>make clean comp comp_verdi</code> then hit enter.</p> </li> <li> <p>You should see the test case is getting compiled and at the end should see:  </p> </li> </ol> <p>You've completed this lab. Click Next.</p>"},{"location":"workshops/DVCon/modules/04-submit-batch/","title":"Lab 4: Submit Batch Jobs","text":"<p>This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows.</p>"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-1-submit-jobs-to-the-scheduler","title":"Step 1: Submit jobs to the scheduler","text":"<p>Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab.</p> <ol> <li> <p>Execute the run_tests.sh script which will submit 20 batch jobs to the queue by typing <code>./run_tests.sh</code> then hit enter. You'll observe that the PBS scheduler will report the corresponding job idsfor each of these 20 jobs. </p> </li> <li> <p>You can examine the run_tests.sh script by typing <code>cat run_tests.sh</code> and observe that for each test we're specifying a different instance_type. This will usually depend on the CPU and memory requirements for the corresponding test.</p> </li> </ol>"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-2-watch-job-status","title":"Step 2: Watch job status","text":"<ol> <li> <p>As soon as jobs are sent to the queue, SOCA automation scripts will create a new compute instance to execute each job. Run the <code>qstat</code> command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar.     </p> </li> <li> <p>You can run the <code>pbsnodes -aSjL</code> command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue.     !!! note        The scheduler is configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running. </p> </li> </ol>"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-3-monitor-test20-job","title":"Step 3: Monitor test20 job","text":"<ol> <li> <p>Monitor the status of test20 job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with test20 under Name column.</p> </li> <li> <p>You can also monitor the job status in the terminal by typing <code>qstat</code> command.</p> </li> <li> <p>Once the job is in the running state, look inside test20 directory for test.log and novas.fsdb by typing <code>ls test20/*</code>. Wait until test20/novas.fsdb is created as you'll need to use it in the next lab.</p> </li> </ol> <p>Click Next to move to the next lab.</p>"},{"location":"workshops/DVCon/modules/05-load-verdi/","title":"Lab 5: Bring up Verdi","text":"<p>This module provides instructions for loading a graphical tool to debug a design which is commonly used in integrated cicuit design workflows.</p>"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-1-start-verdi-and-load-test20-database","title":"Step 1: Start Verdi and load test20 database","text":"<ol> <li>Start Verdi and load test20 waveform database by typing the command <code>verdi -ssf test20/novas.fsdb</code>. When Verdi GUI comes-up, click Ok to ignore the license expiration warning.     </li> </ol>"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-2-select-signals","title":"Step 2: Select Signals","text":"<ol> <li> <p>On the lower pane, click on Signal then on the pop-up click on Get Signals </p> </li> <li> <p>On the left pane, expand + top(top) then expand + nvdla_top(NV_nvdla) then click on u_parition_a(NV_NVDLA_partition_a)</p> </li> <li> <p>In the middle pane, click on accu2sc_credit_size[2:0] then hit shift and click on mac_a2accu_data2[175:0] then click OK </p> </li> </ol>"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-3-search-for-signal-transition","title":"Step 3: Search for Signal Transition","text":"<ol> <li>On the lower left pane, click on a2accu_dst_data0[175:0] and click twice on the Search Forward button to find the time where the bus changes to non-zero values and observe the timestamp 419,090,000 ps     </li> </ol>"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-4-exit-verdi","title":"Step 4: Exit Verdi","text":"<ol> <li>On the upper pane, click on File menu item then click on Exit to close Verdi. Click Yes on the exit confirmation question</li> </ol> <p>Click Next to move to the next lab.</p>"},{"location":"workshops/DVCon/modules/06-analytics/","title":"Lab 6: Explore Analytics Dashboard","text":""},{"location":"workshops/DVCon/modules/06-analytics/#step-1-open-cluster-dashboard","title":"Step 1: Open Cluster Dashboard","text":"<p>Return to the your cluster web UI and click on the Analytics section on the left sidebar.</p> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#step-2-add-data-to-your-cluster","title":"Step 2: Add Data to your Cluster","text":"<p>By default, job information is ingested by the analytics system on an hourly basis.</p> <ol> <li> <p>Log back into the scheduler host via SSH as <code>ec2-user</code> and run the follow command to force immediate ingestion into ElasticSearch:</p> <pre><code>source /etc/environment; /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/job_tracking.py\n</code></pre> </li> </ol>"},{"location":"workshops/DVCon/modules/06-analytics/#step-3-create-indexes","title":"Step 3: Create Indexes","text":"<p>Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\"</p> <p></p> <p>Go under Management and Click Index Patterns</p> <p></p> <p>Create your first index by typing pbsnodes*.</p> <p></p> <p>Click next, and then specify the Time Filter key (timestamp). Once done, click Create Index Pattern.</p> <p></p> <p>Repeat the same operation for jobs* index </p> <p></p> <p>This time,  select start_iso as time filter key.</p> <p></p> <p>Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data</p> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#index-information","title":"Index Information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time"},{"location":"workshops/DVCon/modules/06-analytics/#examples","title":"Examples","text":""},{"location":"workshops/DVCon/modules/06-analytics/#cluster-node","title":"Cluster Node","text":""},{"location":"workshops/DVCon/modules/06-analytics/#job-metadata","title":"Job Metadata","text":""},{"location":"workshops/DVCon/modules/06-analytics/#generate-graph","title":"Generate Graph","text":""},{"location":"workshops/DVCon/modules/06-analytics/#money-spent-by-instance-type","title":"Money spent by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Sum</li> <li>Field: estimate_price_ondemand</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: instance_type_used.keyword</li> <li>Order By: metric: Sum of estimated_price_on_demand</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By:  metric: Sum of price_on_demand</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#jobs-per-user-split-by-instance-type","title":"Jobs per user split by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p> </p>"},{"location":"workshops/DVCon/modules/06-analytics/#most-active-projects","title":"Most active projects","text":"<p>Configuration</p> <ul> <li>Select \"Pie\" and \"jobs\" index</li> <li>Slice Size (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Split Slices (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: project.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#instance-type-launched-by-user","title":"Instance type launched by user","text":"<p>Configuration</p> <ul> <li>Select \"Heat Map\" and \"jobs\" index</li> <li>Value (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Y Axis (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#number-of-nodes-in-the-cluster","title":"Number of nodes in the cluster","text":"<p>Configuration</p> <ul> <li>Select \"Lines\" and \"pbsnodes\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Unique Count</li> <li>Field: Mom.keyword</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Date Histogram,</li> <li>Field: timestamp</li> <li>Interval: Minute</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/DVCon/modules/06-analytics/#find-the-price-for-a-given-simulation","title":"Find the price for a given simulation","text":"<p>Each job comes with <code>estimated_price_ondemand</code> and <code>estimated_price_reserved</code> attributes which are calculated based on: <code>number of nodes * ( simulation_hours * instance_hourly_rate )</code></p> <p></p>"},{"location":"workshops/DVCon/modules/09-wrap-up/","title":"Wrap-up","text":"<p>Congratulations!  You've completed this workshop.</p>"},{"location":"workshops/DVCon/modules/09-wrap-up/#next-steps","title":"Next steps","text":"<ol> <li>Please complete the session survey in the mobile app.</li> </ol>"},{"location":"workshops/DVCon/modules/09-wrap-up/#clean-up","title":"Clean up","text":"<p>No cleanup required! This responsibility falls to AWS.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/","title":"Scaling EDA with Vivado Workshop Overview","text":""},{"location":"workshops/TKO-Scale-Out-Computing/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","title":"Launch a turnkey scale-out compute environment in minutes on AWS","text":"<p>The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. </p> <p>In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation, a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management.</p> <p>Note</p> <p>This tutorial assumes familiarity with the Linux command line.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/#lab-environment-at-a-glance","title":"Lab environment at a glance","text":"<p>At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions.</p> <p>The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access.</p> <p>Let's get started. Click the Next link in the bottom right corner to move on to the next module.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/","title":"Getting Started","text":""},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#account","title":"Account","text":"<p>You will use your own AWS account for this workshop.  You should log into an account with full admin privileges.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#region","title":"Region","text":"<p>You will deploy the CloudFormation stack into the US West (Oregon).  Other regions are not supported in this workshop at this time.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#key-pair","title":"Key Pair","text":"<p>This solution requires SSH login, so be sure you have a Key Pair for the US West (Oregon) region.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/","title":"Lab 1: Deploy Environment","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/#step-1-launch-stack","title":"Step 1: Launch stack","text":"<p>This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud.</p> <ol> <li> <p>Verify that you have a key pair in the  US West (Oregon) region.  If not, create a new key pair.</p> </li> <li> <p>Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template.</p> <ul> <li>Launch Stack in US West (Oregon)</li> </ul> </li> <li> <p>On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign the name \"soca\" to the stack.</p> <p>Warning</p> <p>The stack name must be less than 20 characters and must be lower-case only.</p> </li> <li> <p>Under Parameters, modify the the last four parameters, which are marked with REQUIRED.  Leave all other fields with their default values.  These are variables passed the CloudFormation automation that deploys the environment.</p> Parameter Default Description Install Location Installer S3 Bucket <code>solutions-reference</code> The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder <code>scale-out-computing-on-aws/latest</code> The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances.  Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler.  Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment.  You can leave it at default, but we recommend you change it to your public-facing IP address. Add the /32 suffix to the IP number. Key Pair Name REQUIRED Select your key pair. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) <li> <p>Choose Next.</p> </li> <li> <p>On the Configure Stack Options page, choose Next.</p> </li> <li> <p>On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of <code>CREATE_COMPLETE</code> in approximately 35 minutes.  Please wait for instructions from the workshop staff before moving on to the next lab.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/03-configure-desktop/","title":"Lab 2: Configure Remote Desktop","text":"<p>Once the solution has been deployed, we will configure the environment to use a specific Amazon Machine Image (AMI) for booting the remote desktop server.  As you saw in the architecture diagram, the DCV remote desktop will be your portal into the computing environment.  This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop.</p> <ol> <li> <p>Obtain IP address of scheduler server</p> <ol> <li> <p>In the AWS console, navigate to the CloudFormation page.</p> </li> <li> <p>Select the root stack named \"soca-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs.</p> <p></p> </li> <li> <p>The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString.  We'll use this command to SSH into the scheduler instance.  </p> <p></p> </li> </ol> </li> <li> <p>Connect to the instance over SSH</p> <ol> <li>For macOS, paste the SSH command into a terminal on the Mac.  Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance.</li> <li>For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs.</li> </ol> </li> <li> <p>Once logged in, as sudo, open the file <code>/apps/soca/cluster_manager/settings/queue_mapping.yml</code> using your favorite text editor.</p> <p>Example: <code>sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml</code></p> </li> <li> <p>Change the the highlighted values in the file to match the example below.</p> <p>Note</p> <p>Indentation matters in this file.  Therefore, we advise against copying and pasting this entire block into the file, as this can result in malformed formatting.  Instead, please edit the value of each highlighted line individually.</p> <pre><code>queue_type:\ncompute:\nqueues: [\"high\", \"normal\", \"low\"]\ninstance_ami: \"ami-05d709335d603daac\"\ninstance_type: \"c5.large\"\nht_support: \"false\"\nroot_size: \"100\"\nbase_os: \"centos7\"\n#scratch_size: \"100\"\n#scratch_iops: \"3600\"\n#efa_support: \"false\"\n# .. Refer to the doc for more supported parameters\ndesktop:\nqueues: [\"desktop\"]\ninstance_ami: \"ami-05d709335d603daac\"\ninstance_type: \"c5.2xlarge\"\nht_support: \"false\"\nroot_size: \"100\"\nbase_os: \"centos7\"\n</code></pre> </li> <li> <p>Save the changes to the file.</p> </li> </ol> <p>Keep this SSH session open; you will come back to it later.  Click the Next to move on to the next module.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/","title":"Lab 3: Launch Remote Desktop Session","text":"<p>The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster.  You will use the cluster's management console to start and monitor the session.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","title":"Step 1: Subscribe to AWS FPGA Developer AMI","text":"<p>This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace. This is the AMI you added to the <code>/apps/soca/cluster_manager/settings/queue_mapping.yml</code> file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop.</p> <ol> <li> <p>Return to the AWS console for your temporary AWS account.</p> </li> <li> <p>Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe.</p> </li> <li> <p>Review the terms and conditions for software usage, and then choose Accept Terms.</p> <p>When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure; the workshop CloudFormation templates will deploy the AMI for you.</p> </li> <li> <p>Click here to verify the subscription within the Marketplace dashboard.  You should see the FPGA Developer AMI in the list of subscriptions.</p> <p></p> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-2-log-into-web-ui","title":"Step 2: Log into web UI","text":"<ol> <li> <p>Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs.</p> <p></p> </li> <li> <p>The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI</p> <p></p> <p>Note</p> <p>Your web browser will warn you about a certificate problem with the site.  To open the webpage, you must authorize the browser to trust the self-signed security certificate.</p> </li> <li> <p>Log in using the web UI using credentials you provided in the CloudFormation template username and password parameters:</p> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-3-launch-remote-desktop-server","title":"Step 3: Launch remote desktop server","text":"<p>Follow these instructions to start a full remote desktop experience in your new cluster:</p> <ol> <li> <p>Click Graphical Access on the left sidebar.</p> <p></p> </li> <li> <p>Select  1 day in the Session Validity popup menu.</p> </li> <li> <p>Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu.</p> </li> <li> <p>Click Launch my Session #1</p> </li> </ol> <p>After you click Launch my session, a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. </p> <p>You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server.</p> <p>Note</p> <p>You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in <code>job-0</code>.  If after 5 minutes the status of the stack is not <code>CREATE_COMPLETE</code>, please raise your hand for assistance.</p> <p>Let's move on to the next step while we wait for the desktop instance to launch.  Click Next.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/","title":"Lab 4: Launch Vivado GUI","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#launch-the-vivado-cad-tool","title":"Launch the Vivado CAD Tool","text":"<p>Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI).  The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-1-log-into-your-session","title":"Step 1: Log into your session","text":"<p>By now your remote desktop session should be ready and you should see the following under Your Session #1:</p> <p></p> <ol> <li> <p>Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above.</p> <p>Note</p> <p>You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com</p> </li> <li> <p>To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager.</p> </li> <li> <p>Type <code>vivado</code> at the command prompt and hit enter</p> <p></p> <p>The Vivado GUI start and show the following screen:</p> <p></p> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-2-load-sample-project","title":"Step 2: Load sample project","text":"<p>Next, load a sample workload using one of the included example projects:</p> <ol> <li>Go to the Quick Start section and select Open Project.</li> <li>Navigate to <code>/projects/mfg405/</code>, select the <code>project_1.xpr</code> file, and click OK.</li> <li> <p>Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel.</p> <p>After the design opens you should see this:</p> <p></p> <p>You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience.</p> </li> <li> <p>For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design.</p> </li> </ol> <p>You've completed this lab. Click Next.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/","title":"Lab 5: Submit Batch Workloads","text":"<p>This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows.  The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-1-clone-workload-repo","title":"Step 1: Clone workload repo","text":"<ol> <li> <p>End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab.</p> </li> <li> <p>Minimize the Vivado GUI and open a new terminal window.</p> </li> <li> <p>Clone the example workload from the <code>aws-fpga-sa-demo</code> Github repo into your user's home directory on the NFS file system.</p> <pre><code>cd $HOME\ngit clone https://github.com/morrmt/aws-fpga-sa-demo.git\n</code></pre> </li> <li> <p>Change into the repo's workshop directory.</p> <p><code>cd $HOME/aws-fpga-sa-demo/eda-workshop</code></p> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","title":"Step 2: Submit jobs into the queue","text":"<p>Next, you'll submit four jobs into the cluster, each job requesting a specific instance type.  Using multiple instance types will help provide more interesting data to look at in the analytics lab.</p> <ol> <li> <p>Submit jobs. Submit each of the jobs below:</p> <p><code>qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> </li> <li> <p>Watch job status. As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job.  Run the <code>qstat</code> command to view the status of the jobs.  You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar.</p> <p></p> </li> </ol> <p>You can also run the <code>pbsnodes -a</code> command to see the EC2 instances that have joined the cluster.</p> <p>Click Next to move to the next section.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/","title":"Lab 6: Configure Budgets","text":"<p>This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached.  </p> <p>In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-1-create-an-aws-budget","title":"Step 1. Create an AWS Budget","text":"<ol> <li>Go to the AWS Billing Dashboard, and click Budgets on the left sidebar.     </li> <li>Click Create budget.</li> <li>Select Cost budget and click Set your budget.</li> <li>Name your budget \"Project 1\" and set Budgeted amount to $100.  Leave all other fields at their default values.</li> <li>Click Configure alerts.</li> <li>Set Alert threshold to 80 and add an email address to Email contacts.</li> <li>Click Confirm budget to review the configuration, then click Create.</li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-2-enable-budget-enforcement","title":"Step 2. Enable budget enforcement","text":"<p>Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget.</p> <p>To enable this feature, you will first need to verify the project assigned to each job during submission time.</p> <ol> <li> <p>Find the account ID for your temporary AWS account.</p> <ol> <li>Click here to go to the Account page of the AWS console.</li> <li>Copy the twelve digit Account Id number located underneath the Account Settings section.</li> </ol> </li> <li> <p>Log back into the scheduler instance via SSH and edit the <code>/apps/soca/cluster_hooks/queuejob/check_project_budget.py</code> script and paste the AWS account ID as the value for the <code>aws_account_id</code> variable.  Save the file when done.</p> <pre><code># User Variables\naws_account_id = '&lt;ENTER_YOUR_AWS_ACCOUNT_ID&gt;'\nbudget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt'\n</code></pre> </li> <li> <p>Enable the integration with the scheduler by running the following commands on the scheduler host:</p> <pre><code>sudo -i\nsource /etc/environment\nqmgr -c \"create hook check_project_budget event=queuejob\"\nqmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"\n</code></pre> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-3-test-budget-enforcement","title":"Step 3. Test budget enforcement","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#submit-a-job-without-budget-assignment","title":"Submit a job without budget assignment","text":"<ol> <li> <p>Switch to the <code>admin</code> cluster user.</p> <p><code>sudo su - admin</code></p> </li> <li> <p>Submit a job.</p> <p><code>qsub -- /bin/echo Hello</code></p> <p>This job will be rejected and you will see the following messages:</p> <pre><code>qsub: Error. You tried to submit job without project. Specify project using -P parameter\n</code></pre> </li> <li> <p>OK, let's add a project tag to comply with the policy.</p> <p><code>qsub -P \"Project 1\" -- /bin/echo Hello</code></p> <p>This job will also be rejected:</p> <pre><code>qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt\n</code></pre> <p>Next, we'll associated the user with \"Project 1\" by adding the username to the <code>project_cost_manager.txt</code> mapping file.</p> </li> <li> <p>As sudo, open the <code>/apps/soca/cluster_manager/settings/project_cost_manager.txt</code> file and add \"Project 1\" budget and the \"admin\" user.</p> <pre><code># This file is used to prevent job submission when budget allocated to a project exceed your threshold\n# This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py\n# Help &amp; Documentation: https://soca.dev/tutorials/set-up-budget-project/\n#\n#\n# Syntax:\n#   [project 1]\n#   user1\n#   user2\n#   [project 2]\n#   user1\n#   user3\n#   [project blabla]\n#   user4\n#   user5\n\n[Project 1]\nadmin\n</code></pre> <p>Important</p> <p>The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive)</p> </li> <li> <p>Save this file and try to submit a job. This time the job will be accepted.</p> </li> </ol> <p>The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them.</p> <p>If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error:</p> <pre><code>qsub -P \"Project 2\" -- /bin/echo Hello\nqsub: Error. Unable to query AWS Budget API. ERROR: An error occurred (NotFoundException) when calling the DescribeBudget operation: [Exception=NotFoundException] Failed to call DescribeBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Failed to call GetBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Unable to get budget: Project 2 - the budget doesn't exist.\n</code></pre> <p>Done!  On to the next lab.  Click Next.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/","title":"Lab 7: Explore Analytics Dashboard","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-1-open-cluster-dashboard","title":"Step 1: Open Cluster Dashboard","text":"<p>Return to the your cluster web UI and click on the Analytics section on the left sidebar.</p> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-2-add-data-to-your-cluster","title":"Step 2: Add Data to your Cluster","text":"<p>By default, job information is ingested by the analytics system on an hourly basis.</p> <ol> <li> <p>Log back into the scheduler host via SSH as <code>ec2-user</code> and run the follow command to force immediate ingestion into ElasticSearch:</p> <pre><code>source /etc/environment; /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/job_tracking.py\n</code></pre> </li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-3-create-indexes","title":"Step 3: Create Indexes","text":"<p>Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\"</p> <p></p> <p>Go under Management and Click Index Patterns</p> <p></p> <p>Create your first index by typing pbsnodes*.</p> <p></p> <p>Click next, and then specify the Time Filter key (timestamp). Once done, click Create Index Pattern.</p> <p></p> <p>Repeat the same operation for jobs* index </p> <p></p> <p>This time,  select start_iso as time filter key.</p> <p></p> <p>Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data</p> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#index-information","title":"Index Information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#examples","title":"Examples","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#cluster-node","title":"Cluster Node","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#job-metadata","title":"Job Metadata","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#generate-graph","title":"Generate Graph","text":""},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#money-spent-by-instance-type","title":"Money spent by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Sum</li> <li>Field: estimate_price_ondemand</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: instance_type_used.keyword</li> <li>Order By: metric: Sum of estimated_price_on_demand</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By:  metric: Sum of price_on_demand</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#jobs-per-user-split-by-instance-type","title":"Jobs per user split by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p> </p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#most-active-projects","title":"Most active projects","text":"<p>Configuration</p> <ul> <li>Select \"Pie\" and \"jobs\" index</li> <li>Slice Size (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Split Slices (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: project.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#instance-type-launched-by-user","title":"Instance type launched by user","text":"<p>Configuration</p> <ul> <li>Select \"Heat Map\" and \"jobs\" index</li> <li>Value (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Y Axis (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#number-of-nodes-in-the-cluster","title":"Number of nodes in the cluster","text":"<p>Configuration</p> <ul> <li>Select \"Lines\" and \"pbsnodes\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Unique Count</li> <li>Field: Mom.keyword</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Date Histogram,</li> <li>Field: timestamp</li> <li>Interval: Minute</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#find-the-price-for-a-given-simulation","title":"Find the price for a given simulation","text":"<p>Each job comes with <code>estimated_price_ondemand</code> and <code>estimated_price_reserved</code> attributes which are calculated based on: <code>number of nodes * ( simulation_hours * instance_hourly_rate )</code></p> <p></p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/","title":"Wrap-up","text":"<p>Congratulations!  You've completed this workshop.</p>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#next-steps","title":"Next steps","text":"<ol> <li>Try this at home! The code for this workshop is open source and publically available. Click on the link to the GitHub repo in the upper-right corner to view the code. </li> <li>Please complete the session survey in the mobile app.</li> </ol>"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#clean-up","title":"Clean up","text":"<p>Delete the root stack to remove the resources from your account.</p>"},{"location":"workshops/reinvent19-MFG405/","title":"MFG405 Workshop Overview","text":""},{"location":"workshops/reinvent19-MFG405/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","title":"Launch a turnkey scale-out compute environment in minutes on AWS","text":"<p>The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. </p> <p>In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation, a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management.</p> <p>Note</p> <p>This tutorial assumes familiarity with the Linux command line.</p>"},{"location":"workshops/reinvent19-MFG405/#lab-environment-at-a-glance","title":"Lab environment at a glance","text":"<p>At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions.</p> <p>The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access.</p> <p>Let's get started. Click the Next link in the bottom right corner to move on to the next module.</p>"},{"location":"workshops/reinvent19-MFG405/getting-started/","title":"Getting Started","text":"<p>To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.</p>"},{"location":"workshops/reinvent19-MFG405/getting-started/#accessing-your-aws-account","title":"Accessing your AWS account","text":"<p>At the beginning of the workshop, you will be given a 12-character access code. This access code grants you access to a temporary AWS account that you'll use for this workshop.</p>"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-1-log-in","title":"Step 1. Log in","text":"<p>Go to https://dashboard.eventengine.run, and enter the access code in the Team Hash field.  Click Proceed.</p> <p></p>"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-2-get-credentials","title":"Step 2. Get Credentials","text":"<ol> <li> <p>On the Team Dashboard, click SSH Key to download the SSH Keypair PEM file.  You'll use this file later to SSH into an EC2 instance.</p> </li> <li> <p>If your using a Mac, change permissions of the PEM file that you just downloaded.  This is an SSH security requirement.</p> <p><code>chmod 600 /path/to/file.pem</code></p> </li> <li> <p>Next, click AWS Console to begin the login process to the AWS account.</p> </li> </ol> <p></p>"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-3-open-aws-console","title":"Step 3. Open AWS Console","text":"<p>Click Open AWS Console. For this workshop, you will not need the Credentials or CLI Snippets</p> <p></p> <p>Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next.</p>"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/","title":"Lab 1: Deploy Environment","text":"<p>Your temporary AWS account has been pre-provisioned with a multiuser computing environment.  You'll use this pre-built cluster for the workshop labs.</p> <p>We also want you to have hands-on experience deploying you're own cluster, and in this module we will have you walk through the process of launching a second cluster in your temporary AWS account.  This second cluster will be provisioned in the background while you work on the rest of the workshop in the pre-built cluster.</p>"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/#step-1-launch-stack","title":"Step 1: Launch stack","text":"<p>This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud.</p> <ol> <li> <p>Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template.</p> <p>Launch Stack</p> </li> <li> <p>Verify the launch region is Oregon</p> <p></p> <p>Important</p> <p>The template must be launched in Oregon for this workshop.</p> </li> <li> <p>On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.  We recommend naming it \"mfg405\".</p> <p>Warning</p> <p>The stack name must be less than 20 characters and must be lower-case only.</p> </li> <li> <p>Under Parameters, modify the the last four parameters, which are marked with REQUIRED.  Leave all other fields with their default values.  These are variables passed the CloudFormation automation that deploys the environment.</p> Parameter Default Description Install Location Installer S3 Bucket <code>solutions-reference</code> The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder <code>scale-out-computing-on-aws/latest</code> The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances.  Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler.  Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment.  You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com.  Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the <code>ee-default-keypair</code> provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) <li> <p>Choose Next.</p> </li> <li> <p>On the Configure Stack Options page, choose Next.</p> </li> <li> <p>On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of <code>CREATE_COMPLETE</code> in approximately 35 minutes.</p> <p>You can now move on to the next lab.  Click Next.</p>"},{"location":"workshops/reinvent19-MFG405/modules/03-configure-desktop/","title":"Lab 2: Configure Remote Desktop","text":"<p>While the cluster you deployed in the previous lab is running in the background, we'll configure your pre-built cluster to use a specific Amazon Machine Image (AMI) for booting the remote desktop server.  As you saw in the architecture diagram, the DCV remote desktop will be your portal into the computing environment.  This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop.</p> <ol> <li> <p>Obtain IP address of scheduler server</p> <ol> <li> <p>In the AWS console, navigate to the CloudFormation page.</p> </li> <li> <p>Select the root stack named \"mod-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs.</p> <p></p> </li> <li> <p>The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString.  We'll use this command to SSH into the scheduler instance.  </p> <p></p> </li> </ol> </li> <li> <p>Connect to the instance over SSH</p> <ol> <li>For macOS, paste the SSH command into a terminal on the Mac.  Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance.</li> <li>For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs.</li> </ol> </li> <li> <p>Once logged in, as sudo, open the file <code>/apps/soca/cluster_manager/settings/queue_mapping.yml</code> using your favorite text editor.</p> <p>Example: <code>sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml</code></p> </li> <li> <p>Change the the highlighted values in the file to match the example below.</p> <p>Note</p> <p>We strongly recommend that you do not copy and paste this entire block into the file.  Instead, please edit the value of each highlighted line individually.</p> <pre><code>queue_type:\ncompute:\nqueues: [\"high\", \"normal\", \"low\"]\ninstance_ami: \"ami-05d709335d603daac\"\ninstance_type: \"c5.large\"\nht_support: \"false\"\nroot_size: \"100\"\nbase_os: \"centos7\"\n#scratch_size: \"100\"\n#scratch_iops: \"3600\"\n#efa_support: \"false\"\n# .. Refer to the doc for more supported parameters\ndesktop:\nqueues: [\"desktop\"]\ninstance_ami: \"ami-05d709335d603daac\"\ninstance_type: \"c5.2xlarge\"\nht_support: \"false\"\nroot_size: \"100\"\nbase_os: \"centos7\"\n</code></pre> </li> <li> <p>Save the changes to the file.</p> </li> </ol> <p>Keep this SSH session open; you will come back to it later.  Click the Next to move on to the next module.</p>"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/","title":"Lab 3: Launch Remote Desktop Session","text":"<p>The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster.  You will use the cluster's management console to start and monitor the session.</p>"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","title":"Step 1: Subscribe to AWS FPGA Developer AMI","text":"<p>This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace. This is the AMI you added to the <code>/apps/soca/cluster_manager/settings/queue_mapping.yml</code> file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop.</p> <ol> <li> <p>Return to the AWS console for your temporary AWS account.</p> </li> <li> <p>Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe.</p> </li> <li> <p>Review the terms and conditions for software usage, and then choose Accept Terms.</p> <p>When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure; the workshop CloudFormation templates will deploy the AMI for you.</p> </li> <li> <p>Click here to verify the subscription within the Marketplace dashboard.  You should see the FPGA Developer AMI in the list of subscriptions.</p> <p></p> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-2-log-into-web-ui","title":"Step 2: Log into web UI","text":"<ol> <li> <p>Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs.</p> <p></p> </li> <li> <p>The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI</p> <p></p> <p>Note</p> <p>Your web browser will warn you about a certificate problem with the site.  To open the webpage, you must authorize the browser to trust the self-signed security certificate.  In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint.</p> </li> <li> <p>Log in using the web UI using the following credentials:</p> <p>username: admin</p> <p>password: passw0rd (use a zero instead of 'o')</p> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-3-launch-remote-desktop-server","title":"Step 3: Launch remote desktop server","text":"<p>Follow these instructions to start a full remote desktop experience in your new cluster:</p> <ol> <li> <p>Click Graphical Access on the left sidebar.</p> <p></p> </li> <li> <p>Select  1 day in the Session Validity popup menu.</p> </li> <li> <p>Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu.</p> </li> <li> <p>Click Launch my Session #1</p> </li> </ol> <p>After you click Launch my session, a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. </p> <p>You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server.</p> <p>Note</p> <p>You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in <code>job-0</code>.  If after 5 minutes the status of the stack is not <code>CREATE_COMPLETE</code>, please raise your hand for assistance.</p> <p>Let's move on to the next step while we wait for the desktop instance to launch.  Click Next.</p>"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/","title":"Lab 4: Launch Vivado GUI","text":""},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#launch-the-vivado-cad-tool","title":"Launch the Vivado CAD Tool","text":"<p>Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI).  The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.</p>"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-1-log-into-your-session","title":"Step 1: Log into your session","text":"<p>By now your remote desktop session should be ready and you should see the following under Your Session #1:</p> <p></p> <ol> <li> <p>Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above.</p> <p>Note</p> <p>You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com</p> </li> <li> <p>To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager.</p> </li> <li> <p>Type <code>vivado</code> at the command prompt and hit enter</p> <p></p> <p>The Vivado GUI start and show the following screen:</p> <p></p> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-2-load-sample-project","title":"Step 2: Load sample project","text":"<p>Next, load a sample workload using one of the included example projects:</p> <ol> <li>Go to the Quick Start section and select Open Project.</li> <li>Navigate to <code>/projects/mfg405/</code>, select the <code>project_1.xpr</code> file, and click OK.</li> <li> <p>Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel.</p> <p>After the design opens you should see this:</p> <p></p> <p>You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience.</p> </li> <li> <p>For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design.</p> </li> </ol> <p>You've completed this lab. Click Next.</p>"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/","title":"Lab 5: Submit Batch Workloads","text":"<p>This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows.  The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.</p>"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-1-clone-workload-repo","title":"Step 1: Clone workload repo","text":"<ol> <li> <p>End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab.</p> </li> <li> <p>Minimize the Vivado GUI and open a new terminal window.</p> </li> <li> <p>Clone the example workload from the <code>aws-fpga-sa-demo</code> Github repo into your user's home directory on the NFS file system.</p> <pre><code>cd $HOME\ngit clone https://github.com/morrmt/aws-fpga-sa-demo.git\n</code></pre> </li> <li> <p>Change into the repo's workshop directory.</p> <p><code>cd $HOME/aws-fpga-sa-demo/eda-workshop</code></p> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","title":"Step 2: Submit jobs into the queue","text":"<p>Next, you'll submit four jobs into the cluster, each job requesting a specific instance type.  Using multiple instance types will help provide more interesting data to look at in the analytics lab.</p> <ol> <li> <p>Submit jobs. Submit each of the jobs below:</p> <p><code>qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> <p><code>qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME</code></p> </li> <li> <p>Watch job status. As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job.  Run the <code>qstat</code> command to view the status of the jobs.  You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar.</p> <p></p> </li> </ol> <p>You can also run the <code>pbsnodes -a</code> command to see the EC2 instances that have joined the cluster.</p> <p>Click Next to move to the next section.</p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/","title":"Lab 6: Explore Analytics Dashboard","text":""},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-1-open-cluster-dashboard","title":"Step 1: Open Cluster Dashboard","text":"<p>Return to the your cluster web UI and click on the Analytics section on the left sidebar.</p> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-2-add-data-to-your-cluster","title":"Step 2: Add Data to your Cluster","text":"<p>By default, job information is ingested by the analytics system on an hourly basis.</p> <ol> <li> <p>Log back into the scheduler host via SSH as <code>ec2-user</code> and run the follow command to force immediate ingestion into ElasticSearch:</p> <pre><code>source /etc/environment; /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/job_tracking.py\n</code></pre> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-3-create-indexes","title":"Step 3: Create Indexes","text":"<p>Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\"</p> <p></p> <p>Go under Management and Click Index Patterns</p> <p></p> <p>Create your first index by typing pbsnodes*.</p> <p></p> <p>Click next, and then specify the Time Filter key (timestamp). Once done, click Create Index Pattern.</p> <p></p> <p>Repeat the same operation for jobs* index </p> <p></p> <p>This time,  select start_iso as time filter key.</p> <p></p> <p>Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data</p> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#index-information","title":"Index Information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#examples","title":"Examples","text":""},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#cluster-node","title":"Cluster Node","text":""},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#job-metadata","title":"Job Metadata","text":""},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#generate-graph","title":"Generate Graph","text":""},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#money-spent-by-instance-type","title":"Money spent by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Sum</li> <li>Field: estimate_price_ondemand</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: instance_type_used.keyword</li> <li>Order By: metric: Sum of estimated_price_on_demand</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By:  metric: Sum of price_on_demand</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#jobs-per-user-split-by-instance-type","title":"Jobs per user split by instance type","text":"<p>Configuration</p> <ul> <li>Select \"Vertical Bars\" and \"jobs\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user.keyword</li> <li>Order By: metric: Count</li> </ul> </li> <li>Split Series (Buckets):<ul> <li>Sub Aggregation: Terms</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p> </p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#most-active-projects","title":"Most active projects","text":"<p>Configuration</p> <ul> <li>Select \"Pie\" and \"jobs\" index</li> <li>Slice Size (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Split Slices (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: project.keyword</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#instance-type-launched-by-user","title":"Instance type launched by user","text":"<p>Configuration</p> <ul> <li>Select \"Heat Map\" and \"jobs\" index</li> <li>Value (Metrics):<ul> <li>Aggregation: Count</li> </ul> </li> <li>Y Axis (Buckets):<ul> <li>Aggregation: Term</li> <li>Field: instance_type_used</li> <li>Order By: metric: Count</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Terms</li> <li>Field: user</li> <li>Order By: metric: Count</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#number-of-nodes-in-the-cluster","title":"Number of nodes in the cluster","text":"<p>Configuration</p> <ul> <li>Select \"Lines\" and \"pbsnodes\" index</li> <li>Y Axis (Metrics):<ul> <li>Aggregation: Unique Count</li> <li>Field: Mom.keyword</li> </ul> </li> <li>X Axis (Buckets):<ul> <li>Aggregation: Date Histogram,</li> <li>Field: timestamp</li> <li>Interval: Minute</li> </ul> </li> </ul> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#find-the-price-for-a-given-simulation","title":"Find the price for a given simulation","text":"<p>Each job comes with <code>estimated_price_ondemand</code> and <code>estimated_price_reserved</code> attributes which are calculated based on: <code>number of nodes * ( simulation_hours * instance_hourly_rate )</code></p> <p></p>"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/","title":"Wrap-up","text":"<p>Congratulations!  You've completed this workshop.</p>"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#next-steps","title":"Next steps","text":"<ol> <li>Try this at home! The code for this workshop is open source and publically available.  </li> <li>Please complete the session survey in the mobile app.</li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#clean-up","title":"Clean up","text":"<p>No cleanup required! This responsibility falls to AWS.</p>"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/","title":".","text":""},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#lab-4-configure-budgets","title":"Lab 4 Configure Budgets","text":"<p>This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached.  </p> <p>In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget.</p>"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-1-create-an-aws-budget","title":"Step 1. Create an AWS Budget","text":"<ol> <li>Go to the AWS Billing Dashboard, and click Budgets on the left sidebar.     </li> <li>Click Create budget.</li> <li>Select Cost budget and click Set your budget.</li> <li>Name your budget \"Project 1\" and set Budgeted amount to $100.  Leave all other fields at their default values.</li> <li>Click Configure alerts.</li> <li>Set Alert threshold to 80 and add an email address to Email contacts.</li> <li>Click Confirm budget to review the configuration, then click Create.</li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-2-enable-budget-enforcement","title":"Step 2. Enable budget enforcement","text":"<p>Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget.</p> <p>To enable this feature, you will first need to verify the project assigned to each job during submission time.</p> <ol> <li> <p>Find the account ID for your temporary AWS account.</p> <ol> <li>Click here to go to the Account page of the AWS console.</li> <li>Copy the twelve digit Account Id number located underneath the Account Settings section.</li> </ol> </li> <li> <p>Log back into the scheduler instance via SSH and edit the <code>/apps/soca/cluster_hooks/queuejob/check_project_budget.py</code> script and paste the AWS account ID as the value for the <code>aws_account_id</code> variable.  Save the file when done.</p> <pre><code># User Variables\naws_account_id = '&lt;ENTER_YOUR_AWS_ACCOUNT_ID&gt;'\nbudget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt'\n</code></pre> </li> <li> <p>Enable the integration with the scheduler by running the following commands on the scheduler host:</p> <pre><code>sudo -i\nsource /etc/environment\nqmgr -c \"create hook check_project_budget event=queuejob\"\nqmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"\n</code></pre> </li> </ol>"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-3-test-budget-enforcement","title":"Step 3. Test budget enforcement","text":""},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#submit-a-job-without-budget-assignment","title":"Submit a job without budget assignment","text":"<ol> <li> <p>Switch to the <code>admin</code> cluster user.</p> <p><code>sudo su - admin</code></p> </li> <li> <p>Submit a job.</p> <p><code>qsub -- /bin/echo Hello</code></p> <p>This job will be rejected and you will see the following messages:</p> <pre><code>qsub: Error. You tried to submit job without project. Specify project using -P parameter\n</code></pre> </li> <li> <p>OK, let's add a project tag to comply with the policy.</p> <p><code>qsub -P \"Project 1\" -- /bin/echo Hello</code></p> <p>This job will also be rejected:</p> <p><pre><code>qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt\n</code></pre> Next, we'll associated the user with \"Project 1\" by adding the username to the <code>project_cost_manager.txt</code> mapping file.</p> </li> <li> <p>As sudo, open the <code>/apps/soca/cluster_manager/settings/project_cost_manager.txt</code> file and add \"Project 1\" budget and the \"admin\" user.</p> <pre><code># This file is used to prevent job submission when budget allocated to a project exceed your threshold\n# This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py\n# Help &amp; Documentation: https://soca.dev/tutorials/set-up-budget-project/\n#\n#\n# Syntax:\n#   [project 1]\n#   user1\n#   user2\n#   [project 2]\n#   user1\n#   user3\n#   [project blabla]\n#   user4\n#   user5\n\n[Project 1]\nadmin\n</code></pre> <p>Important</p> <p>The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive)</p> </li> <li> <p>Save this file and try to submit a job. This time the job will be accepted.</p> </li> </ol> <p>The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them.</p> <p>If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error:</p> <p><pre><code>bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello\nqsub: Error. Unable to query AWS Budget API. ERROR: An error occurred (NotFoundException) when calling the DescribeBudget operation: [Exception=NotFoundException] Failed to call DescribeBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Failed to call GetBudget for [AccountId: &lt;REDACTED_ACCOUNT_ID&gt;] - Unable to get budget: Project 2 - the budget doesn't exist.\n</code></pre> Done!  On to the next lab.  Click Next.</p>"}]}